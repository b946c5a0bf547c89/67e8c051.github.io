<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>lfs command | 己不由心，身又岂能由己</title><meta name="keywords" content="lfs"><meta name="author" content="Ginger"><meta name="copyright" content="Ginger"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="check lfs log12$ strings dk$ lctl df &lt;input file&gt; &lt;output file&gt;  format1234567891011121314151617# mdtFSNAME&#x3D;lustreMDS1NID&#x3D;0.0.0.0@tcpecho mkfs.lustre --reformat --mgs --backfstype&#x3D;zfs --fs">
<meta property="og:type" content="article">
<meta property="og:title" content="lfs command">
<meta property="og:url" content="http://yoursite.com/2019/12/29/lfs_command/index.html">
<meta property="og:site_name" content="己不由心，身又岂能由己">
<meta property="og:description" content="check lfs log12$ strings dk$ lctl df &lt;input file&gt; &lt;output file&gt;  format1234567891011121314151617# mdtFSNAME&#x3D;lustreMDS1NID&#x3D;0.0.0.0@tcpecho mkfs.lustre --reformat --mgs --backfstype&#x3D;zfs --fs">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/img/photo_by_spacex.jpg">
<meta property="article:published_time" content="2019-12-29T07:41:16.000Z">
<meta property="article:modified_time" content="2020-10-09T12:18:12.665Z">
<meta property="article:author" content="Ginger">
<meta property="article:tag" content="lfs">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/img/photo_by_spacex.jpg"><link rel="shortcut icon" href="/img/stout-shield.png"><link rel="canonical" href="http://yoursite.com/2019/12/29/lfs_command/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.2.0',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-10-09 20:18:12'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><meta name="generator" content="Hexo 5.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/244247-guts.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">58</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">57</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">9</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#check-lfs-log"><span class="toc-number">1.</span> <span class="toc-text">check lfs log</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#format"><span class="toc-number">2.</span> <span class="toc-text">format</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Enable-large-dir-feature"><span class="toc-number">3.</span> <span class="toc-text">Enable large_dir feature</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#set-stripe-size-for-tiny-files"><span class="toc-number">4.</span> <span class="toc-text">set stripe size for tiny files</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#change-ipaddr"><span class="toc-number">5.</span> <span class="toc-text">change ipaddr</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Public"><span class="toc-number">6.</span> <span class="toc-text">Public</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Client"><span class="toc-number">7.</span> <span class="toc-text">Client</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#metadata-server"><span class="toc-number">8.</span> <span class="toc-text">metadata server</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#object-server"><span class="toc-number">9.</span> <span class="toc-text">object server</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#net"><span class="toc-number">10.</span> <span class="toc-text">net</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Trace-log"><span class="toc-number">11.</span> <span class="toc-text">Trace log</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lfs-log"><span class="toc-number">11.1.</span> <span class="toc-text">lfs log</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#kdump"><span class="toc-number">11.2.</span> <span class="toc-text">kdump</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages"><span class="toc-number"></span> <span class="toc-text">1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages"><span class="toc-number"></span> <span class="toc-text">1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#changelog"><span class="toc-number">1.</span> <span class="toc-text">changelog</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Enable"><span class="toc-number">1.1.</span> <span class="toc-text">Enable</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Flush-the-journal"><span class="toc-number">1.2.</span> <span class="toc-text">Flush the journal</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#re-writeconf"><span class="toc-number">1.3.</span> <span class="toc-text">re-writeconf</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#User-group-quota"><span class="toc-number">2.</span> <span class="toc-text">User group quota</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Disable-the-ost"><span class="toc-number">3.</span> <span class="toc-text">Disable the ost</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Skip-recovery"><span class="toc-number">4.</span> <span class="toc-text">Skip recovery</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-migarate"><span class="toc-number">5.</span> <span class="toc-text">lfs migarate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-fid-and-path"><span class="toc-number">6.</span> <span class="toc-text">lfs fid and path</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#increase-openzfs-sync-performance-in-test-env"><span class="toc-number">7.</span> <span class="toc-text">increase openzfs sync performance in test env</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lustre-issues"><span class="toc-number">8.</span> <span class="toc-text">Lustre issues</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Don-%E2%80%98t-use-openzfs-with-lfs-file-system"><span class="toc-number">8.1.</span> <span class="toc-text">Don ‘t use openzfs with lfs file system</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Multipath-failed-cause"><span class="toc-number">8.2.</span> <span class="toc-text">Multipath failed cause</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-client-random-read-performance"><span class="toc-number">9.</span> <span class="toc-text">lfs client random read performance</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#disable-OSS-read-cache"><span class="toc-number">9.1.</span> <span class="toc-text">disable OSS read cache</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#client-metadata-setting"><span class="toc-number">10.</span> <span class="toc-text">client metadata setting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#record-several-years-ago-trace-the-file-by-debugfs"><span class="toc-number">11.</span> <span class="toc-text">record several years ago trace the file by debugfs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Monitor"><span class="toc-number">12.</span> <span class="toc-text">Monitor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mdt-reduce-5-osq-lock-overhead-in-perf-top"><span class="toc-number">13.</span> <span class="toc-text">mdt reduce 5% osq_lock overhead in perf top</span></a></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(/img/photo_by_spacex.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">己不由心，身又岂能由己</a></span><span id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">lfs command</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-12-29T07:41:16.000Z" title="Created 2019-12-29 15:41:16">2019-12-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2020-10-09T12:18:12.665Z" title="Updated 2020-10-09 20:18:12">2020-10-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Storage/">Storage</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h3 id="check-lfs-log"><a href="#check-lfs-log" class="headerlink" title="check lfs log"></a>check lfs log</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ strings dk<br>$ lctl df &lt;input file&gt; &lt;output file&gt;<br></code></pre></td></tr></table></figure>

<h3 id="format"><a href="#format" class="headerlink" title="format"></a>format</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># mdt</span><br>FSNAME=lustre<br>MDS1NID=0.0.0.0@tcp<br><br><span class="hljs-built_in">echo</span> mkfs.lustre --reformat --mgs --backfstype=zfs --fsname=<span class="hljs-variable">$&#123;FSNAME&#125;</span> --servicenode=<span class="hljs-variable">$&#123;MDS1NID&#125;</span> mdt_0/mgt_0<br><span class="hljs-built_in">echo</span> mkfs.lustre --reformat --mdt --backfstype=zfs --mgsnode=<span class="hljs-variable">$&#123;MDS1NID&#125;</span> --fsname=<span class="hljs-variable">$&#123;FSNAME&#125;</span> --servicenode=<span class="hljs-variable">$&#123;MDS1NID&#125;</span> --index=0 mdt_0/mdt_0<br><br><span class="hljs-comment"># ost</span><br>FSNAME=lustre<br>MDS1NID=1.1.1.1@tcp<br>MDS2NID=2.2.2.2@tcp<br>OSS1NID=3.3.3.3@tcp <br>OSS2NID=4.4.4.4@tcp <br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> &#123;0..6&#125;<br><span class="hljs-keyword">do</span><br>  <span class="hljs-built_in">echo</span> mkfs.lustre --reformat --backfstype=zfs --ost  --index=<span class="hljs-variable">$i</span>  --fsname=<span class="hljs-variable">$&#123;FSNAME&#125;</span> --servicenode=<span class="hljs-variable">$&#123;OSS1NID&#125;</span> --servicenode=<span class="hljs-variable">$&#123;OSS2NID&#125;</span> --mgsnode=<span class="hljs-variable">$&#123;MDS1NID&#125;</span>  ost_<span class="hljs-variable">$i</span>/ost_<span class="hljs-variable">$i</span><br><span class="hljs-keyword">done</span><br></code></pre></td></tr></table></figure>

<h3 id="Enable-large-dir-feature"><a href="#Enable-large-dir-feature" class="headerlink" title="Enable large_dir feature"></a>Enable large_dir feature</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ tune2fs -O large_dir /dev/nvme0n1p1<br>$ dumpe2fs -h /dev/nvme0n1p1 | grep feat<br>$ dumpe2fs 1.45.2.wc1 (27-May-2019)<br>Filesystem features:      has_journal ext_attr resize_inode dir_index filetype needs_recovery mmp flex_bg ea_inode dirdata large_dir sparse_super large_file huge_file uninit_bg dir_nlink quota<br>Journal features:         journal_incompat_revoke<br></code></pre></td></tr></table></figure>

<h3 id="set-stripe-size-for-tiny-files"><a href="#set-stripe-size-for-tiny-files" class="headerlink" title="set stripe size for tiny files"></a>set stripe size for tiny files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ lfs setstripe -S 65536 /tfs10<br></code></pre></td></tr></table></figure>

<h3 id="change-ipaddr"><a href="#change-ipaddr" class="headerlink" title="change ipaddr"></a>change ipaddr</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ tunefs.lfs --erase-params --mgsnode=<span class="hljs-variable">$new_mgs_ip</span>@tcp --servicenode=<span class="hljs-variable">$new_service_ip</span>@tcp0 --writeconf /dev/md4<br></code></pre></td></tr></table></figure>

<h3 id="Public"><a href="#Public" class="headerlink" title="Public"></a>Public</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">options lnet networks=tcp(bond0)<br>options ptlrpc ldlm_num_threads=16<br>options ptlrpc at_max=300<br>options ptlrpc at_min=50<br>options ptlrpc ldlm_enqueue_min=250<br><br><span class="hljs-comment"># ldlm_enqueue_min = max(2*net_latency, net_latency + quiescent_time) +\\ 2*service_time</span><br><span class="hljs-comment"># ldlm_enqueue_min = max(2*50, 50 + 140) + 2*50 = 50+140 + 100 = 290</span><br><br><span class="hljs-comment">#at_max The largest potential RPC timeout that a client can set is 2*at_max. By lowering at_max from 600 to 400 seconds we reduce the worst case I/O delay from 1200 seconds, or 20 minutes, to 800 seconds or just over 13 minutes.</span><br><span class="hljs-comment">#at_min The 40 second value factors into our calculation for an appropriate LDLM timeout as discussed in section LDLM Timeouts. Our recommendation for Lustre servers is also 40 seconds</span><br><span class="hljs-comment">#Adaptive Timeouts: In a Lustre file system servers keep track of the time it takes for RPCs to be completed</span><br><span class="hljs-comment">#The quiescent_time in this formula is to account for the time it takes all Lustre clients to reestablish connections with all Lustre targets following an HSN quiesce. We&#x27;ve experimentally determined an average time to be approximately 140 seconds, but it is possible that this value may vary based on different factors such as the number of Lustre clients, the number of Lustre targets, the number of Lustre file systems mounted on each client, etc. Thus, given an at_min of 40 seconds</span><br><br><br>options ptlrpc ldlm_enqueue_min=250<br><br><span class="hljs-comment"># readonly mount</span><br>$ mount.lfs <span class="hljs-variable">$zpool</span> /ost0 -o rdonly_dev<br></code></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="http://wiki.lfs.org/Lustre_Resiliency:_Understanding_Lustre_Message_Loss_and_Tuning_for_Resiliency#Tuning_Lustre_for_Resiliency">Understanding Lustre Message Loss and Tuning for Resiliency</a></p>
<a id="more"></a>

<h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment">#reports the amount of space this client has reserved for writeback cache with each OST</span><br>$ lctl get_param osc.*.cur_grant_bytes<br><br><span class="hljs-comment">#limit ldlm threads, ldlm threads will exhaust all CPUs resources like LU-7330</span><br>options ptlrpc <span class="hljs-attribute">ldlm_num_threads</span>=16<br><br><span class="hljs-comment"># Flush all of the metadata client (mdc) locks on this node</span><br>$ lctl set_param ldlm.namespaces.*mdc*.<span class="hljs-attribute">lru_size</span>=clear<br><br><span class="hljs-comment"># setstripe</span><br>$ lfs setstripe -S 4000M -c 50 /mnt/striped<br><br><span class="hljs-comment">#Monitor</span><br>$ lctl get_param  obdfilter.*OST0000*.stats<br><br><span class="hljs-comment"># re-compile the lfs 2.13.0 client, you must import openmpi PATH</span><br>$ <span class="hljs-builtin-name">export</span> <span class="hljs-attribute">PATH</span>=/usr/lib64/openmpi/bin/:$PATH<br>$ rpmbuild --rebuild --without servers  lfs-2.10.3-1.src.rpm<br>$ rpmbuild  --rebuild --without servers --with lnet-dlc  lfs-2.10.3-1.src.rpm<br>$ rpmbuild --rebuild --without lfs-tests lfs-2.10.3-1.src.rpm<br><br><span class="hljs-comment"># centos 7.7</span><br><span class="hljs-comment"># /root/rpmbuild/BUILD/lustre-2.10.8/lnet/klnds/o2iblnd/o2iblnd.h:69:27: fatal error: linux/pci-dma.h: No such file or directory</span><br><br><span class="hljs-comment"># copy the file from the old kernel</span><br>copy /usr/src/kernels/<span class="hljs-variable">$&#123;the old version kernel&#125;</span>/include/linux/pci-dma.h /usr/src/kernels/<span class="hljs-variable">$&#123;the centos 7.7 kernel&#125;</span>/include/linux/pci-dma.h<br><br><span class="hljs-comment"># New stupid bug when you compile lfs 2.10.3-1, if you are not export $PATH with openmpi, the compile will failed.</span><br><span class="hljs-keyword">If</span> you want it pass, I was clear /tmp/tmp.* rpmbuild <span class="hljs-keyword">not</span> help I guess maybe the old<span class="hljs-built_in"> config </span><span class="hljs-keyword">in</span> some tmpfs path.<br>after you reboot <span class="hljs-keyword">and</span> re-<span class="hljs-builtin-name">export</span> the env, the compile will be successful.<br><span class="hljs-comment">#Is real the realease production ? `too stupid` bug. just waste my time to type these words.</span><br>There is <span class="hljs-literal">no</span> test team <span class="hljs-keyword">in</span> lfs develop team, All<span class="hljs-built_in"> users </span>was the test team except you are going <span class="hljs-keyword">to</span> buy DDN.<br><br><span class="hljs-comment"># from source</span><br>$ ./configure --enable-client --disable-server <span class="hljs-attribute">--with-linux</span>=/usr/src/kernels/$(uname -r);make rpms/deps<br><span class="hljs-comment">## install in ubuntu 18.04</span><br>$ apt install uuid-dev libblkid-dev dietlibc-dev<br>$ apt install build-essential debhelper devscripts fakeroot kernel-wedge libudev-dev pciutils-dev<br>$ apt install module-assistant libreadline-dev dpatch libsnmp-dev quilt<br>$ apt install linux-headers-$(uname -r)<br>$ cd <span class="hljs-variable">$&#123;BUILDPATH&#125;</span>/lfs-release<br>$ git reset --hard &amp;&amp; git clean -dfx<br>$ sh autogen.sh<br>$ ./configure --disable-server <span class="hljs-attribute">--with-linux</span>=/usr/src/linux-headers-4.15.0-64-generic<br>$ make install<br>$ rm -rf  /lib/modules/4.15.0-64-generic/kernel/drivers/staging/lfs/<br>$ depmod -a<br><br><span class="hljs-comment">### set lfs client for a lot metadata ops</span><br><span class="hljs-comment">#restricting the number of locks kept on the client (10000 locks, 10 minutes age)</span><br>$ lctl set_param ldlm.namespaces.*.<span class="hljs-attribute">lru_size</span>=10000 ldlm.namespaces.*.<span class="hljs-attribute">lru_max_age</span>=600000<br><br><span class="hljs-comment"># check object server status</span><br>client $ lfs osts<br>client $ cat /proc/fs/lfs/lov/<span class="hljs-variable">$fsname</span>-clilov-fffff882037467800/target_obd<br>mds    $ cat /proc/fs/lfs/lov/<span class="hljs-variable">$fsname</span>-MDT0000-mdtlov/target_obd<br>client $ lctl get_param osc.*-OST*.active<br><br><span class="hljs-comment"># check object server status in my production env</span><br>$ (lfs osts | awk -F <span class="hljs-string">&#x27;[ :_]+&#x27;</span> <span class="hljs-string">&#x27;$0~/OST/&#123;print $2&#125;&#x27;</span> | <span class="hljs-keyword">while</span> read line; <span class="hljs-keyword">do</span> grep <span class="hljs-string">&quot;FULL&quot;</span> /proc/fs/lfs/osc/<span class="hljs-variable">$&#123;line&#125;</span>-*/state &gt;/dev/<span class="hljs-literal">null</span> 2&gt;&amp;1 || echo -e <span class="hljs-string">&quot;Got these bad OSTs:&quot;</span> <span class="hljs-variable">$&#123;RED&#125;</span><span class="hljs-variable">$line</span><span class="hljs-variable">$&#123;NC&#125;</span> ; done) &amp;&amp; exit 0<br><br><span class="hljs-comment"># show ost ip  addr</span><br>$ lctl dl -t<br><br><span class="hljs-comment"># list nids</span><br>$ lctl lst_nids<br>$ lctl which_nid <span class="hljs-variable">$your_ipaddr</span>@tcp<br>$ lctl<span class="hljs-built_in"> ping </span><span class="hljs-variable">$your_ipaddr</span>@tcp <br><br><span class="hljs-comment"># mount namespace &quot;D&quot; and clear mds info from client</span><br>client $ echo 0 &gt; cat /sys/fs/lfs/mdc/<span class="hljs-variable">$FNAME</span>-*/active<br>client $ lctl set_param mdc.<span class="hljs-variable">$FNAME</span>-*.<span class="hljs-attribute">active</span>=0<br><br><span class="hljs-comment"># lustre rpc</span><br>$ lctl get_param mdc.*.max_rpcs_in_flight<br>mdc.test0-MDT0000-mdc-ffff95067b45a000.<span class="hljs-attribute">max_rpcs_in_flight</span>=8<br>$ lctl set_param mdc.*.<span class="hljs-attribute">max_rpcs_in_flight</span>=16<br>mdc.test0-MDT0000-mdc-ffff95067b45a000.<span class="hljs-attribute">max_rpcs_in_flight</span>=16<br>$ cat /sys/module/mdt/parameters/max_mod_rpcs_per_client<br><br><span class="hljs-comment"># mds</span><br>$ cat /sys/module/mdt/parameters/max_mod_rpcs_per_client<br>8<br>$ echo 16 &gt; /sys/module/mdt/parameters/max_mod_rpcs_per_client<br><br><span class="hljs-comment"># oss, lfs is extended to support RPCs up to 16MB in size</span><br>$ lctl get_param obdfilter.test0-OST*.brw_size<br>obdfilter.test0-OST0000.<span class="hljs-attribute">brw_size</span>=1<br>$ lctl set_param obdfilter.fsname-OST*.<span class="hljs-attribute">brw_size</span>=16<br></code></pre></td></tr></table></figure>

<h3 id="metadata-server"><a href="#metadata-server" class="headerlink" title="metadata server"></a>metadata server</h3><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs gams"># Get <span class="hljs-keyword">all</span> client info from mds<br><span class="hljs-symbol">$</span> cat /proc/fs/lfs/nodemap/default/exports<br><br># <span class="hljs-keyword">No</span> root squash<br>mds <span class="hljs-symbol">$</span> lctl set_param <span class="hljs-symbol">$</span>fsname.mdt.root_squash=<span class="hljs-number">108</span>:<span class="hljs-number">108</span><br>error: set_param: param_path <span class="hljs-string">&#x27;$fsname/mdt/root_squash&#x27;</span>: <span class="hljs-keyword">No</span> such <span class="hljs-keyword">file</span> <span class="hljs-keyword">or</span> directory<br>mds <span class="hljs-symbol">$</span> lctl conf_param <span class="hljs-symbol">$</span>fsname.mdt.root_squash=<span class="hljs-number">108</span>:<span class="hljs-number">108</span><br>mds <span class="hljs-symbol">$</span> lctl conf_param <span class="hljs-symbol">$</span>fsname.mdt.nosquash_nids=<span class="hljs-string">&quot;ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp&quot;</span><br>mds <span class="hljs-symbol">$</span> cat /proc/fs/lf/mdt/<span class="hljs-symbol">$</span>FNAME-MDT0000/nosquash_nids<br>ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp<br><br>client <span class="hljs-symbol">$</span> lctl set_param llite.<span class="hljs-symbol">$</span>FNAME-*.nosquash_nids=<span class="hljs-string">&quot;ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp&quot;</span><br><br># MDS to totally avoid new object creation on that OST<br><span class="hljs-symbol">$</span> lctl set_param osp.<span class="hljs-symbol">$</span>FNAME-OST00XX.max_create_count=<span class="hljs-number">0</span><br><br>#degrade will only prefer to skip the OST<br><span class="hljs-symbol">$</span> lctl set_param obdfilter.<span class="hljs-symbol">$</span>FNAME-OST00XX.degraded=<span class="hljs-number">1</span><br><br><span class="hljs-symbol">$</span> lctl set_param -P timeout=<span class="hljs-number">300</span><br><span class="hljs-symbol">$</span> lctl set_param timeout=<span class="hljs-number">300</span> <br># <span class="hljs-keyword">if</span> you want it work ,you have <span class="hljs-keyword">set</span> it <span class="hljs-comment">twice...</span><br><br># Monitor <span class="hljs-comment">status</span><br>$ cat /proc/<span class="hljs-comment">fs</span>/lfs/<span class="hljs-comment">mdc</span>/$&#123;fname&#125;-MDT0000-mdc-ffff88091eff0800/<span class="hljs-comment">state</span> <br>$ lctl <span class="hljs-comment">get_param mdc.$fname-MDT*.state</span><br>$ lctl <span class="hljs-comment">get_param mdc.$fname-MDT0000-mdc-*.rpc_stats</span><br><br>$ watch <span class="hljs-comment">-d lctl get_param mdt.*.md_stats</span><br>snapshot_time             <span class="hljs-comment">1556726087.189561170 secs.nsecs</span><br>open                      <span class="hljs-comment">3412130101 samples [reqs]</span><br>close                     <span class="hljs-comment">2926922120 samples [reqs]</span><br>mknod                     <span class="hljs-comment">293730475 samples [reqs]</span><br>link                      <span class="hljs-comment">20713305 samples [reqs]</span><br>unlink                    <span class="hljs-comment">316042257 samples [reqs]</span><br>mkdir                     <span class="hljs-comment">3275032 samples [reqs]</span><br>rmdir                     <span class="hljs-comment">2731821 samples [reqs]</span><br>rename                    <span class="hljs-comment">7687699 samples [reqs]</span><br>getattr                   <span class="hljs-comment">2060900881 samples [reqs]</span><br>setattr                   <span class="hljs-comment">320658776 samples [reqs]</span><br>getxattr                  <span class="hljs-comment">1080139037 samples [reqs]</span><br>setxattr                  <span class="hljs-comment">222105 samples [reqs]</span><br>statfs                    <span class="hljs-comment">11587278 samples [reqs]</span><br>sync                      <span class="hljs-comment">20670980 samples [reqs]</span><br>samedir_rename            <span class="hljs-comment">7199107 samples [reqs]</span><br>crossdir_rename           <span class="hljs-comment">488592 samples [reqs]</span><br></code></pre></td></tr></table></figure>

<h3 id="object-server"><a href="#object-server" class="headerlink" title="object server"></a>object server</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># osd_sync_destroy_max_size &quot;Maximum object size to use synchronous destroy</span><br><span class="hljs-string">options</span> <span class="hljs-string">osd_zfs</span> <span class="hljs-string">osd_sync_destroy_max_size=1048576</span><br><br><span class="hljs-string">options</span> <span class="hljs-string">ost</span> <span class="hljs-string">oss_num_threads=0</span><br><span class="hljs-comment"># you can limit the server performance by num_threads </span><br><br><span class="hljs-comment">#limit ost io threads</span><br><span class="hljs-string">$</span> <span class="hljs-string">lctl</span> <span class="hljs-string">set_param</span> <span class="hljs-string">ost.OSS.ost_io.threads_max=128</span><br><span class="hljs-string">$</span> <span class="hljs-string">lctl</span> <span class="hljs-string">set_param</span> <span class="hljs-string">-P</span> <span class="hljs-string">ost.OSS.ost_io.threads_max=128</span><br><br><span class="hljs-comment"># rpc info</span><br><span class="hljs-string">$</span> <span class="hljs-string">cat</span> <span class="hljs-string">/proc/fs/lfs/osc/lfs-OST0000-osc-ffff88103a993000/import</span><br><br><span class="hljs-comment"># Get status</span><br><span class="hljs-string">$</span> <span class="hljs-string">lctl</span> <span class="hljs-string">get_param</span> <span class="hljs-string">osc.*OST0000*.&#123;state,timeouts&#125;</span><br><span class="hljs-string">$</span> <span class="hljs-string">lctl</span> <span class="hljs-string">get_param</span> <span class="hljs-string">at_*</span> <span class="hljs-string">timeout</span><br><span class="hljs-string">$</span> <span class="hljs-string">lctl</span> <span class="hljs-string">get_param</span> <span class="hljs-string">llite.*$FNAME*.stats</span><br><span class="hljs-comment">### clear stats, lctl set_param llite.*.stats=c</span><br><span class="hljs-string">$</span> <span class="hljs-string">lctl</span> <span class="hljs-string">get_param</span> <span class="hljs-string">obdfilter.*OST005e*.brw_stats</span><br><br><span class="hljs-comment">#Read and print the last_rcvd file from a device</span><br><span class="hljs-comment">#display client information</span><br><span class="hljs-string">$</span> <span class="hljs-string">lr_reader</span> <span class="hljs-string">-c</span> <span class="hljs-string">/dev/sdh</span><br><span class="hljs-attr">last_rcvd:</span><br><span class="hljs-attr">uuid:</span> <span class="hljs-string">fsms-MDT0000_UUID</span><br> <span class="hljs-attr">feature_compat:</span> <span class="hljs-number">0x8</span><br> <span class="hljs-attr">feature_incompat:</span> <span class="hljs-number">0x61c</span><br> <span class="hljs-attr">feature_rocompat:</span> <span class="hljs-number">0x1</span><br> <span class="hljs-attr">last_transaction:</span> <span class="hljs-number">4294967298</span><br> <span class="hljs-attr">target_index:</span> <span class="hljs-number">0</span><br> <span class="hljs-attr">mount_count:</span> <span class="hljs-number">1</span><br> <span class="hljs-attr">client_area_start:</span> <span class="hljs-number">8192</span><br> <span class="hljs-attr">client_area_size:</span> <span class="hljs-number">128</span><br> <span class="hljs-attr">79136f3b-7d85-e265-37aa-dbb40ec5a30c:</span><br> <span class="hljs-attr">generation:</span> <span class="hljs-number">2</span><br> <span class="hljs-attr">last_transaction:</span> <span class="hljs-number">0</span><br> <span class="hljs-attr">last_xid:</span> <span class="hljs-number">0</span><br> <span class="hljs-attr">last_result:</span> <span class="hljs-number">0</span><br> <span class="hljs-attr">last_data:</span> <span class="hljs-number">0</span><br><span class="hljs-comment">#display reply data information</span><br><span class="hljs-string">$</span> <span class="hljs-string">lr_reader</span> <span class="hljs-string">-r</span> <span class="hljs-string">/dev/sdh</span><br><span class="hljs-string">...</span><br><span class="hljs-attr">reply_data:</span><br> <span class="hljs-attr">0:</span><br> <span class="hljs-attr">client_generation:</span> <span class="hljs-number">2</span><br> <span class="hljs-attr">last_transaction:</span> <span class="hljs-number">4426736549</span><br> <span class="hljs-attr">last_xid:</span> <span class="hljs-number">1511845291497772</span><br> <span class="hljs-attr">last_result:</span> <span class="hljs-number">0</span><br> <span class="hljs-attr">last_data:</span> <span class="hljs-number">0</span><br> <span class="hljs-attr">1:</span><br> <span class="hljs-attr">client_generation:</span> <span class="hljs-number">2</span><br> <span class="hljs-attr">last_transaction:</span> <span class="hljs-number">4426736566</span><br> <span class="hljs-attr">last_xid:</span> <span class="hljs-number">1511845291498048</span><br> <span class="hljs-attr">last_result:</span> <span class="hljs-number">0</span><br> <span class="hljs-attr">last_data:</span> <span class="hljs-number">0</span><br><br><span class="hljs-comment"># disable ost cache</span><br><span class="hljs-string">$</span> <span class="hljs-string">lctl</span> <span class="hljs-string">get_param</span> <span class="hljs-string">osd-ldiskfs.*.read_cache_enable</span><br><span class="hljs-string">$</span> <span class="hljs-string">lctl</span> <span class="hljs-string">get_param</span> <span class="hljs-string">ldlm.namespaces.*.lru_size</span><br><br><span class="hljs-comment">#make sure ost mount parameters and flag</span><br><span class="hljs-string">$</span> <span class="hljs-string">cat</span> <span class="hljs-string">/proc/fs/ldiskfs/dm-xx/options</span><br><span class="hljs-string">rw</span><br><span class="hljs-string">barrier</span><br><span class="hljs-string">no_mbcache</span><br><span class="hljs-string">user_xattr</span><br><span class="hljs-string">acl</span><br><span class="hljs-string">resuid=0</span><br><span class="hljs-string">resgid=0</span><br><span class="hljs-string">errors=remount-ro</span><br><span class="hljs-string">commit=5</span><br><span class="hljs-string">min_batch_time=0</span><br><span class="hljs-string">max_batch_time=15000</span><br><span class="hljs-string">stripe=0</span><br><span class="hljs-string">data=ordered</span><br><span class="hljs-string">inode_readahead_blks=32</span><br><span class="hljs-string">init_itable=10</span><br><span class="hljs-string">max_dir_size_kb=0</span><br></code></pre></td></tr></table></figure>

<h3 id="net"><a href="#net" class="headerlink" title="net"></a>net</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros">options lnet <span class="hljs-attribute">networks</span>=<span class="hljs-string">&quot;tcp0(myri10ge),tcp1(eth0)&quot;</span><br>lctl<br>lctl &gt;<span class="hljs-built_in"> network </span>tcp1<br>lctl &gt; peer_list<br>12345-xx.xx.xx.xx@tcp [0]0.0.0.0-&gt;0.0.0.0:0 #0<br></code></pre></td></tr></table></figure>

<p>I think in some the bad network quality env, you must improve them</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#Peer Credits</span><br><span class="hljs-comment">#Governs the number of concurrent sends to a single peer, End-to-end flow control accomplished at higher layer. e.g. max_rpcs_in_flight</span><br><br>$ cat /proc/sys/lnet/peers <br>nid                      refs state  last   max   rtr   min    tx   min queue <br>xx.xx.xx.xx@o2ib            3    up    -1   126   126   126   126   110 0<br>tx is the number of peer credits currently available <span class="hljs-keyword">for</span> this peer<br>min is the smallest number of peer credits seen <br>Negative credit count indicates the number of messages awaiting a credit<br><br><span class="hljs-comment">#Network Interface Credits</span><br>$ cat /proc/sys/lnet/nis<br>nid                      status alive refs peer  rtr   max    tx   min <br>xx.xx.xx.xx@o2ib              up    -1    9  126    0  2048  2048  1796 <br>max is total available (i.e. value of ko2iblnd credits) <br>tx is the number currently available, Negative number indicates number of messages awaiting a credit<br>min is the low water mark<br><br><span class="hljs-comment">#Lctl conn_list–List active TCP connections, type (bulk/control), tx_buffer_size, rx_buffer_size</span><br>$ lctl --net tcp conn_list<br><br>chmod a+w /sys/module/ksocklnd/parameters/peer_timeout /sys/module/ksocklnd/parameters/peer_credits /sys/module/ksocklnd/parameters/credits<br><span class="hljs-built_in">echo</span> 1024 &gt; /sys/module/ksocklnd/parameters/credits<br><span class="hljs-comment"># the number of concurrent sends (to all peers), defaults:64</span><br><br><span class="hljs-built_in">echo</span> 128 &gt; /sys/module/ksocklnd/parameters/peer_timeout<br>peer_buffer_credits=256<br>concurrent_sends=256 - send work-queue sizing<br><span class="hljs-comment"># not test</span><br><br><span class="hljs-built_in">echo</span> 128 &gt; /sys/module/ksocklnd/parameters/peer_credits<br>the number of concurrent sends to a single peer, <span class="hljs-comment">#default:8</span><br><br>chmod a+w /sys/module/lnet/parameters/accept_backlog  /sys/module/lnet/parameters/accept_timeout<br><span class="hljs-built_in">echo</span> 128 &gt; /sys/module/lnet/parameters/accept_timeout<br><span class="hljs-built_in">echo</span> 2000 &gt; /sys/module/lnet/parameters/accept_backlog<br><br>options lnet accept_backlog=2000<br><span class="hljs-comment">## Acceptor&#x27;s listen backlog, the number of the connections the server instance can buffer in the wait queue.</span><br>options lnet accept_timeout=128<br><span class="hljs-comment">## Specifies the number of seconds the server waits for data to arrive from the client. If data does not arrive before the timeout expires then the connection is closed. By setting it to less than the default 30 seconds, you can free up threads sooner. However, you may also disconnect users with slower connections.</span><br>k<br><br>$ lctl get_param osc.*.max_pages_per_rpc<br>$ lctl set_param osc.*.max_pages_per_rpc=1024 <span class="hljs-comment"># 1024 = 1024*4KB =4MB per RPC</span><br><span class="hljs-comment">#Max RPCS in flight between OSC and OST</span><br>$ lctl set_param -P <span class="hljs-variable">$FNAME</span>.osc.max_pages_per_rpc=1024<br><br>$ lctl set_param osc.*.max_rpcs_in_flight=64;<br><span class="hljs-comment"># Max number of 4K pages per RPC</span><br><span class="hljs-comment"># Increase for small IO or long fast network paths (high BDP), May want to decrease to preempt TCP congestion</span><br><br>256 = 1MB per RPC<br><span class="hljs-comment"># max_pages_per_rpc*4*max_rpcs_in_flight*2=max_dirty_mb</span><br>1024*4KB/1024(KB to MB)*64*2=512<br>lctl set_param osc.*.max_dirty_mb=512<br><span class="hljs-comment"># Maximum MBs of dirty data that can be written and queued on a client</span><br><br>Set per OST or each clients<br>256*4/1024*64*2=128<br>lctl set_param osc.*.max_pages_per_rpc=256; lctl set_param osc.*.max_rpcs_in_flight=64;lctl set_param osc.*.max_dirty_mb=128<br><br><br>$ modinfo mdt | grep max_mod_rpcs_per_client<br>parm:           max_mod_rpcs_per_client:maximum number of modify RPCs <span class="hljs-keyword">in</span> flight allowed per client (uint)<br><br>mds $ <span class="hljs-built_in">echo</span> 16 &gt; /sys/module/mdt/parameters/max_mod_rpcs_per_client<br><br><span class="hljs-comment"># config</span><br>lctl network up/down<br>lctl list_nids<br>lctl ping xxxxx@tcp<br>lctl network unconfigure<br><span class="hljs-comment">### from lfs 2.7</span><br>lnetctl lnet configure/unconfigure<br>lnetctl net show --verbose<br>lnetctl net add --net LNET --<span class="hljs-keyword">if</span> eth0<br>lnetctl net del --net LNET<br><br><span class="hljs-comment">### Lnet multiple-plane</span><br>options lnet networks=<span class="hljs-string">&quot;tcp1(eth1),tcp2(eth2),o2ib0(ib0)&quot;</span><br><span class="hljs-comment">###</span><br>options lnet ip2nets=<span class="hljs-string">&quot;tcp1(eth0) 192.168.0.[2,4] \</span><br><span class="hljs-string"> tcp1 192.168.0.*; o2ib1 132.6.[1-3],[2-8/2]&quot;</span><br><span class="hljs-comment">### [2-8/2] means 2,4,6,8</span><br><br></code></pre></td></tr></table></figure>

<h3 id="Trace-log"><a href="#Trace-log" class="headerlink" title="Trace log"></a>Trace log</h3><h4 id="lfs-log"><a href="#lfs-log" class="headerlink" title="lfs log"></a>lfs log</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># F_SETPIPE_SZ F_GETPIPE_SZ</span><br><span class="hljs-comment"># echo 104857600 &gt; /proc/sys/fs/pipe-max-size</span><br><span class="hljs-comment"># fs.pipe-max-size=104857600</span><br><span class="hljs-comment"># ulimit pipe size            (512 bytes, -p) 8 = 4096 bytes, it &#x27;s pipe buffer size in the ulimit, not pipe size</span><br><span class="hljs-comment">## POSIX.1-2001 says that write(2)s of less than PIPE_BUF  bytes  must  be atomic:  the  output  data  is  written  to  the  pipe  as a contiguous sequence.  Writes of more than PIPE_BUF bytes may  be  non-atomic:  the kernel  may  interleave the data with data written by other processes.</span><br><br><span class="hljs-comment"># here 2 tools for pipe</span><br><span class="hljs-comment"># pv - Pipe Viewer - is a terminal-based tool for monitoring the progress of data through a pipeline.</span><br><span class="hljs-comment"># process1 | pv -pterbTCB 1G | process2</span><br><br>$ pv -cN sources linux-image-unsigned-4.15.0-65-generic-dbgsym_4.15.0-65.74_amd64.ddeb | dd of=/tmp/tmp bs=512 | pv -cN cat<br>      cat: 0.00 B 0:00:00 [0.00 B/s] [&lt;=&gt;                                                                                                                                                                                                    ]<br>  sources:  751MiB 0:00:03 [ 191MiB/s] [===================================================================================================================================================================================&gt;] 100%            <br>1538323+1 records <span class="hljs-keyword">in</span><br>1538323+1 records out<br>787621648 bytes (788 MB, 751 MiB) copied, 3.92424 s, 201 MB/s<br><br><br>$ mkfifo -m 777 /tmp/lfs.log<br>$ lctl debug_daemon start /tmp/lfs.log<br><br><br>$ trace-cmd record -p <span class="hljs-keyword">function</span> mount <span class="hljs-variable">$ipaddr</span>@tcp:/lfs /mnt<br><span class="hljs-comment"># it &#x27;ll create trace.dat</span><br>$ trace-cmd report<br></code></pre></td></tr></table></figure>

<h4 id="kdump"><a href="#kdump" class="headerlink" title="kdump"></a>kdump</h4><p>yum -y install kexec-tools<br>cat /etc/kdump.conf<br>nfs my.nfsserver.example.org:/path/to/expor<br>core_collector makedumpfile -d 16 -c<br>#-c Compress dump data by each page<br>#core_collector makedumpfile -d 16 -c message_level 16</p>
<h1 id="1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages"><a href="#1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages" class="headerlink" title="1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages"></a>1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages</h1><h1 id="1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages"><a href="#1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages" class="headerlink" title="1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages"></a>1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages</h1><p>#ssh <a href="mailto:&#x75;&#115;&#101;&#114;&#64;&#109;&#x79;&#46;&#115;&#101;&#114;&#x76;&#101;&#114;&#46;&#x65;&#x78;&#x61;&#109;&#x70;&#x6c;&#x65;&#46;&#x6f;&#x72;&#103;">&#x75;&#115;&#101;&#114;&#64;&#109;&#x79;&#46;&#115;&#101;&#114;&#x76;&#101;&#114;&#46;&#x65;&#x78;&#x61;&#109;&#x70;&#x6c;&#x65;&#46;&#x6f;&#x72;&#103;</a>:/dest/path<br>#By default, uses ssh key at /root/.ssh/kdump_id_rsa<br>#core_collector makedumpfile <options></p>
<h3 id="changelog"><a href="#changelog" class="headerlink" title="changelog"></a>changelog</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">* MARK – Internal record keeping<br>* CREAT – Regular file creation<br>* MKDIR – Directory creation<br>* HLINK – Hard link<br>* SLINK – Soft link<br>* OPEN – open file<br>* CLOSE – close file<br>* MKNOD – Other file creation<br>* UNLNK – Regular file removal<br>* RMDIR – Directory removal<br>* RNMFM – Rename, original<br>* RNMTO – Rename, final<br>* IOCTL – ioctl on file or directory<br>* TRUNC – Regular file truncated<br>* SATTR – Attribute change<br>* XATTR – Extended Attribute change<br>* HSM – HSM action<br>* UNKNW – Unkown operation<br></code></pre></td></tr></table></figure>

<h4 id="Enable"><a href="#Enable" class="headerlink" title="Enable"></a>Enable</h4><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs ruby">$ lctl set_param mdt.$FNAME-MDT000<span class="hljs-number">0</span>.hsm_control=enabled<br>$ lctl set_param -P  mdt.$FNAME-MDT000<span class="hljs-number">0</span>.hsm_control=enabled<br>$ lctl set_param mdt.$FNAME-MDT000<span class="hljs-number">0</span>.hsm.max_requests=<span class="hljs-number">8</span><br><br><span class="hljs-comment"># create user</span><br>$ lctl --device $FNAME-MDT000<span class="hljs-number">0</span> changelog_register<br><span class="hljs-comment"># del user</span><br>$ lctl --device fsname-MDT000<span class="hljs-number">0</span> changelog_deregister cl1<br><span class="hljs-comment"># Get the size</span><br>$ lctl get_param mdd.$FNAME-MDT000<span class="hljs-number">0</span>.changelog_users mdd.$FNAME-MDT000<span class="hljs-number">0</span>.changelog_size<br><br><span class="hljs-comment"># changelog mask</span><br>$ lctl set_param mdd.$FNAME-MDT*.changelog_mask=MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RNMFM RNMTO OPEN LYOUT TRUNC CLOSE IOCTL TRUNC SATTR XATTR HSM MTIME CTIME<br>$ lctl get_param mdd.$FNAME-MDT*.changelog_mask<br>MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RENME RNMTO OPEN LYOUT TRUNC SATTR XATTR HSM MTIME CTIME<br><br><span class="hljs-comment"># Get the changelog</span><br>$ lfs changelog $FNAME-MDT000<span class="hljs-number">0</span> &gt; lfs-changelog<br>$ fs changelog $fsname-MDT000<span class="hljs-number">0</span> [startrec [endrec]]<br><br><span class="hljs-comment"># clear all</span><br>$ lctl changelog_clear mdt_name userid endrec<br><span class="hljs-string">``</span><span class="hljs-string">``</span><br><br><span class="hljs-comment">#### Disable </span><br></code></pre></td></tr></table></figure>
<p>#Notify a device that user cl1 no longer needs records (up toand including 3)<br>$ lfs changelog_clear $FNAME-MDT0000 cl1 3</p>
<p>#To stop changelogs, changelog_mask should be set to MARK only<br>$ lctl set_param mdd.$FNAME-MDT0000.changelog_mask=MARK<br>mdd.lfs-MDT0000.changelog_mask=MARK</p>
<p>#or youcan set it -all<br>$ lctl set_param mdd.$FNAME-MDT0000.changelog_mask=-all</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs angelscript"><br>### FSCK<br>```bash<br>Dec <span class="hljs-number">29</span> <span class="hljs-number">14</span>:<span class="hljs-number">11</span>:<span class="hljs-number">32</span> mookie kernel: LDISKFS-fs error (device sdz): ldiskfs_lookup: unlinked inode <span class="hljs-number">5384166</span> <span class="hljs-keyword">in</span> dir #<span class="hljs-number">145170469</span><br>Dec <span class="hljs-number">29</span> <span class="hljs-number">14</span>:<span class="hljs-number">11</span>:<span class="hljs-number">32</span> mookie kernel: Remounting filesystem read-only<br></code></pre></td></tr></table></figure>

<h4 id="Flush-the-journal"><a href="#Flush-the-journal" class="headerlink" title="Flush the journal"></a>Flush the journal</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ umount /lfs<br>$ mount -t ldiskfs /dev/sdx /lfs<br>$ umount /lfs<br></code></pre></td></tr></table></figure>

<ul>
<li><p>Ensure e2fsprogs version ,it ‘s not default linux version ,it ‘s lfs version</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">rpm -qa | grep e2fsprogs<br>e2fsprogs-1.42.12.wc1-7.el6.x86_64<br>e2fsprogs-libs-1.42.12.wc1-7.el6.x86_64<br></code></pre></td></tr></table></figure>
</li>
<li><p>Before fsck，make sure the mount point has been <font color=red>umount</font></p>
</li>
<li><p>Can check multiple MDT/OSTs in parallel</p>
</li>
</ul>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># Check only mode</span><br>$ e2fsck -fn <span class="hljs-regexp">/dev/</span>sdx<br><br><span class="hljs-comment"># Prudent mode</span><br>$ e2fsck -fp <span class="hljs-regexp">/dev/</span>sdx<br><br><span class="hljs-comment"># Answer yes</span><br>$ e2fsck -fy <span class="hljs-regexp">/dev/</span>sdx<br></code></pre></td></tr></table></figure>

<h4 id="re-writeconf"><a href="#re-writeconf" class="headerlink" title="re-writeconf"></a>re-writeconf</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">mds$ tunefs.lfs --writeconf /dev/sdx<br>oss$ tunefs.lfs --writeconf /dev/ost0<br></code></pre></td></tr></table></figure>
<p>If MGS and MDT in single block device, you can add “-o nosvc” to avoid mount MDT</p>
<h3 id="User-group-quota"><a href="#User-group-quota" class="headerlink" title="User group quota"></a>User group quota</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">## you can &#x27;t use lctl set_param, it &#x27;s not work</span><br><br>mds $ lctl conf_param <span class="hljs-variable">$FNAME</span>.quota.mdt=ug<br>mds $ cat /proc/fs/lfs/osd-ldiskfs/<span class="hljs-variable">$FNAME</span>-MDT0000/quota_slave/info<br>mds $ lctl get_param osd-*.*.quota_slave.info<br>quota enabled:  <span class="hljs-string">&quot;ug&quot;</span><br>mds $ lctl conf_param <span class="hljs-variable">$FNAME</span>.quota.ost=ug<br><br><span class="hljs-comment">### lctl set_param -P must reboot, no -P not work, if you don&#x27; t reboot ,you have too use conf_param</span><br><br>client $ lfs setquota -u user1 -b 307200 -B 309200 -i 10000 -I 11000 /mnt/lfs<br>client $ lfs setquota -g group1 -b 5120000 -B 5150000 -i 100000 -I 101000 /mnt/lfs<br><br>client $ lfs quota –u user1 -v /mnt/lfs<br>client $ lfs quota -t -p /mnt/lfs<br>Block grace time: 1w; Inode grace time: 1w<br></code></pre></td></tr></table></figure>

<h3 id="Disable-the-ost"><a href="#Disable-the-ost" class="headerlink" title="Disable the ost"></a>Disable the ost</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">mds $ mds lctl dl | grep osc<br>8 UP osp lfs-OST0000-osc-MDT0000 lfs-MDT0000-mdtlov_UUID 5<br><br>mds $ lctl --device 8 deactivate<br>mds $ lctl --device 8 activate<br></code></pre></td></tr></table></figure>

<h3 id="Skip-recovery"><a href="#Skip-recovery" class="headerlink" title="Skip recovery"></a>Skip recovery</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">mds</span> $ mds lctl dl | grep osc<br><span class="hljs-attribute">8</span> UP osp lfs-OST<span class="hljs-number">0000</span>-osc-MDT<span class="hljs-number">0000</span> lfs-MDT<span class="hljs-number">0000</span>-mdtlov_UUID <span class="hljs-number">5</span><br><br><span class="hljs-attribute">mds</span> $ lctl --device <span class="hljs-number">8</span> abort_recovery<br><br><span class="hljs-attribute">or</span> <br><br><span class="hljs-attribute">mount</span>.lfs xxx xxx -o abort_recov<br></code></pre></td></tr></table></figure>

<h3 id="lfs-migarate"><a href="#lfs-migarate" class="headerlink" title="lfs migarate"></a>lfs migarate</h3><p>Strong not recommand this command, because the command will cause loss the data, I suggest you copy data by index and checksum the copy file</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ lfs setstripe -c 1  -i 4 /lfs/dir1<br>$ copy /lfs/old_dir1/file1 /lfs/dir1<br>$ md5sum /lfs/old_dir1/file1 /lfs/dir1/file1<br><br><span class="hljs-comment"># dont &#x27;t use lfs migrate, it &#x27;s too dangerous, it will cause data loss</span><br><span class="hljs-comment">## lfs find /opt/lfswh -obd lfswh-OST000c_UUID -size +4G | lfs_migrate -y</span><br><span class="hljs-comment">## lfs migrate -c 1  -i 4 filepath</span><br>``` <br><br><span class="hljs-comment">### Job status</span><br>```bash<br>client $  lctl get_param jobid_var<br>client $  jobid_var=<span class="hljs-built_in">disable</span><br><br>SLURM: jobid_var=SLURM_JOB_ID<br>SGE: jobid_var=JOB_ID<br>LSF: jobid_var=LSB_JOBID<br>Loadleveler: jobid_var=LOADL_STEP_ID<br>PBS: jobid_var=PBS_JOBID<br>Maui/MOAB: jobid_var=PBS_JOBID<br><span class="hljs-comment"># Enable for sge</span><br>mds $ lctl conf_param testfs.sys.jobid_var=JOB_ID<br><br><span class="hljs-comment"># disable</span><br>mds $ lctl conf_param testfs.sys.jobid_var=<span class="hljs-built_in">disable</span><br><br><span class="hljs-comment"># If there isn&#x27;t any job scheduler is running over the system, or user just want to collect the stats for process &amp; uid:</span><br>mds $ lctl conf_param testfs.sys.jobid_var=procname_uid<br><br><span class="hljs-comment"># Check Job status</span><br>oss $ lctl get_param obdfilter.testfs5-OST0004.job_stats<br>job_stats:<br>- job_id:          9158530<br>  snapshot_time:   1503038800<br>  read_bytes:      &#123; samples:           0, unit: bytes, min:       0, max:       0, sum:               0 &#125;<br>  write_bytes:     &#123; samples:       32452, unit: bytes, min:  262144, max: 1048576, sum:     34009513984 &#125;<br>  getattr:         &#123; samples:           0, unit:  reqs &#125;<br>  setattr:         &#123; samples:           0, unit:  reqs &#125;<br><br><span class="hljs-comment"># get mdt ops</span><br>mds $ lctl get_param mdt.*.job_stats<br>mds $ lctl get_param  mdt.testfs5-MDT0000.job_stats<br>mdt.testfs5-MDT0000.job_stats=<br>job_stats:<br>- job_id:          278685<br>  snapshot_time:   1503068243<br>  open:            &#123; samples:           0, unit:  reqs &#125;<br>  close:           &#123; samples:           0, unit:  reqs &#125;<br>  mknod:           &#123; samples:           0, unit:  reqs &#125;<br>  link:            &#123; samples:           0, unit:  reqs &#125;<br>  unlink:          &#123; samples:           0, unit:  reqs &#125;<br>  mkdir:           &#123; samples:           0, unit:  reqs &#125;<br><br><br><span class="hljs-comment"># clear stats for all job on testfs-OST0001</span><br>oss $ lctl set_param obdfilter.testfs-OST0001.job_stats=clear<br><br><span class="hljs-comment"># Clear stats for job &quot;dd.0&quot; on lfs-MDT0000</span><br>mds $ lctl set_param mdt.lfs-MDT0000.job_stats=dd.0<br><br><span class="hljs-comment"># cleanup interval (seconds)</span><br>lctl set_param -P testfs5.mdt.job_cleanup_interval=604800<br>lctl set_param  testfs5.mdt.job_cleanup_interval=604800<br>mds $  cat /proc/fs/lfs/mdt/testfs5-MDT0000/job_cleanup_interval<br></code></pre></td></tr></table></figure>

<h3 id="lfs-fid-and-path"><a href="#lfs-fid-and-path" class="headerlink" title="lfs fid and path"></a>lfs fid and path</h3><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">[client]<span class="hljs-comment"># lfs fid2path /mnt      [0x200000400:0x1:0x0]</span><br>                                       |<span class="hljs-string">         </span>|<span class="hljs-string">   </span>|<br>                                       |<span class="hljs-string">         </span>|<span class="hljs-string">   -- version</span><br><span class="hljs-string">                                       </span>|<span class="hljs-string">         ---- object id</span><br><span class="hljs-string">                                       ----------Sequence</span><br><span class="hljs-string">[client]# lfs path2fid /mnt</span><br><span class="hljs-string">[0x200000007:0x1:0x0]</span><br></code></pre></td></tr></table></figure>

<h3 id="increase-openzfs-sync-performance-in-test-env"><a href="#increase-openzfs-sync-performance-in-test-env" class="headerlink" title="increase openzfs sync performance in test env"></a>increase openzfs sync performance in test env</h3><p><code>this setting will cause data loss, if client roll back log failed</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">lctl set_param osd-zfs.*.osd_obj_sync_delay_us=0<br><br>osd_object_sync_delay_us<br>To improve fsync() performance until ZIL device,it is possible <span class="hljs-built_in">disable</span> the code <span class="hljs-built_in">which</span> causes Lustre to block waiting on a TXG to sync<br></code></pre></td></tr></table></figure>

<h3 id="Lustre-issues"><a href="#Lustre-issues" class="headerlink" title="Lustre issues"></a>Lustre issues</h3><h4 id="Don-‘t-use-openzfs-with-lfs-file-system"><a href="#Don-‘t-use-openzfs-with-lfs-file-system" class="headerlink" title="Don ‘t use openzfs with lfs file system"></a>Don ‘t use openzfs with lfs file system</h4><ul>
<li>No bility to support openzfs</li>
<li>If your zpool brain split, they will not show you any help</li>
<li>If you enable openzfs MMP, if single HDD will broken, your zpool have to suspend once<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/issues/7045">https://github.com/zfsonlinux/zfs/issues/7045</a><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/issues/7118">https://github.com/zfsonlinux/zfs/issues/7118</a></li>
<li>this issue impact from zfs 0.7 to 0.7.9, I ‘m not sure 0.8 has the same issue, the best way is disable MMP and use single link for your SAS devices</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Multipath-failed-cause"><a href="#Multipath-failed-cause" class="headerlink" title="Multipath failed cause"></a>Multipath failed cause</h4><ul>
<li>Found error log; <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><code class="hljs bash">Sep 22 03:16:00 lfs-node21 kernel: scsi 14:0:5:31: attempting task abort! scmd(ffff97ac51784a80)<br>Sep 22 03:16:00 lfs-node21 kernel: scsi 14:0:5:31: [sg64] tag<span class="hljs-comment">#76 CDB: Read(6) 08 00 02 61 01 00</span><br>Sep 22 03:16:00 lfs-node21 kernel: scsi target14:0:5: _scsih_tm_display_info: handle(0x000a), sas_address(0x500a098b4c66c014), phy(4)<br>Sep 22 03:16:00 lfs-node21 kernel: scsi target14:0:5: enclosurelogical id(0x51866da0a8a62200), slot(4)<br>Sep 22 03:16:00 lfs-node21 kernel: scsi target14:0:5: enclosure level(0x0000), connector name( 1   )<br>Sep 22 03:16:01 lfs-node21 systemd: Started Session 361698 of user root.<br>Sep 22 03:16:01 lfs-node21 systemd: Starting Session 361698 of user root.<br>Sep 22 03:16:09 lfs-node21 kernel: sd 14:0:5:6: attempting task abort! scmd(ffff97b2ae507b80)<br>Sep 22 03:16:09 lfs-node21 kernel: sd 14:0:5:6: [sdp] tag<span class="hljs-comment">#8 CDB: Write(16) 8a 00 00 00 00 07 c6 2a 89 e8 00 00 08 00 00 00</span><br>Sep 22 03:16:09 lfs-node21 kernel: scsi target14:0:5: _scsih_tm_display_info: handle(0x000a), sas_address(0x500a098b4c66c014), phy(4)<br>Sep 22 03:16:09 lfs-node21 kernel: scsi target14:0:5: enclosurelogical id(0x51866da0a8a62200), slot(4)<br>Sep 22 03:16:09 lfs-node21 kernel: scsi target14:0:5: enclosure level(0x0000), connector name( 1   )<br>Sep 22 03:16:30 lfs-node21 kernel: mpt3sas_cm3: Command Timeout<br>Sep 22 03:16:30 lfs-node21 kernel: mf:<span class="hljs-comment">#012#0110100000a 00000100 00000000 00001f00 00000000 00000000 00000000 00000000 #012#01100000000 00000000 00000000 00000000 0000004d</span><br>Sep 22 03:16:32 lfs-node21 kernel: scsi 13:0:0:31: attempting task abort! scmd(ffff97bc6aad1c00)<br>Sep 22 03:16:32 lfs-node21 kernel: scsi 13:0:0:31: [sg45] tag<span class="hljs-comment">#197 CDB: Read(6) 08 00 02 00 01 00</span><br>Sep 22 03:16:32 lfs-node21 kernel: scsi target13:0:0: _scsih_tm_display_info: handle(0x0009), sas_address(0x500a098b4c6c3014), phy(0)<br>Sep 22 03:16:32 lfs-node21 kernel: scsi target13:0:0: enclosurelogical id(0x51866da0a8a62100), slot(0)<br>Sep 22 03:16:32 lfs-node21 kernel: scsi target13:0:0: enclosure level(0x0000), connector name( 0   )<br>Sep 22 03:16:39 lfs-node21 kernel: sd 13:0:1:6: attempting task abort! scmd(ffff97b2ae506680)<br>Sep 22 03:16:39 lfs-node21 kernel: sd 13:0:1:6: [sdap] tag<span class="hljs-comment">#4 CDB: Write(16) 8a 00 00 00 00 07 c5 fd 27 10 00 00 08 00 00 00</span><br>Sep 22 03:16:39 lfs-node21 kernel: scsi target13:0:1: _scsih_tm_display_info: handle(0x000a), sas_address(0x500a098b4c5d9014), phy(4)<br>Sep 22 03:16:39 lfs-node21 kernel: scsi target13:0:1: enclosurelogical id(0x51866da0a8a62100), slot(4)<br>Sep 22 03:16:39 lfs-node21 kernel: scsi target13:0:1: enclosure level(0x0000), connector name( 1   )<br>Sep 22 03:16:40 lfs-node21 kernel: mpt3sas_cm3: sending diag reset !!<br>Sep 22 03:16:41 lfs-node21 kernel: mpt3sas_cm3: diag reset: SUCCESS<br>Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: IOC Number : 0<br>Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: CurrentHostPageSize is 0: Setting default host page size to 4k<br>Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: FW Package Version(16.17.00.03)<br>Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: LSISAS3008: FWVersion(16.00.04.00), ChipRevision(0x02), BiosVersion(18.00.00.00)<br>Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: Dell 12Gbps SAS HBA<br>Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: Protocol=(Initiator,Target), Capabilities=(TLR,EEDP,Snapshot Buffer,Diag Trace Buffer,Task Set Full,NCQ)<br>Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: sending port <span class="hljs-built_in">enable</span> !!<br><br>Sep 22 03:32:04 lfs-node21 multipathd: 3600a098000b4c3060000024359244375: sdbb - rdac checker reports path is down: inquiry failed<br>Sep 22 03:32:04 lfs-node21 multipathd: checker failed path 67:80 <span class="hljs-keyword">in</span> map 3600a098000b4c3060000024359244375<br>Sep 22 03:32:04 lfs-node21 multipathd: 3600a098000b4c3060000024359244375: remaining active paths: 1<br>Sep 22 03:32:04 lfs-node21 multipathd: 8:240: reinstated<br>Sep 22 03:32:04 lfs-node21 multipathd: sdbg: mark as failed<br>Sep 22 03:32:04 lfs-node21 multipathd: 3600a098000b4c66c0000020d59243745: remaining active paths: 1<br>Sep 22 03:32:04 lfs-node21 multipathd: sdbi: mark as failed<br>Sep 22 03:32:04 lfs-node21 multipathd: 3600a098000b4c66c00000213592437d2: remaining active paths: 1<br>Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdu, open() failed: No such device<br>Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdv, open() failed: No such device<br>Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdw, open() failed: No such device<br>Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdx, open() failed: No such device<br>Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdy, open() failed: No such device<br>Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdar, open() failed: No such device<br>Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdas, open() failed: No such device<br>Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdat, open() failed: No such device<br>Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdau, open() failed: No such device<br>Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdav, open() failed: No such device<br>Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdaw, open() failed: No such device<br>Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdax, open() failed: No such device<br>Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sday, open() failed: No such device<br>Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdaz, open() failed: No such device<br>Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c3060000024359244375: sdbb - rdac checker reports path is upa<br><br><span class="hljs-comment">### All devices single link offline at the same time, I have 4 x 9300-8E, impossible offline at the same time</span><br><span class="hljs-comment">### Filter all error devices and make sure just single HBA status not correct, and you can found mpt3sas_cm0 in the system message</span><br><br>Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c3060000024359244375: sdbb - rdac checker reports path is up<br>Sep 22 03:32:14 lfs-node21 kernel: device-mapper: multipath: Reinstating path 67:80.<br>Sep 22 03:32:14 lfs-node21 kernel: device-mapper: multipath: Reinstating path 67:160.<br>Sep 22 03:32:14 lfs-node21 multipathd: 67:80: reinstated<br>Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c3060000024359244375: remaining active paths: 2<br>Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c66c0000020d59243745: sdbg - rdac checker reports path is up<br>Sep 22 03:32:14 lfs-node21 multipathd: 67:160: reinstated<br>Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c66c0000020d59243745: remaining active paths: 2<br>Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c66c00000213592437d2: sdbi - rdac checker reports path is up<br>Sep 22 03:32:14 lfs-node21 kernel: device-mapper: multipath: Reinstating path 67:192.<br>Sep 22 03:32:14 lfs-node21 multipathd: 67:192: reinstated<br>Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c66c00000213592437d2: remaining active paths: 2<br><br>Sep 22 03:34:16 lfs-node21 kernel: scsi 1:0:0:31: attempting task abort! scmd(ffff97b265c7b2c0)<br>Sep 22 03:34:16 lfs-node21 kernel: scsi 1:0:0:31: [sg9] tag<span class="hljs-comment">#17 CDB: Read(6) 08 00 02 00 01 00</span><br>Sep 22 03:34:16 lfs-node21 kernel: scsi target1:0:0: _scsih_tm_display_info: handle(0x0009), sas_address(0x500a098b4c834014), phy(0)<br>Sep 22 03:34:16 lfs-node21 kernel: scsi target1:0:0: enclosurelogical id(0x51866da09fad6700), slot(0)<br>Sep 22 03:34:16 lfs-node21 kernel: scsi target1:0:0: enclosure level(0x0000), connector name( 0   )<br>Sep 22 03:34:46 lfs-node21 kernel: mpt3sas_cm0: Command Timeout<br>Sep 22 03:34:46 lfs-node21 kernel: mf:<span class="hljs-comment">#012#01101000009 00000100 00000000 00001f00 00000000 00000000 00000000 00000000 #012#01100000000 00000000 00000000 00000000 00000012</span><br>Sep 22 03:34:55 lfs-node21 kernel: sd 1:0:1:24: attempting task abort! scmd(ffff97a35d8c9500)<br>Sep 22 03:34:55 lfs-node21 kernel: sd 1:0:1:24: [sdk] tag<span class="hljs-comment">#42 CDB: Read(16) 88 00 00 00 00 09 1f 1d 30 00 00 00 00 08 00 00</span><br>Sep 22 03:34:55 lfs-node21 kernel: scsi target1:0:1: _scsih_tm_display_info: handle(0x000a), sas_address(0x500a098b4c810014), phy(4)<br>Sep 22 03:34:55 lfs-node21 kernel: scsi target1:0:1: enclosurelogical id(0x51866da09fad6700), slot(4)<br>Sep 22 03:34:55 lfs-node21 kernel: scsi target1:0:1: enclosure level(0x0000), connector name( 1   )<br>Sep 22 03:34:56 lfs-node21 kernel: mpt3sas_cm0: sending diag reset !!<br>Sep 22 03:34:57 lfs-node21 kernel: mpt3sas_cm0: diag reset: SUCCESS<br>Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: IOC Number : 0<br>Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: CurrentHostPageSize is 0: Setting default host page size to 4k<br>Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: FW Package Version(16.17.00.03)<br>Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: LSISAS3008: FWVersion(16.00.04.00), ChipRevision(0x02), BiosVersion(18.00.00.00)<br>Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: Dell 12Gbps SAS HBA<br>Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: Protocol=(Initiator,Target), Capabilities=(TLR,EEDP,Snapshot Buffer,Diag Trace Buffer,Task Set Full,NCQ)<br>Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: sending port <span class="hljs-built_in">enable</span> !!<br><br><span class="hljs-comment"># del the others</span><br><br>Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: port <span class="hljs-built_in">enable</span>: SUCCESS<br>Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="hljs-keyword">for</span> end-devices: start<br>Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:0: handle(0x0009), sas_address(0x500a098b4c834014), port: 255<br>Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:0: enclosure logical id(0x51866da09fad6700), slot(0)<br>Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:1: handle(0x000a), sas_address(0x500a098b4c810014), port: 255<br>Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:1: enclosure logical id(0x51866da09fad6700), slot(4)<br>Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: <span class="hljs-comment">#011break from _scsih_search_responding_sas_devices: ioc_status(0x0022), loginfo(0x310f0400)</span><br>Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="hljs-keyword">for</span> end-devices: complete<br>Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="hljs-keyword">for</span> end-devices: start<br>Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="hljs-keyword">for</span> PCIe end-devices: complete<br>Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="hljs-keyword">for</span> expanders: start<br>Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="hljs-keyword">for</span> expanders: complete<br>Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: removing unresponding devices: start<br>Sep 22 03:35:05 lfs-node21 kernel: scsi 1:0:0:31: task abort: SUCCESS scmd(ffff97b265c7b2c0)<br>Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: removing unresponding devices: sas end-devices<br>Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: removing unresponding devices: pcie end-devices<br>Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: removing unresponding devices: expanders<br>Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: removing unresponding devices: complete<br>Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: scan devices: start<br>Sep 22 03:35:05 lfs-node21 kernel: sd 1:0:0:13: attempting task abort! scmd(ffff97b37ad6a4c0)<br>Sep 22 03:35:05 lfs-node21 kernel: sd 1:0:0:13: [sdg] tag<span class="hljs-comment">#89 CDB: Inquiry 12 01 c9 00 30 00</span><br>Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:0: _scsih_tm_display_info: handle(0x0009), sas_address(0x500a098b4c834014), phy(0)<br>Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:0: enclosurelogical id(0x51866da09fad6700), slot(0)<br>Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:0: enclosure level(0x0000), connector name( 0   )<br></code></pre></td></tr></table></figure>
Single link offline, after reset the link attch back, and try to recovery the connnection</li>
</ul>
<p>But lfs has a <a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-10510">bug, LU-10510</a>, lfs will modify the value when it mount, but the designer not consider link/HBA/IO expander offline or some reason cause the value back to default value. what a pity, the operate (modify to 16383) only trigger on mount.lfs operation, the link back to recovery the connection ? no way.</p>
<p>It ‘s a simple logic. Why the desiger not consider the link offline ? I can ‘t understand. it must be an intern.<br>When the link recovery, the max_sectors_kb recovery to default value (512)<br>You have to use echo 16383 to improve it, why 16383, sorry , I don ‘t know.<br>Does the lfs system rigorous enough ? hahahahaahaha……………….<br>Does is it for Enterprise file system ? hahahahahahaha……………….</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ grep . /sys/block/*/queue/max_sectors_kb<br>/sys/block/sdak/queue/max_sectors_kb 16383<br>/sys/block/sdal/queue/max_hw_sectors_kb 16383<br>/sys/block/sdal/queue/max_sectors_kb 512<br>/sys/block/sda/queue/max_hw_sectors_kb 256<br>/sys/block/sda/queue/max_sectors_kb 256<br>/sys/block/sdba/queue/max_hw_sectors_kb 16383<br>/sys/block/sdba/queue/max_sectors_kb 16383<br>/sys/block/sdbb/queue/max_hw_sectors_kb 16383<br>/sys/block/sdbb/queue/max_sectors_kb 16383<br>/sys/block/sdbc/queue/max_hw_sectors_kb 16383<br>/sys/block/sdbc/queue/max_sectors_kb 512<br>/sys/block/sdbd/queue/max_hw_sectors_kb 16383<br>/sys/block/sdbd/queue/max_sectors_kb 16383<br>/sys/block/sdbe/queue/max_hw_sectors_kb 16383<br>/sys/block/sdbe/queue/max_sectors_kb 512<br>/sys/block/sdbf/queue/max_hw_sectors_kb 16383<br><br>it could cause the multipath error too.<br>blk_cloned_rq_check_limits: over max size <span class="hljs-built_in">limit</span>.<br>device-mapper: multipath: Failing path 65:176.<br>device-mapper: multipath: Reinstating path 65:160.<br>device-mapper: multipath: Reinstating path 65:240.<br>blk_cloned_rq_check_limits: over max size <span class="hljs-built_in">limit</span>.<br>device-mapper: multipath: Failing path 65:240.<br>device-mapper: multipath: Reinstating path 65:144.<br>blk_cloned_rq_check_limits: over max size <span class="hljs-built_in">limit</span>.<br>device-mapper: multipath: Failing path 65:144.<br>device-mapper: multipath: Reinstating path 65:176.<br>blk_cloned_rq_check_limits: over max size <span class="hljs-built_in">limit</span>.<br>device-mapper: multipath: Failing path 65:176.<br>device-mapper: multipath: Reinstating path 65:240.<br>blk_cloned_rq_check_limits: over max size <span class="hljs-built_in">limit</span>.<br>device-mapper: multipath: Failing path 65:240.<br>device-mapper: multipath: Reinstating path 65:144.<br>device-mapper: multipath: Reinstating path 65:176.<br>device-mapper: multipath: Reinstating path 65:240.<br><br>avg-cpu:  %user   %nice %system %iowait  %steal   %idle<br>           0.00    0.00    0.02   47.92    0.00   52.06<br><br>Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util<br>sda               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00<br>sdb               0.00     0.00    0.00    0.00     0.00     0.00     0.00    67.00    0.00    0.00    0.00   0.00 100.00<br>sdc               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00<br>sdi               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00<br>sdf               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00<br>sdd               0.00     0.00    0.00    0.00     0.00     0.00     0.00   104.00    0.00    0.00    0.00   0.00 100.00<br>sdh               0.00     0.00    0.00    0.00     0.00     0.00     0.00    31.00    0.00    0.00    0.00   0.00 100.00<br>sdj               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00<br>sdk               0.00     0.00    0.00    0.00     0.00     0.00     0.00    63.00    0.00    0.00    0.00   0.00 100.00<br>sdl               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00<br>sdm               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00<br>sdn               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00<br>sdo               0.00     0.00    0.00    0.00     0.00     0.00     0.00    64.00    0.00    0.00    0.00   0.00 100.00<br></code></pre></td></tr></table></figure>

<h3 id="lfs-client-random-read-performance"><a href="#lfs-client-random-read-performance" class="headerlink" title="lfs client random read performance"></a>lfs client random read performance</h3><p>In lfs 2.13<br>direct IO could reach high 120k, but the buffered IO only 4k. there are read<br>When I disable<br>echo 0 &gt; /sys/fs/lustre/llite/lfs-ffff9455c0a4b800/read_ahead_async_file_threshold_mb<br>I could got the high IOPS too.</p>
<h4 id="disable-OSS-read-cache"><a href="#disable-OSS-read-cache" class="headerlink" title="disable OSS read cache"></a>disable OSS read cache</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#disable read cache on all the OSTs of an OSS</span><br>$ lctl set_param obdfilter.*.read_cache_enable=0<br><br><span class="hljs-comment">#re-enable read cache </span><br>$ lctl set_param obdfilter.&#123;OST_name&#125;.read_cache_enable=1<br>$ lctl get_param obdfilter.*.read_cache_enable<br></code></pre></td></tr></table></figure>

<p>writethrough_cache_enable - Controls whether data sent to the OSS as a write request is kept in the read cache and available for later reads, or if it is discarded from cache when the write is completed. By default, the writethrough cache is enabled (writethrough_cache_enable=1).</p>
<p>If the writethrough cache is disabled (writethrough_cache_enabled=0), the OSS discards the data after the write request from the client is completed. For subsequent read requests, or partial-page write requests, the OSS must re-read the data from disk.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#disable writethrough_cache</span><br>$ lctl set_param obdfilter.*.writethrough_cache_enable=0<br><br><span class="hljs-comment"># re-enable </span><br>$ lctl set_param obdfilter.&#123;OST_name&#125;.writethrough_cache_enable=1<br>$ lctl get_param obdfilter.*.writethrough_cache_enable<br></code></pre></td></tr></table></figure>

<p>readcache_max_filesize - Controls the maximum size of a file that both the read cache and writethrough cache will try to keep in memory. Files larger than readcache_max_filesize will not be kept in cache for either reads or writes.</p>
<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">$ lctl <span class="hljs-keyword">set</span>_param obdfilter.*<span class="hljs-string">.readcache_max_filesize=32M</span><br><br><span class="hljs-comment">#disable the maxinum cached file size on the OST</span><br>$ lctl <span class="hljs-keyword">set</span>_param obdfilter.&#123;OST_name&#125;<span class="hljs-string">.readcache_max_filesize=-1</span><br>$ lctl get_param obdfilter.*<span class="hljs-string">.readcache_max_filesize</span><br></code></pre></td></tr></table></figure>

<h3 id="client-metadata-setting"><a href="#client-metadata-setting" class="headerlink" title="client metadata setting"></a>client metadata setting</h3><p>The MDC max_rpcs_in_flight parameter defines the maximum number of metadata RPCs, both modifying and non-modifying RPCs, that can be sent in parallel by a client to a MDT target. This includes every file system metadata operations, such as file or directory stat, creation, unlink. The default setting is 8, minimum setting is 1 and maximum setting is 256.</p>
<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">client $ lctl <span class="hljs-keyword">set</span>_param mdc.*<span class="hljs-string">.max_rpcs_in_flight=16</span><br></code></pre></td></tr></table></figure>

<p>The MDC max_mod_rpcs_in_flight parameter defines the maximum number of file system modifying RPCs that can be sent in parallel by a client to a MDT target. For example, the Lustre client sends modify RPCs when it performs file or directory creation, unlink, access permission modification or ownership modification. The default setting is 7, minimum setting is 1 and maximum setting is 256.</p>
<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">clinet $  lctl <span class="hljs-keyword">set</span>_param mdc.*<span class="hljs-string">.max_mod_rpcs_in_flight=12</span><br></code></pre></td></tr></table></figure>
<p>The max_mod_rpcs_in_flight value must be strictly less than the max_rpcs_in_flight value. It must also be less or equal to the MDT max_mod_rpcs_per_client value. If one of theses conditions is not enforced, the setting fails and an explicit message is written in the Lustre log.</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">mds $ echo <span class="hljs-number">12</span> &gt; <span class="hljs-regexp">/sys/m</span>odule<span class="hljs-regexp">/mdt/</span>parameters/max_mod_rpcs_per_client<br></code></pre></td></tr></table></figure>

<h3 id="record-several-years-ago-trace-the-file-by-debugfs"><a href="#record-several-years-ago-trace-the-file-by-debugfs" class="headerlink" title="record several years ago trace the file by debugfs"></a>record several years ago trace the file by debugfs</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ dd <span class="hljs-keyword">if</span>=RHEL5.5_x86_64.iso of=rhel5-1 bs=1M count=1 oflag=sync<br><br>$ cat $(blktrace.log)<br>  8,32   0     2591  1720.406537446 14972  Q   W 19144704 + 2048 [ll_ost_io00_002]<br>  8,32   0     2592  1720.406539742 14972  G   W 19144704 + 2048 [ll_ost_io00_002]<br>  8,32   0     2593  1720.406542136 14972  P   N [ll_ost_io00_002]<br>  8,32   0     2594  1720.406543690 14972  I   W 19144704 + 2048 [ll_ost_io00_002]<br><br>$ line=19144704<br>$ debugfs -c -R <span class="hljs-string">&quot;icheck <span class="hljs-subst">$(($line/8)</span>)&quot;</span> /dev/sdc<br>debugfs 1.42.12.wc1 (15-Sep-2014)<br>/dev/sdc: catastrophic mode - not reading inode or group bitmaps<br>Block	Inode number<br>2393088	2623<br>$ debugfs -R <span class="hljs-string">&quot;ncheck 2623&quot;</span> /dev/sdc<br>debugfs 1.42.12.wc1 (15-Sep-2014)<br>Inode	Pathname<br>2623	/O/0/d13/2669<br><br><br>$ lfs getstripe rhel5-1<br>rhel5-1<br>lmm_stripe_count:   1<br>lmm_stripe_size:    1048576<br>lmm_pattern:        1<br>lmm_layout_gen:     0<br>lmm_stripe_offset:  3<br>lmm_pool:           OST0003<br>	obdidx		 objid		 objid		 group<br>	     3	          2669	        0xa6d	             0<br><br>$ debugfs -c -R <span class="hljs-string">&quot;stat &lt;2623&gt;&quot;</span> /dev/sdc<br>debugfs 1.42.12.wc1 (15-Sep-2014)<br>/dev/sdc: catastrophic mode - not reading inode or group bitmaps<br>Inode: 2623   Type: regular    Mode:  0666   Flags: 0x80000<br>Generation: 2790813459    Version: 0x00000005:00002bf8<br>User:     0   Group:     0   Size: 1048576<br>File ACL: 0    Directory ACL: 0<br>Links: 1   Blockcount: 2048<br>Fragment:  Address: 0    Number: 0    Size: 0<br> ctime: 0x5715e344:00000000 -- Tue Apr 19 15:50:28 2016<br> atime: 0x00000000:00000000 -- Thu Jan  1 08:00:00 1970<br> mtime: 0x5715e344:00000000 -- Tue Apr 19 15:50:28 2016<br>crtime: 0x57148057:dfa2ff44 -- Mon Apr 18 14:36:07 2016<br>Size of extra inode fields: 28<br>Extended attributes stored <span class="hljs-keyword">in</span> inode body:<br>  lma = <span class="hljs-string">&quot;08 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 6d 0a 00 00 00 00 00 00 &quot;</span> (24)<br>  lma: fid=[0x100000000:0xa6d:0x0] compat=8 incompat=0<br>  fid = <span class="hljs-string">&quot;d2 0b 00 00 02 00 00 00 40 00 00 00 00 00 00 00 &quot;</span> (16)<br>  fid: parent=[0x200000bd2:0x40:0x0] stripe=0<br>EXTENTS:<br>(0-255):2393088-2393343<br><br>$ lfs fid2path /opt/ [0x200000bd2:0x40:0x0]<br>/opt/OST0003/rhel5-1<br></code></pre></td></tr></table></figure>

<h3 id="Monitor"><a href="#Monitor" class="headerlink" title="Monitor"></a>Monitor</h3><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs angelscript">$ lctl get_param osc.testfs-OST0001*.*max*<br>$ lctl get_param osc.testfs-OST0001*.*grant*<br><br>client<br>$ lctl get_param llite.*.stats<br><br>#OSS<br>$ lctl get_param obdfilter.*.brw_stats<br>obdfilter.testfs-OST0000.brw_stats=<br>snapshot_time:         <span class="hljs-number">1593407004.726097825</span> (secs.nsecs)<br><br>                           read      |     write<br>pages per bulk r/w     rpcs  % cum % |  rpcs        % cum %<br><span class="hljs-number">1</span>:		   <span class="hljs-number">6006159</span>   <span class="hljs-number">2</span>   <span class="hljs-number">2</span>   | <span class="hljs-number">1239974</span>   <span class="hljs-number">1</span>   <span class="hljs-number">1</span><br><span class="hljs-number">2</span>:		  <span class="hljs-number">45845506</span>  <span class="hljs-number">16</span>  <span class="hljs-number">18</span>   | <span class="hljs-number">1809852</span>   <span class="hljs-number">2</span>   <span class="hljs-number">4</span><br><span class="hljs-number">4</span>:		   <span class="hljs-number">1952464</span>   <span class="hljs-number">0</span>  <span class="hljs-number">18</span>   | <span class="hljs-number">53814</span>   <span class="hljs-number">0</span>   <span class="hljs-number">4</span><br><span class="hljs-number">8</span>:		  <span class="hljs-number">33533746</span>  <span class="hljs-number">11</span>  <span class="hljs-number">30</span>   | <span class="hljs-number">832520</span>   <span class="hljs-number">1</span>   <span class="hljs-number">5</span><br><span class="hljs-number">16</span>:		  <span class="hljs-number">12859649</span>   <span class="hljs-number">4</span>  <span class="hljs-number">35</span>   | <span class="hljs-number">91425</span>   <span class="hljs-number">0</span>   <span class="hljs-number">5</span><br><span class="hljs-number">32</span>:		    <span class="hljs-number">620131</span>   <span class="hljs-number">0</span>  <span class="hljs-number">35</span>   | <span class="hljs-number">81660</span>   <span class="hljs-number">0</span>   <span class="hljs-number">5</span><br><span class="hljs-number">64</span>:		  <span class="hljs-number">38856531</span>  <span class="hljs-number">13</span>  <span class="hljs-number">49</span>   | <span class="hljs-number">307195</span>   <span class="hljs-number">0</span>   <span class="hljs-number">5</span><br><span class="hljs-number">128</span>:		   <span class="hljs-number">1697223</span>   <span class="hljs-number">0</span>  <span class="hljs-number">49</span>   | <span class="hljs-number">223084</span>   <span class="hljs-number">0</span>   <span class="hljs-number">6</span><br><span class="hljs-number">256</span>:		 <span class="hljs-number">142793845</span>  <span class="hljs-number">50</span> <span class="hljs-number">100</span>   | <span class="hljs-number">69219436</span>  <span class="hljs-number">93</span> <span class="hljs-number">100</span><br><br>                           read      |     write<br>discontiguous pages    rpcs  % cum % |  rpcs        % cum %<br><span class="hljs-number">0</span>:		 <span class="hljs-number">284165254</span> <span class="hljs-number">100</span> <span class="hljs-number">100</span>   | <span class="hljs-number">1244480</span>   <span class="hljs-number">1</span>   <span class="hljs-number">1</span><br><span class="hljs-number">1</span>:		         <span class="hljs-number">0</span>   <span class="hljs-number">0</span> <span class="hljs-number">100</span>   | <span class="hljs-number">1810011</span>   <span class="hljs-number">2</span>   <span class="hljs-number">4</span><br><span class="hljs-number">2</span>:		         <span class="hljs-number">0</span>   <span class="hljs-number">0</span> <span class="hljs-number">100</span>   | <span class="hljs-number">14961</span>   <span class="hljs-number">0</span>   <span class="hljs-number">4</span><br><span class="hljs-number">3</span>:		         <span class="hljs-number">0</span>   <span class="hljs-number">0</span> <span class="hljs-number">100</span>   | <span class="hljs-number">38853</span>   <span class="hljs-number">0</span>   <span class="hljs-number">4</span><br><span class="hljs-number">4</span>:		         <span class="hljs-number">0</span>   <span class="hljs-number">0</span> <span class="hljs-number">100</span>   | <span class="hljs-number">10409</span>   <span class="hljs-number">0</span>   <span class="hljs-number">4</span><br><span class="hljs-number">5</span>:		         <span class="hljs-number">0</span>   <span class="hljs-number">0</span> <span class="hljs-number">100</span>   | <span class="hljs-number">23853</span>   <span class="hljs-number">0</span>   <span class="hljs-number">4</span><br><br>$ lctl get_param ost.OSS.ost_io.req_history<br>````<br><br>### [lfs zfs direct IO support](https:<span class="hljs-comment">//lustre-discuss.lustre.narkive.com/S7kbvnG2/lustre-on-zfs-pooer-direct-i-o-performance)</span><br>```bash<br>John, with newer Lustre clients it <span class="hljs-keyword">is</span> possible <span class="hljs-keyword">for</span> multiple threads to submit non-overlapping writes concurrently (also <span class="hljs-keyword">not</span> conflicting within a single page), see LU<span class="hljs-number">-1669</span> <span class="hljs-keyword">for</span> details.<br><br>Even so, O_DIRECT writes need to be synchronous to disk on the OSS, as Patrick reports, because <span class="hljs-keyword">if</span> the OSS fails before the write <span class="hljs-keyword">is</span> on disk there <span class="hljs-keyword">is</span> no cached copy of the data on the client that can be used to resend the RPC.<br><br>The problem <span class="hljs-keyword">is</span> that the ZFS OSD has very long transaction commit times <span class="hljs-keyword">for</span> synchronous writes because it does <span class="hljs-keyword">not</span> yet have support <span class="hljs-keyword">for</span> the ZIL. Using buffered writes, <span class="hljs-keyword">or</span> having very large O_DIRECT writes (e.g. <span class="hljs-number">40</span>MB <span class="hljs-keyword">or</span> larger) <span class="hljs-keyword">and</span> large RPCs (<span class="hljs-number">4</span>MB, <span class="hljs-keyword">or</span> up to <span class="hljs-number">16</span>MB <span class="hljs-keyword">in</span> <span class="hljs-number">2.9</span><span class="hljs-number">.0</span>) to amortize the sync overhead may be beneficial <span class="hljs-keyword">if</span> you really want to use O_DIRECT.<br></code></pre></td></tr></table></figure>


<h3 id="mdt-reduce-5-osq-lock-overhead-in-perf-top"><a href="#mdt-reduce-5-osq-lock-overhead-in-perf-top" class="headerlink" title="mdt reduce 5% osq_lock overhead in perf top"></a>mdt reduce 5% osq_lock overhead in perf top</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><code class="hljs bash">ZFS Tunables:<br>	dbuf_cache_hiwater_pct                            10<br>	dbuf_cache_lowater_pct                            10<br>	dbuf_cache_max_bytes                              104857600<br>	dbuf_cache_max_shift                              5<br>	dmu_object_alloc_chunk_shift                      7<br>	ignore_hole_birth                                 1<br>	l2arc_feed_again                                  1<br>	l2arc_feed_min_ms                                 200<br>	l2arc_feed_secs                                   1<br>	l2arc_headroom                                    2<br>	l2arc_headroom_boost                              200<br>	l2arc_noprefetch                                  1<br>	l2arc_norw                                        0<br>	l2arc_write_boost                                 8388608<br>	l2arc_write_max                                   8388608<br>	metaslab_aliquot                                  524288<br>	metaslab_bias_enabled                             1<br>	metaslab_debug_load                               0<br>	metaslab_debug_unload                             1<br>	metaslab_fragmentation_factor_enabled             1<br>	metaslab_lba_weighting_enabled                    1<br>	metaslab_preload_enabled                          1<br>	metaslabs_per_vdev                                200<br>	send_holes_without_birth_time                     1<br>	spa_asize_inflation                               24<br>	spa_config_path                                   /etc/zfs/zpool.cache<br>	spa_load_verify_data                              1<br>	spa_load_verify_maxinflight                       10000<br>	spa_load_verify_metadata                          1<br>	spa_slop_shift                                    5<br>	zfetch_array_rd_sz                                1048576<br>	zfetch_max_distance                               8388608<br>	zfetch_max_streams                                8<br>	zfetch_min_sec_reap                               2<br>	zfs_abd_scatter_enabled                           1<br>	zfs_abd_scatter_max_order                         10<br>	zfs_admin_snapshot                                1<br>	zfs_arc_average_blocksize                         8192<br>	zfs_arc_dnode_limit                               0<br>	zfs_arc_dnode_limit_percent                       10<br>	zfs_arc_dnode_reduce_percent                      10<br>	zfs_arc_grow_retry                                0<br>	zfs_arc_lotsfree_percent                          10<br>	zfs_arc_max                                       0<br>	zfs_arc_meta_adjust_restarts                      4096<br>	zfs_arc_meta_limit                                12091328614<br>	zfs_arc_meta_limit_percent                        75<br>	zfs_arc_meta_min                                  0<br>	zfs_arc_meta_prune                                10000<br>	zfs_arc_meta_strategy                             1<br>	zfs_arc_min                                       0<br>	zfs_arc_min_prefetch_lifespan                     0<br>	zfs_arc_p_dampener_disable                        1<br>	zfs_arc_p_min_shift                               0<br>	zfs_arc_pc_percent                                0<br>	zfs_arc_shrink_shift                              0<br>	zfs_arc_sys_free                                  0<br>	zfs_autoimport_disable                            1<br>	zfs_checksums_per_second                          20<br>	zfs_compressed_arc_enabled                        1<br>	zfs_dbgmsg_enable                                 0<br>	zfs_dbgmsg_maxsize                                4194304<br>	zfs_dbuf_state_index                              0<br>	zfs_deadman_checktime_ms                          5000<br>	zfs_deadman_enabled                               1<br>	zfs_deadman_synctime_ms                           1000000<br>	zfs_dedup_prefetch                                0<br>	zfs_delay_min_dirty_percent                       60<br>	zfs_delay_scale                                   500000<br>	zfs_delays_per_second                             20<br>	zfs_delete_blocks                                 20480<br>	zfs_dirty_data_max                                17179869184<br>	zfs_dirty_data_max_max                            4294967296<br>	zfs_dirty_data_max_max_percent                    25<br>	zfs_dirty_data_max_percent                        10<br>	zfs_dirty_data_sync                               67108864<br>	zfs_dmu_offset_next_sync                          0<br>	zfs_expire_snapshot                               300<br>	zfs_flags                                         0<br>	zfs_free_bpobj_enabled                            1<br>	zfs_free_leak_on_eio                              0<br>	zfs_free_max_blocks                               100000<br>	zfs_free_min_time_ms                              1000<br>	zfs_immediate_write_sz                            32768<br>	zfs_max_recordsize                                1048576<br>	zfs_mdcomp_disable                                0<br>	zfs_metaslab_fragmentation_threshold              70<br>	zfs_metaslab_segment_weight_enabled               1<br>	zfs_metaslab_switch_threshold                     2<br>	zfs_mg_fragmentation_threshold                    85<br>	zfs_mg_noalloc_threshold                          0<br>	zfs_multihost_fail_intervals                      5<br>	zfs_multihost_history                             0<br>	zfs_multihost_import_intervals                    10<br>	zfs_multihost_interval                            17000<br>	zfs_multilist_num_sublists                        0<br>	zfs_no_scrub_io                                   0<br>	zfs_no_scrub_prefetch                             0<br>	zfs_nocacheflush                                  0<br>	zfs_nopwrite_enabled                              1<br>	zfs_object_mutex_size                             64<br>	zfs_pd_bytes_max                                  52428800<br>	zfs_per_txg_dirty_frees_percent                   30<br>	zfs_prefetch_disable                              1<br>	zfs_read_chunk_size                               16384<br>	zfs_read_history                                  0<br>	zfs_read_history_hits                             0<br>	zfs_recover                                       0<br>	zfs_recv_queue_length                             16777216<br>	zfs_resilver_delay                                2<br>	zfs_resilver_min_time_ms                          3000<br>	zfs_scan_idle                                     50<br>	zfs_scan_ignore_errors                            0<br>	zfs_scan_min_time_ms                              1000<br>	zfs_scrub_delay                                   4<br>	zfs_send_corrupt_data                             0<br>	zfs_send_queue_length                             16777216<br>	zfs_sync_pass_deferred_free                       2<br>	zfs_sync_pass_dont_compress                       5<br>	zfs_sync_pass_rewrite                             2<br>	zfs_sync_taskq_batch_pct                          75<br>	zfs_top_maxinflight                               32<br>	zfs_txg_history                                   0<br>	zfs_txg_timeout                                   10<br>	zfs_vdev_aggregation_limit                        131072<br>	zfs_vdev_async_read_max_active                    10<br>	zfs_vdev_async_read_min_active                    2<br>	zfs_vdev_async_write_active_max_dirty_percent     60<br>	zfs_vdev_async_write_active_min_dirty_percent     60<br>	zfs_vdev_async_write_max_active                   12<br>	zfs_vdev_async_write_min_active                   2<br>	zfs_vdev_cache_bshift                             20<br>	zfs_vdev_cache_max                                1048576<br>	zfs_vdev_cache_size                               0<br>	zfs_vdev_max_active                               1000<br>	zfs_vdev_mirror_non_rotating_inc                  0<br>	zfs_vdev_mirror_non_rotating_seek_inc             1<br>	zfs_vdev_mirror_rotating_inc                      0<br>	zfs_vdev_mirror_rotating_seek_inc                 5<br>	zfs_vdev_mirror_rotating_seek_offset              16384<br>	zfs_vdev_queue_depth_pct                          1000<br>	zfs_vdev_raidz_impl                               [fastest] original scalar sse2 ssse3 avx2<br>	zfs_vdev_read_gap_limit                           32768<br>	zfs_vdev_scheduler                                none<br>	zfs_vdev_scrub_max_active                         2<br>	zfs_vdev_scrub_min_active                         1<br>	zfs_vdev_sync_read_max_active                     128<br>	zfs_vdev_sync_read_min_active                     128<br>	zfs_vdev_sync_write_max_active                    128<br>	zfs_vdev_sync_write_min_active                    128<br>	zfs_vdev_write_gap_limit                          4096<br>	zfs_zevent_cols                                   80<br>	zfs_zevent_console                                0<br>	zfs_zevent_len_max                                320<br>	zil_replay_disable                                0<br>	zil_slog_bulk                                     786432<br>	zio_delay_max                                     30000<br>	zio_dva_throttle_enabled                          1<br>	zio_requeue_io_start_cut_in_line                  1<br>	zio_taskq_batch_pct                               35<br>	zvol_inhibit_dev                                  0<br>	zvol_major                                        230<br>	zvol_max_discard_blocks                           16384<br>	zvol_prefetch_bytes                               131072<br>	zvol_request_sync                                 0<br>	zvol_threads                                      32<br>	zvol_volmode                                      1<br></code></pre></td></tr></table></figure>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Ginger</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2019/12/29/lfs_command/">http://yoursite.com/2019/12/29/lfs_command/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/lfs/">lfs</a></div><div class="post_share"><div class="social-share" data-image="/img/photo_by_spacex.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/03/03/gdb/"><img class="prev-cover" src="/img/photo_by_spacex.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">GDB tips</div></div></a></div><div class="next-post pull-right"><a href="/2019/11/26/smartctl/"><img class="next-cover" src="/img/photo_by_spacex.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">smartctl</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2015/11/06/deploy_lfs_and_zfs/" title="Deploy Lfs and OpenZFS"><img class="cover" src="/img/photo_by_spacex.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2015-11-06</div><div class="title">Deploy Lfs and OpenZFS</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></article></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2015 - 2020 By Ginger</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><div class="js-pjax"><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk({
      clientID: '0b9d5b98d0a972b33cf7',
      clientSecret: '769efc11b32f6c0d03bcbf3ee800dfc4e2690459',
      repo: 'homerl.github.io',
      owner: 'homerl',
      admin: ['homerl'],
      id: '73dd8e8ab493ab1fcb4f54f43e4399fb',
      language: 'en',
      perPage: 10,
      distractionFreeMode: false,
      pagerDirection: 'last',
      createIssueManually: false,
      updateCountCallback: commentCount
    })
    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    $.getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js', initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>