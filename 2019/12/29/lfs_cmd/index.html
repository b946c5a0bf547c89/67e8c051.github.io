<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>lfs command | 无常无相无功;不动不破不空</title><meta name="keywords" content="lfs"><meta name="author" content="Homer"><meta name="copyright" content="Homer"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="The lfs server can only handle about 15,000 remote procedure calls (RPCs, inter-process communications that allow the client to cause a procedure to be executed on the server) per second. Contention">
<meta property="og:type" content="article">
<meta property="og:title" content="lfs command">
<meta property="og:url" content="http://yoursite.com/2019/12/29/lfs_cmd/index.html">
<meta property="og:site_name" content="无常无相无功;不动不破不空">
<meta property="og:description" content="The lfs server can only handle about 15,000 remote procedure calls (RPCs, inter-process communications that allow the client to cause a procedure to be executed on the server) per second. Contention">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://homerl.github.io/img/hard-disk_223cf.jpg">
<meta property="article:published_time" content="2019-12-29T07:41:16.000Z">
<meta property="article:modified_time" content="2022-05-13T02:25:42.761Z">
<meta property="article:author" content="Homer">
<meta property="article:tag" content="lfs">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://homerl.github.io/img/hard-disk_223cf.jpg"><link rel="shortcut icon" href="/img/stout-shield.png"><link rel="canonical" href="http://yoursite.com/2019/12/29/lfs_cmd/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.2.0',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2022-05-13 10:25:42'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="无常无相无功;不动不破不空" type="application/atom+xml">
</head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/fighting-spiri-logot.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">57</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">57</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li><li><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#All"><span class="toc-number">1.</span> <span class="toc-text">All</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#check-lfs-log"><span class="toc-number">1.1.</span> <span class="toc-text">check lfs log</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#list-all-parameters"><span class="toc-number">1.2.</span> <span class="toc-text">list all parameters</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MDS"><span class="toc-number">2.</span> <span class="toc-text">MDS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MDS-ops"><span class="toc-number">2.1.</span> <span class="toc-text">MDS ops</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#OSS"><span class="toc-number">3.</span> <span class="toc-text">OSS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#format"><span class="toc-number">3.1.</span> <span class="toc-text">format</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#change-ipaddr"><span class="toc-number">3.2.</span> <span class="toc-text">change ipaddr</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Enable-large-dir-feature"><span class="toc-number">3.3.</span> <span class="toc-text">Enable large_dir feature</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Client"><span class="toc-number">4.</span> <span class="toc-text">Client</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#set-stripe-size-for-tiny-files"><span class="toc-number">4.1.</span> <span class="toc-text">set stripe size for tiny files</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mds"><span class="toc-number">5.</span> <span class="toc-text">mds</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#OSS-1"><span class="toc-number">6.</span> <span class="toc-text">OSS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#TCP-Ethernet-network"><span class="toc-number">6.1.</span> <span class="toc-text">TCP Ethernet network</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Trace-log"><span class="toc-number">7.</span> <span class="toc-text">Trace log</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lfs-log"><span class="toc-number">7.1.</span> <span class="toc-text">lfs log</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#kdump"><span class="toc-number">7.2.</span> <span class="toc-text">kdump</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages"><span class="toc-number"></span> <span class="toc-text">1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages"><span class="toc-number"></span> <span class="toc-text">1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#changelog"><span class="toc-number">1.</span> <span class="toc-text">changelog</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Enable"><span class="toc-number">1.1.</span> <span class="toc-text">Enable</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Flush-the-journal"><span class="toc-number">1.2.</span> <span class="toc-text">Flush the journal</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#re-writeconf"><span class="toc-number">1.3.</span> <span class="toc-text">re-writeconf</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#User-group-quota"><span class="toc-number">2.</span> <span class="toc-text">User group quota</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Disable-the-ost"><span class="toc-number">3.</span> <span class="toc-text">Disable the ost</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Skip-recovery"><span class="toc-number">4.</span> <span class="toc-text">Skip recovery</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-migarate"><span class="toc-number">5.</span> <span class="toc-text">lfs migarate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-fid-and-path"><span class="toc-number">6.</span> <span class="toc-text">lfs fid and path</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#increase-openzfs-sync-performance-in-test-env"><span class="toc-number">7.</span> <span class="toc-text">increase openzfs sync performance in test env</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lfs-issues"><span class="toc-number">8.</span> <span class="toc-text">Lfs issues</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Don-%E2%80%98t-use-openzfs-with-lfs-file-system"><span class="toc-number">8.1.</span> <span class="toc-text">Don ‘t use openzfs with lfs file system</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Multipath-failed-cause"><span class="toc-number">8.2.</span> <span class="toc-text">Multipath failed cause</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#disable-OSS-read-cache"><span class="toc-number">8.3.</span> <span class="toc-text">disable OSS read cache</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#client-metadata-setting"><span class="toc-number">9.</span> <span class="toc-text">client metadata setting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#record-several-years-ago-trace-the-file-by-debugfs"><span class="toc-number">10.</span> <span class="toc-text">record several years ago trace the file by debugfs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-zfs-direct-IO-support"><span class="toc-number">11.</span> <span class="toc-text">lfs zfs direct IO support</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Got-the-client-uuid-map-nid"><span class="toc-number">12.</span> <span class="toc-text">Got the client uuid map nid</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lnet"><span class="toc-number">13.</span> <span class="toc-text">lnet</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lnet-health"><span class="toc-number">13.1.</span> <span class="toc-text">lnet health</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ban-the-lfs-client"><span class="toc-number">14.</span> <span class="toc-text">ban the lfs client</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rocev2-test"><span class="toc-number">15.</span> <span class="toc-text">rocev2 test</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-magic-num"><span class="toc-number">16.</span> <span class="toc-text">lfs magic num</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#build-lfs"><span class="toc-number">17.</span> <span class="toc-text">build lfs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sub-cmd"><span class="toc-number">18.</span> <span class="toc-text">sub cmd</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#llog-reader"><span class="toc-number">18.1.</span> <span class="toc-text">llog_reader</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SPEC"><span class="toc-number">19.</span> <span class="toc-text">SPEC</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Ethernet"><span class="toc-number">19.1.</span> <span class="toc-text">Ethernet</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Backup-and-recovery-lfs-ZFS-OST"><span class="toc-number">20.</span> <span class="toc-text">Backup and recovery lfs ZFS OST</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lustre-multiple-ethernet-port-for-diff-LAN"><span class="toc-number">21.</span> <span class="toc-text">lustre multiple ethernet port for diff LAN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#proc-net-tcp"><span class="toc-number"></span> <span class="toc-text">&#x2F;proc&#x2F;net&#x2F;tcp</span></a></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://homerl.github.io/img/hard-disk_223cf.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">无常无相无功;不动不破不空</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li><li><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">lfs command</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-12-29T07:41:16.000Z" title="Created 2019-12-29 15:41:16">2019-12-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-05-13T02:25:42.761Z" title="Updated 2022-05-13 10:25:42">2022-05-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Storage/">Storage</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><ul>
<li><a target="_blank" rel="noopener" href="https://hpcf.umbc.edu/general-productivity/lustre-best-practices/">The lfs server can only handle about 15,000 remote procedure calls (RPCs, inter-process communications that allow the client to cause a procedure to be executed on the server) per second. Contention slows the performance of your applications and weakens the overall health of the lfs filesystem</a><ul>
<li>Avoid Using ls -l</li>
<li>Avoid Having a Large Number of Files in a Single Directory</li>
<li>Avoid Accessing Small Files</li>
<li>Avoid Repetitive “stat” Operations</li>
<li>Avoid Having Multiple Processes Open the Same File(s) at the Same Time</li>
<li>Avoid Repetitive Open/Close Operations<a id="more"></a>

</li>
</ul>
</li>
</ul>
<h3 id="All"><a href="#All" class="headerlink" title="All"></a>All</h3><h4 id="check-lfs-log"><a href="#check-lfs-log" class="headerlink" title="check lfs log"></a>check lfs log</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">$ lctl  get_param  at_max</span><br><span class="line">at_max=600</span><br><span class="line">Maximum adaptive timeout (<span class="keyword">in</span> seconds). The at_max parameter is an upper-limit on the service time estimate. If at_max is reached, an RPC request <span class="built_in">times</span> out.Setting at_max to 0 causes adaptive timeouts to be disabled and a fixed timeout method to be used instead (see the section called “Setting Static Timeouts” Note If slow hardware causes the service estimate to increase beyond the default value of at_max, increase at_max to the maximum time you are willing to <span class="built_in">wait</span> <span class="keyword">for</span> an RPC completion.</span><br><span class="line">$ lctl  get_param  at_min</span><br><span class="line">at_min=50</span><br><span class="line">Minimum adaptive timeout (<span class="keyword">in</span> seconds). The default value is 0. The at_min parameter is the minimum processing time that a server will report. Ideally, at_min should be <span class="built_in">set</span> to its default value. Clients base their timeouts on this value, but they <span class="keyword">do</span> not use this value directly.If, <span class="keyword">for</span> unknown reasons (usually due to temporary network outages), the adaptive timeout value is too short and clients time out their RPCs, you can increase the at_min value to compensate <span class="keyword">for</span> this.</span><br><span class="line">$ lctl get_param at_extra</span><br><span class="line">at_extra=30</span><br><span class="line"><span class="comment">#Incremental amount of time that a server requests with each early reply (in seconds). The server does not know how much time the RPC will take, so it asks for a fixed value. The default is 30, which provides a balance between sending too many early replies for the same RPC and overestimating the actual completion time.When a server finds a queued request about to time out and needs to send an early reply out, the server adds the at_extra value. If the time expires, the lfs server drops the request, and the client enters recovery status and reconnects to restore the connection to normal status.If you see multiple early replies for the same RPC asking for 30-second increases, change the at_extra value to a larger number to cut down on early replies sent and, therefore, network load.</span></span><br><span class="line"></span><br><span class="line">$ lctl  get_param  timeout</span><br><span class="line">timeout=300</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ lctl get_param -n debug</span><br><span class="line">$ lctl set_param debug=<span class="string">&quot;ioctl neterror warning error emerg ha config console lfsck mmap page dentry cache malloc quota dlmtrace reada vfstrace rpctrace&quot;</span></span><br><span class="line">$ lctl set_param debug=<span class="string">&quot;ioctl neterror warning error emerg ha config console lfsck cache reada quota&quot;</span></span><br><span class="line">$ lctl set_param debug=<span class="string">&quot;ioctl neterror warning error emerg ha config console lfsck&quot;</span></span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.dump_namespaces</span><br><span class="line"><span class="comment">##no output</span></span><br><span class="line">$ lctl set_param ldlm.dump_namespaces <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">$ lctl get_param llite.*.dump_page_cache | grep -cEi <span class="string">&#x27;lockd|dirty|writeback&#x27;</span></span><br><span class="line"></span><br><span class="line">$ strings dk</span><br><span class="line">$ lctl df &lt;input file&gt; &lt;output file&gt;</span><br><span class="line"></span><br><span class="line">lfs rpm package install will not overvide the /lib/modules/$(uname -r)/extra</span><br><span class="line">$ rm -rf /lib/modules/$(uname -r)/extra/; mkdir /lib/modules/$(uname -r)/extra</span><br><span class="line"></span><br><span class="line">$ lctl get_param debug_path</span><br><span class="line">debug_path=/tmp/lfs-log</span><br><span class="line"></span><br><span class="line"><span class="comment"># enable(set to 1) bulk pages dump upon error on Client</span></span><br><span class="line">Client $ lctl get_param osc.*osc-[^mM]*.checksum_dump</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.checksum_dump=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># enable(set to 1) bulk pages dump upon error on OSS</span></span><br><span class="line">Oss$ lctl get_param obdfilter.*-OST*.checksum_dump</span><br><span class="line">obdfilter.fsname-OST0000.checksum_dump=0</span><br><span class="line"></span><br><span class="line"><span class="comment">## mds and oss</span></span><br><span class="line">can1=$(do_facet mds1 <span class="string">&quot;<span class="variable">$LCTL</span> get_param -n ldlm.services.ldlm_canceld.stats&quot;</span> |awk <span class="string">&#x27;/ldlm_cancel/ &#123;print $2&#125;&#x27;</span>)</span><br><span class="line">blk1=$(<span class="variable">$LCTL</span> get_param -n ldlm.services.ldlm_cbd.stats |awk <span class="string">&#x27;/ldlm_bl_callback/ &#123;print $2&#125;&#x27;</span>) </span><br><span class="line"></span><br><span class="line">test_mkdir -i0 -c1 <span class="variable">$DIR</span>/<span class="variable">$tdir</span>/d1</span><br><span class="line"></span><br><span class="line">can2=$(do_facet mds1 <span class="string">&quot;<span class="variable">$LCTL</span> get_param -n ldlm.services.ldlm_canceld.stats&quot;</span> |awk <span class="string">&#x27;/ldlm_cancel/ &#123;print $2&#125;&#x27;</span>)</span><br><span class="line">blk2=$(<span class="variable">$LCTL</span> get_param -n ldlm.services.ldlm_cbd.stats | awk <span class="string">&#x27;/ldlm_bl_callback/ &#123;print $2&#125;&#x27;</span>)</span><br><span class="line">[ <span class="variable">$can1</span> -eq <span class="variable">$can2</span> ] || error $((can2-can1)) <span class="string">&quot;cancel RPC occured.&quot;</span></span><br><span class="line">[ <span class="variable">$blk1</span> -eq <span class="variable">$blk2</span> ] || error $((blk2-blk1)) <span class="string">&quot;blocking RPC occured.&quot;</span></span><br></pre></td></tr></table></figure>

<h4 id="list-all-parameters"><a href="#list-all-parameters" class="headerlink" title="list all parameters"></a>list all parameters</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ lctl list_param -R <span class="string">&#x27;*&#x27;</span></span><br><span class="line">$ lctl list_param osc.*.*</span><br><span class="line">$ <span class="keyword">for</span> i <span class="keyword">in</span> $(ls /proc/fs/lfs); <span class="keyword">do</span> lctl get_param <span class="variable">$&#123;i&#125;</span>.*.*; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<h3 id="MDS"><a href="#MDS" class="headerlink" title="MDS"></a>MDS</h3><h4 id="MDS-ops"><a href="#MDS-ops" class="headerlink" title="MDS ops"></a>MDS ops</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># del last_recvd in mdt and pass the recovery, mdt and clients mount by no_recov</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#show open files</span></span><br><span class="line">$ /proc/fs/lustre/mdt/*/exports/*/open_files</span><br><span class="line"></span><br><span class="line"><span class="comment">### show all clients</span></span><br><span class="line">$ lshowmount -e</span><br><span class="line"></span><br><span class="line"><span class="comment">### the same deploy like add OST, just change the index, don &#x27;t need to add the MGS </span></span><br><span class="line"><span class="comment">### </span></span><br><span class="line">$ lfs mkdir -c stripe_count ./<span class="built_in">dirs</span> <span class="comment">## stripe dirs in MDT DNE, loading balancing in diff MDT</span></span><br><span class="line">$ lfs mkdir -i mdt_index ./<span class="built_in">dirs</span></span><br><span class="line">$ lctl set_param fsname.mdt.enable_remote_dir=1</span><br><span class="line">$ lctl conf_param fsname.mdt.enable_remote_dir_gid=-1</span><br><span class="line">$ lctl get_param mdt.*.enable_remote_dir mdt.*.enable_remote_dir_gid</span><br><span class="line"></span><br><span class="line"><span class="comment"># increase throughput</span></span><br><span class="line">$ lctl set_param -P osc.*.checksums=0</span><br><span class="line"></span><br><span class="line">$ mkdir /mdtest&#123;0..5&#125; <span class="comment"># for 6 mdt</span></span><br><span class="line">clinet $ <span class="keyword">for</span> i <span class="keyword">in</span> &#123;0..5&#125; </span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  lfs mkdir -i <span class="variable">$i</span> /mdtest/<span class="variable">$I</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="comment">## how to test DNE feature in mdtest</span></span><br><span class="line"><span class="comment">## add &quot;-u&quot; and &quot; -d /mdtest/0/@/mdtest/1/@/mdtest/2/@/mdtest/3/@/mdtest/4/@/mdtest/5/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## DNE2</span></span><br><span class="line">$ lfs setdirstripe -c <span class="variable">$mdt</span> -i -1 <span class="variable">$OUTDIR</span></span><br><span class="line">$ lfs setdirstripe -D -c <span class="variable">$mdt</span> -i -1 <span class="variable">$OUTDIR</span></span><br><span class="line">$ lfs setstripe -c <span class="variable">$OSTCOUNT</span> --pool capacity <span class="variable">$OUTDIR</span></span><br><span class="line">$ lfs setstripe -L mdt -E 64K -E -1 <span class="variable">$OUTDIR</span> (DoM testing)</span><br><span class="line"></span><br><span class="line"><span class="comment">### https://www.opensfs.org/wp-content/uploads/Evaluation-of-DoM-SNE-scaling_Simmons_revised051821.pdf</span></span><br><span class="line">$ lctl set_param mdt.*.enable_dir_auto_split=1</span><br><span class="line">$ lctl <span class="built_in">set</span> param mdt.*.dir_split_delta=1</span><br><span class="line">$ lctl <span class="built_in">set</span> param mdt.*.dir_split_count=15000</span><br><span class="line"></span><br><span class="line"><span class="comment"># modify service node</span></span><br><span class="line"><span class="comment"># --erase-params </span></span><br><span class="line">$ tunefs.lustre --param=servicenode=192.168.0.10@tcp /dev/sdb</span><br><span class="line"></span><br><span class="line"><span class="comment"># show info</span></span><br><span class="line">$ tunefs.lustre /dev/sdb1</span><br><span class="line">$ tune2fs -O mmp /dev/block_device <span class="comment"># Enable MMP on ldiskfs</span></span><br><span class="line">$ tune2fs -O ^mmp /dev/block_device <span class="comment"># Enable MMP on ldiskfs</span></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">$ lctl get_param mdt.lfs-MDT0000.recovery_status</span><br><span class="line"></span><br><span class="line">$ lfs getstripe -m ./stripe/test.2</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line">$ lfs getstripe -m ./stripe/test.1</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">$ lfs mkdir -c 8 ./your_dir</span><br><span class="line"></span><br><span class="line">$ lfs find --mdt-index 0 ./stripe</span><br><span class="line">./stripe/test.2</span><br><span class="line">./stripe/test.8</span><br><span class="line">./stripe/test.6</span><br><span class="line">./stripe/test.4</span><br><span class="line">./stripe/test.0</span><br><span class="line"></span><br><span class="line">$ lfs find --mdt-index 1 ./stripe</span><br><span class="line">./stripe</span><br><span class="line">./stripe/test.1</span><br><span class="line">./stripe/test.5</span><br><span class="line">./stripe/test.7</span><br><span class="line">./stripe/test.3</span><br><span class="line">./stripe/test.9</span><br><span class="line"></span><br><span class="line"><span class="comment">## read only</span></span><br><span class="line">$ lctl set_param mdt.fs-MDT0000.readonly=1</span><br><span class="line"></span><br><span class="line"><span class="comment">### show the recovery status</span></span><br><span class="line">$ lctl get_param mdt.testfs-MDT0000.recovery_status</span><br><span class="line">mdt.testsfs-MDT0000.recovery_status=</span><br><span class="line">status: COMPLETE</span><br><span class="line">recovery_start: 47</span><br><span class="line">recovery_duration: 23</span><br><span class="line">completed_clients: 2/2</span><br><span class="line">replayed_requests: 0</span><br><span class="line">last_transno: 5673970243</span><br><span class="line">VBR: DISABLED</span><br><span class="line">IR: DISABLED</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set all clients parameters</span></span><br><span class="line"><span class="comment">## Hidden QoS ## important for SSD ??</span></span><br><span class="line"><span class="comment">## not in MDS /sys and /proc</span></span><br><span class="line">MDS $ lctl set_param -P osc.*.grant_shrink=0 </span><br><span class="line">MDS $ lctl set_param -P osc.*.read_ahead_async_file_threshold_mb=0</span><br><span class="line">MDS $ lctl set_param -P osc.*.max_read_ahead_mb=64 <span class="comment">## IOPS and throughput balance</span></span><br><span class="line"><span class="comment">#disable the maxinum cached file size on the OST</span></span><br><span class="line">OSS $ lctl set_param obdfilter.&#123;OST_name&#125;.readcache_max_filesize=-1</span><br><span class="line"></span><br><span class="line"><span class="comment">#How many max_dirty_mb set ? </span></span><br><span class="line"><span class="comment"># 256(max_pages_per_rpc) x 4KB = 1MB per RPC</span></span><br><span class="line"><span class="comment"># 1024(max_pages_per_rpc) x 4KB = 4MB per RPC</span></span><br><span class="line"><span class="comment"># max_pages_per_rpc*4*max_rpcs_in_flight*2=max_dirty_mb</span></span><br><span class="line"><span class="comment"># 1024*4KB/1024(KB to MB)*64(max_rpcs_in_flight)*2=512</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## for the extremely performance</span></span><br><span class="line">MDS $ lctl set_param -P osc.*.max_pages_per_rpc=1024 osc.*.max_rpcs_in_flight=64  mdc.*.max_rpcs_in_flight=64 osc.*.max_dirty_mb=512 llite.*.max_read_ahead_mb=1024 osc.*.grant_shrink=0 subsystem_debug=0 debug=0 </span><br><span class="line"></span><br><span class="line">MDS $ lctl set_param -P osc.*.max_pages_per_rpc=1024 osc.*.max_rpcs_in_flight=64  mdc.*.max_rpcs_in_flight=64 osc.*.max_dirty_mb=512 llite.*.max_read_ahead_mb=64 osc.*.grant_shrink=0</span><br><span class="line"></span><br><span class="line"><span class="comment">#restricting the number of locks kept on the client (10000 locks, 20 minutes age), for the many metadata</span></span><br><span class="line"><span class="comment">#lru_max_age is the client LUR lock age</span></span><br><span class="line"><span class="comment">#could not set by -P </span></span><br><span class="line">$ lctl set_param ldlm.namespaces.*.lru_size=10000 ldlm.namespaces.*.lru_max_age=1200000 <span class="comment">## default lru_max_age=3900000(65 mins)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#the others</span></span><br><span class="line">readcache_max_filesize - Controls the maximum size of a file that both the <span class="built_in">read</span> cache and writethrough cache will try to keep <span class="keyword">in</span> memory. Files larger than readcache_max_filesize will not be kept <span class="keyword">in</span> cache <span class="keyword">for</span> either reads or writes.</span><br><span class="line">max_read_ahead_per_file_mb could not large than max_read_ahead_mb</span><br><span class="line"></span><br><span class="line"><span class="comment">## it &#x27;not </span></span><br><span class="line">$ lctl set_param -P llite.*.max_read_ahead_mb=0</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.*.*.lock_timeouts</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.lock_timeouts=0</span><br><span class="line">ldlm.namespaces.MGS.lock_timeouts=0</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.lock_timeouts=0</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.lock_timeouts=0</span><br><span class="line">ldlm.namespaces.fsname-OST0001-osc-MDT0000.lock_timeouts=0</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.lock_timeouts=0</span><br><span class="line"></span><br><span class="line">$ lctl get_param osp.*.active</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.active=1</span><br><span class="line">osp.fsname-OST0001-osc-MDT0000.active=1</span><br><span class="line">$  lctl get_param osd-zfs.fsname-MDT0000.mntdev</span><br><span class="line">osd-zfs.fsname-MDT0000.mntdev=test_mdt/test_mdt0</span><br><span class="line">$ lctl get_param mgs.MGS.mntdev</span><br><span class="line">mgs.MGS.mntdev=test_mdt/test_mdt0</span><br><span class="line">$ lctl barrier_stat fsname</span><br><span class="line">state: init</span><br><span class="line">timeout: 0 seconds</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_dir_migration</span><br><span class="line">1</span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_remote_dir</span><br><span class="line">1</span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_striped_dir</span><br><span class="line">1</span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_dir_migration</span><br><span class="line">1</span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_remote_dir</span><br><span class="line">1</span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_striped_dir</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line"><span class="comment">## The global write barriers</span></span><br><span class="line">Snapshots are non-atomic across multiple MDTs and OSTs, <span class="built_in">which</span> means that <span class="keyword">if</span> there is activity on the file system <span class="keyword">while</span> a snapshot is being taken, there may be user-visible namespace inconsistencies with files created or destroyed <span class="keyword">in</span> the interval between the MDT and OST snapshots. In order to create a consistent snapshot of the file system, we are able to <span class="built_in">set</span> a global write barrier, or “freeze” the system. Once <span class="built_in">set</span>, all metadata modifications will be blocked until the write barrier is actively removed (“thawed”) or expired. The user can <span class="built_in">set</span> a timeout parameter on a global barrier or the barrier can be explicitly removed. The default timeout period is 30 seconds.</span><br><span class="line">$ lctl barrier_freeze <span class="variable">$FSNAME</span> 30</span><br><span class="line">$ lctl barrier_thaw <span class="variable">$FSNAME</span></span><br><span class="line">$ lctl barrier_rescan <span class="variable">$FSNAME</span></span><br><span class="line"></span><br><span class="line">$ lctl get_param  lod.*.qos_prio_free</span><br><span class="line">lod.fsname-MDT0000-mdtlov.qos_prio_free=91%</span><br><span class="line">$ lctl get_param  lod.*.qos_threshold_rr</span><br><span class="line">lod.fsname-MDT0000-mdtlov.qos_threshold_rr=17%</span><br><span class="line">lod.*.qos_threshold_rr - </span><br><span class="line">The threshold at <span class="built_in">which</span> the allocation method switches from round-robin to weighted is <span class="built_in">set</span> <span class="keyword">in</span> this file. The default is to switch to the weighted algorithm when any two OSTs are out of balance by more than 17 percent.</span><br><span class="line">lod.*.qos_prio_free - </span><br><span class="line">The weighting priority used by the weighted allocator can be adjusted <span class="keyword">in</span> this file. Increasing the value of qos_prio_free puts more weighting on the amount of free space available on each OST and less on how stripes are distributed across OSTs. The default value is 91 percent weighting <span class="keyword">for</span> free space rebalancing and 9 percent <span class="keyword">for</span> OST balancing. When the free space priority is <span class="built_in">set</span> to 100, weighting is based entirely on free space and location is no longer used by the striping algorithm</span><br><span class="line"></span><br><span class="line">$ lctl get_param osp.*.max_create_count</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.max_create_count=20000</span><br><span class="line"></span><br><span class="line">With lfs 2.9 and later, the MDS should be <span class="built_in">set</span> to only <span class="built_in">disable</span> file creation on that OST by setting max_create_count to zero:</span><br><span class="line">$ lctl set_param osp.osc_name.max_create_count=0</span><br><span class="line"></span><br><span class="line">This ensures that files deleted or migrated off of the OST will have their corresponding OST objects destroyed, and the space will be freed. For example, to <span class="built_in">disable</span> OST0000 <span class="keyword">in</span> the filesystem testfs, run:</span><br><span class="line">$ lctl set_param osp.testfs-OST0000-osc-MDT*.max_create_count=0</span><br><span class="line"></span><br><span class="line">$ lctl get_param osp.fsname-OST0000-osc-MDT0000.active</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.active=1</span><br><span class="line"><span class="comment">#to deactivate the OSC on the MDS node(s) use:</span></span><br><span class="line">$ lctl set_param osp.fsname-OST0000-osc-MDT0000.active=0</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n mdt.*.dom_lock</span><br><span class="line">always</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n lod.*.dom_stripesize</span><br><span class="line">1048576</span><br><span class="line">$ lctl get_param osp.*.reserved_mb_high</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.reserved_mb_high=255329</span><br><span class="line">$ lctl get_param osp.*.reserved_mb_low</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.reserved_mb_low=127664</span><br><span class="line"><span class="comment"># osp.*.reserved_mb_high - The high watermark used to start object allocation if available space is more than this. The default is 0.2% of total OST size.</span></span><br><span class="line"><span class="comment">#osp.*.reserved_mb_low - The low watermark used to stop object allocation if available space is less than this. The default is 0.1% of total OST size.</span></span><br><span class="line"></span><br><span class="line">$ lctl set_param osp.*.force_sync=1</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.force_sync=1</span><br><span class="line"></span><br><span class="line">$ lctl get_param  osp.*MDT*.sync_changes</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.sync_changes=0</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.namespaces.*.max_nolock_bytes</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.MGS.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.max_nolock_bytes=0</span><br><span class="line">$ lctl get_param ldlm.namespaces.*.contention_seconds</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.contention_seconds=2</span><br><span class="line">ldlm.namespaces.MGS.contention_seconds=2</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.contention_seconds=2</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.contention_seconds=2</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.contention_seconds=2</span><br><span class="line">$ lctl get_param ldlm.namespaces.*.contended_locks</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.contended_locks=32</span><br><span class="line">ldlm.namespaces.MGS.contended_locks=32</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.contended_locks=32</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.contended_locks=32</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.contended_locks=32</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.lock_reclaim_threshold_mb</span><br><span class="line">ldlm.lock_reclaim_threshold_mb=783</span><br><span class="line"></span><br><span class="line"><span class="comment">#you could replace stats to * to get more info, lctl get_param mds.MDS.mdt.*</span></span><br><span class="line">$ lctl get_param mdt.*MDT*.exports.*@*.stats</span><br><span class="line"></span><br><span class="line"><span class="comment">#Monitor all MDS</span></span><br><span class="line">$ lctl get_param mdt.*-MDT0000.md_stats osd-*.*MDT*.filesfree osd-*.*MDT*.filestotal  osd-*.*MDT*.kbytesfree  osd-*.*MDT*.kbytestotal</span><br><span class="line">mdt.fsname-MDT0000.md_stats=</span><br><span class="line">snapshot_time             1619838866.314148577 secs.nsecs</span><br><span class="line">open                      599 samples [reqs]</span><br><span class="line">close                     137 samples [reqs] 1 1 137</span><br><span class="line">mknod                     23 samples [reqs] 1 1 23 (min, max, sum)</span><br><span class="line">unlink                    9 samples [reqs]</span><br><span class="line">mkdir                     3 samples [reqs]</span><br><span class="line">rename                    6 samples [reqs]</span><br><span class="line">getattr                   87 samples [reqs]</span><br><span class="line">setattr                   17 samples [reqs]</span><br><span class="line">getxattr                  25 samples [reqs]</span><br><span class="line">setxattr                  11 samples [reqs]</span><br><span class="line">statfs                    25 samples [reqs]</span><br><span class="line">sync                      4 samples [reqs]</span><br><span class="line">samedir_rename            3 samples [reqs]</span><br><span class="line">crossdir_rename           3 samples [reqs]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ lctl set_param mdt.*.md_stats=clear</span><br><span class="line"></span><br><span class="line">$ lctl list_param mdt.*.rename_stats</span><br><span class="line">mdt.fsname-MDT0000.rename_stats</span><br><span class="line">$ lctl get_param mdt.*.rename_stats</span><br><span class="line">mdt.fsname-MDT0000.rename_stats=</span><br><span class="line">rename_stats:</span><br><span class="line">- snapshot_time:  1619838907.404363357</span><br><span class="line">- same_dir</span><br><span class="line">      16KB: &#123; sample:   3, pct: 100, cum_pct: 100 &#125;</span><br><span class="line">- crossdir_src</span><br><span class="line">      16KB: &#123; sample:   3, pct: 100, cum_pct: 100 &#125;</span><br><span class="line">- crossdir_tgt</span><br><span class="line">      16KB: &#123; sample:   3, pct: 100, cum_pct: 100 &#125;</span><br><span class="line"></span><br><span class="line">$ lctl get_param mds.MDS.mdt.stats</span><br><span class="line">mds.MDS.mdt.stats=</span><br><span class="line">snapshot_time             1619836421.293308463 secs.nsecs</span><br><span class="line">req_waittime              13090 samples [usec] 5 994 1420115 197067923 (max, min, sum, sum of squares)</span><br><span class="line">req_qdepth                13090 samples [reqs] 0 1 1 1</span><br><span class="line">req_active                13090 samples [reqs] 1 2 13094 13102</span><br><span class="line">req_timeout               13090 samples [sec] 50 50 654500 32725000</span><br><span class="line">reqbuf_avail              26152 samples [bufs] 63 64 1673657 107109575</span><br><span class="line">ldlm_ibits_enqueue        519 samples [reqs] 1 1 519 519</span><br><span class="line">mds_reint_rename          2 samples [reqs] 1 1 2 2</span><br><span class="line">mds_reint_open            162 samples [reqs] 1 1 162 162</span><br><span class="line">mds_getattr               2 samples [usec] 65 75 140 9850</span><br><span class="line">mds_getattr_lock          3 samples [usec] 44 192 296 42400</span><br><span class="line">mds_connect               5 samples [usec] 17 10421 10865 108662423</span><br><span class="line">mds_disconnect            1 samples [usec] 210 210 210 44100</span><br><span class="line">mds_get_root              2 samples [usec] 16 41 57 1937</span><br><span class="line">mds_statfs                22 samples [usec] 27 203 1414 122592</span><br><span class="line">obd_ping                  12530 samples [usec] 4 565 439527 19537985</span><br><span class="line"></span><br><span class="line">$ lctl get_param mds.MDS.mdt_setattr.stats</span><br><span class="line">mds.MDS.mdt_setattr.stats=</span><br><span class="line">snapshot_time             1619836714.197733538 secs.nsecs</span><br><span class="line"></span><br><span class="line"><span class="comment"># Metadata readdir service</span></span><br><span class="line">$ lctl get_param mds.MDS.mdt_readpage.stats</span><br><span class="line">mds.MDS.mdt_readpage.stats=</span><br><span class="line">snapshot_time             1619794903.241168757 secs.nsecs</span><br><span class="line">req_waittime              13 samples [usec] 10 86 509 25623</span><br><span class="line">req_qdepth                13 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                13 samples [reqs] 1 1 13 13</span><br><span class="line">req_timeout               13 samples [sec] 50 50 650 32500</span><br><span class="line">reqbuf_avail              32 samples [bufs] 63 64 2042 130310</span><br><span class="line">mds_close                 7 samples [usec] 21 153 578 60484</span><br><span class="line">mds_readpage              6 samples [usec] 270 498 2423 1031057</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*0000-osc-*.stats</span><br><span class="line">osc.fsname-OST0000-osc-MDT0000.stats=</span><br><span class="line">snapshot_time             1619794678.828478520 secs.nsecs</span><br><span class="line">req_waittime              18644 samples [usec] 237 8040 18490448 19195339776</span><br><span class="line">req_active                18644 samples [reqs] 1 2 18645 18647</span><br><span class="line">ost_create                2 samples [usec] 237 685 922 525394</span><br><span class="line">ost_get_info              1 samples [usec] 1951 1951 1951 3806401</span><br><span class="line">ost_connect               1 samples [usec] 1074 1074 1074 1153476</span><br><span class="line">ost_statfs                18640 samples [usec] 306 8040 18486501 19189854505</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.services.ldlm_canceld.stats</span><br><span class="line">ldlm.services.ldlm_canceld.stats=</span><br><span class="line">snapshot_time             1619836833.236763001 secs.nsecs</span><br><span class="line">req_waittime              20 samples [usec] 17 683 2564 723094</span><br><span class="line">req_qdepth                20 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                20 samples [reqs] 1 1 20 20</span><br><span class="line">req_timeout               20 samples [sec] 50 50 1000 50000</span><br><span class="line">reqbuf_avail              53 samples [bufs] 63 64 3388 216580</span><br><span class="line">ldlm_cancel               20 samples [usec] 5 224 1432 162534</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.services.ldlm_cbd.stats</span><br><span class="line">ldlm.services.ldlm_cbd.stats=</span><br><span class="line">snapshot_time             1619836850.112362316 secs.nsecs</span><br><span class="line">req_waittime              5 samples [usec] 25 189 549 80843</span><br><span class="line">req_qdepth                5 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                5 samples [reqs] 1 1 5 5</span><br><span class="line">req_timeout               5 samples [sec] 50 50 250 12500</span><br><span class="line">reqbuf_avail              13 samples [bufs] 1 1 13 13</span><br><span class="line">ldlm_bl_callback          5 samples [usec] 14 67 146 6134</span><br><span class="line"></span><br><span class="line">$ lctl --device MGS llog_print <span class="variable">$&#123;fsname&#125;</span>-MDT0000</span><br><span class="line">$ lctl get_param mdc.*.import | grep <span class="string">&quot;target: <span class="variable">$FSNAME</span>-MDT&quot;</span></span><br><span class="line">$ lctl get_param mdc.*.import | grep <span class="string">&quot;connect_flags&quot;</span> | grep disp_stripe</span><br><span class="line">$ lctl get_param mdc.*.import | grep <span class="string">&quot;import flags&quot;</span> <span class="comment">### compare with under export flags</span></span><br><span class="line">$ lct get_param -n mgc.*.uuid</span><br><span class="line"></span><br><span class="line">$ lctl --device MGS llog_print <span class="variable">$fsname</span>-MDT0000     <span class="comment"># show all</span></span><br><span class="line">$ lctl --device MGS llog_print <span class="variable">$fsname</span>-MDT0000  <span class="variable">$star_index</span> <span class="variable">$end_index</span></span><br><span class="line">$ lctl --device MGS llog_cancel <span class="variable">$fsname</span>-MDT0000 <span class="variable">$wrong_id</span> <span class="comment">## delete wrong </span></span><br><span class="line"></span><br><span class="line">or</span><br><span class="line">$ cat /proc/fs/lfs/mgc/MGC192.168.0.1@tcp1/uuid</span><br><span class="line"></span><br><span class="line">$ lctl get_param -N mgs.MGS.exports.*</span><br><span class="line">mgs.MGS.exports.<span class="variable">$client_ip</span>@tcp1</span><br><span class="line"></span><br><span class="line"><span class="comment">## Got client uuid from mds</span></span><br><span class="line">$ lctl get_param mgs.MGS.exports.192.168.0.2@tcp1.export</span><br><span class="line">mgs.MGS.exports.192.168.0.2@tcp1.export=</span><br><span class="line">e3373379-bb9d-e581-58ee-ce0b83ce9779:</span><br><span class="line">    name: MGS</span><br><span class="line">    client: 192.168.0.2@tcp1</span><br><span class="line">    connect_flags: [ version, barrier, adaptive_timeouts, full20, imp_recov, bulk_mbits ]</span><br><span class="line">    connect_data:</span><br><span class="line">       flags: 0x2000011001002020</span><br><span class="line">       instance: 0</span><br><span class="line">       target_version: 2.12.3.0</span><br><span class="line">    export_flags: [  ]</span><br><span class="line"></span><br><span class="line">$ lctl get_param mgs.MGS.exports.192.168.0.2@tcp1.export | grep <span class="string">&#x27;export flags&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#define OBD_FAIL_MDS_LOV_PREP_CREATE 0x141</span></span><br><span class="line">$ lctl set_param fail_loc=0x80000141</span><br><span class="line"><span class="comment">#define OBD_FAIL_MDS_READLINK_EPROTO     0x143</span></span><br><span class="line">touch <span class="variable">$DIR</span>/<span class="variable">$tdir</span>/<span class="variable">$tfile</span> || <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">$ lctl set_param fail_loc=0x80000143</span><br><span class="line">ls -l /lfs/<span class="variable">$foo</span> &amp;&amp; <span class="built_in">echo</span> <span class="string">&quot;no error&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#define OBD_FAIL_OSD_LMA_INCOMPAT 0x194</span></span><br><span class="line">$ lctl set_param fail_loc=0x194</span><br><span class="line">ls -l <span class="variable">$wdir</span>/<span class="variable">$tfile</span> &amp;&amp; <span class="built_in">echo</span> <span class="string">&quot;no error&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#define OBD_FAIL_LDLM_ENQUEUE_OLD_EXPORT 0x30e</span></span><br><span class="line">touch <span class="variable">$DIR</span>/f74a</span><br><span class="line">lctl set_param fail_loc=0x8000030e</span><br><span class="line">ls <span class="variable">$DIR</span>/f74a</span><br><span class="line">lctl set_param fail_loc=0</span><br><span class="line"></span><br><span class="line"><span class="comment">#define OBD_FAIL_OSC_CHECKSUM_RECEIVE       0x408</span></span><br><span class="line">$ lctl set_param fail_loc=0x80000408</span><br><span class="line">$ dd <span class="keyword">if</span>=<span class="variable">$DIR</span>/<span class="variable">$tfile</span> of=/dev/null bs=1M || error <span class="string">&quot;dd read error: $?&quot;</span></span><br><span class="line">$ lctl set_param fail_loc=0</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n lod.*.stripesize</span><br><span class="line">1048576</span><br><span class="line"></span><br><span class="line"><span class="comment"># 72 bytes is the minimum space required to store striping</span></span><br><span class="line"><span class="comment"># information for a file striped across one OST:</span></span><br><span class="line"><span class="comment"># (sizeof(struct lov_user_md_v3) +</span></span><br><span class="line"><span class="comment">#  sizeof(struct lov_user_ost_data_v1))</span></span><br><span class="line"><span class="comment"># not work in 2.12.6 ?</span></span><br><span class="line">$ lctl set_param -n llite.*.default_easize 72</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0 means Disable O_APPEND striping, verify it works</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line">$ lctl get_param mdd.*.append_stripe_count</span><br><span class="line">mdd.lfs-MDT0000.append_stripe_count=1</span><br><span class="line">$ lctl set_param mdd.*.append_stripe_count=2</span><br><span class="line"></span><br><span class="line">$ lctl get_param mdd.*.append_pool</span><br><span class="line">mdd.lfs-MDT0000.append_pool=</span><br><span class="line">$ lctl set_param mdd.*.append_pool=<span class="string">&#x27;none&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Add pool</span></span><br><span class="line">MDS $ lctl pool_new &lt;fsname&gt;.&lt;poolname&gt;</span><br><span class="line">MDS $ lctl pool_add <span class="variable">$FSNAME</span>.pool1 OST[0-10/2] </span><br><span class="line">MDS $ lctl pool_list <span class="variable">$FSNAME</span></span><br><span class="line">MDS $ lctl setstripe -p <span class="variable">$FSNAME</span>.pool1 /lfs/<span class="built_in">test</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># OSTs by default only hold a transient atime that is updated when clients do read requests. Permanent atime is written to the MDT when the file is closed. However, on-disk atime is only updated if it is more than 60 seconds old </span></span><br><span class="line">In lfs 2.14, it is possible to <span class="built_in">set</span> the OSTs to persistently store atime with each object, <span class="keyword">in</span> order to get more accurate persistent atime updates <span class="keyword">for</span> files that are open <span class="keyword">for</span> a long time via the similarly-named obdfilter.*.atime_diff parameter. </span><br><span class="line">$ lctl get_param -n mdd.*MDT0000*.atime_diff</span><br><span class="line">60</span><br><span class="line"></span><br><span class="line">$ lctl get_param osp.*.prealloc_last_id</span><br><span class="line">osp.lfs-OST0000-osc-MDT0000.prealloc_last_id=121337627</span><br><span class="line">osp.lfs-OST0001-osc-MDT0000.prealloc_last_id=122220489</span><br><span class="line">osp.lfs-OST0002-osc-MDT0000.prealloc_last_id=62659128</span><br><span class="line"></span><br><span class="line">$ lctl get_param osp.*.prealloc_next_id</span><br><span class="line">osp.lfs-OST0000-osc-MDT0000.prealloc_next_id=121337612</span><br><span class="line">osp.lfs-OST0001-osc-MDT0000.prealloc_next_id=122220456</span><br><span class="line">osp.lfs-OST0002-osc-MDT0000.prealloc_next_id=62659083</span><br><span class="line"></span><br><span class="line"><span class="comment"># With lfs software version 2.8, a new tunable is available to allow users with a specific group ID to create and delete remote and striped directories. This tunable is enable_remote_dir_gid. For example, setting this parameter to the &#x27;wheel&#x27; or &#x27;admin&#x27; group ID allows users with that GID to create and delete remote and striped directories. Setting this parameter to -1 on MDT0000 to permanently allow any non-root users create and delete remote and striped directories. On the MGS execute the following command: </span></span><br><span class="line">$ lctl get_param mdt.*.enable_remote_dir_gid</span><br><span class="line">mdt.fsname-MDT0000.enable_remote_dir_gid=0</span><br><span class="line">$ lctl get_param mdt.*.enable_remote_dir_gid=-1</span><br><span class="line"></span><br><span class="line"><span class="comment">##defualt </span></span><br><span class="line">$ lctl get_param mdt.*.enable_remote_dir_gid</span><br><span class="line">mdt.lfs-MDT0000.enable_remote_dir_gid=0</span><br><span class="line"></span><br><span class="line">$ lctl lfsck_start -M $(facet_svc mds1) -A -C -t namespace</span><br><span class="line">$ lctl lfsck_start --device $(facet_svc mds1) -A -C -t namespace</span><br><span class="line">$ lctl get_param mdd.*.lfsck_namespace<span class="string">&quot;</span></span><br><span class="line"><span class="string">mdd.lfs-MDT0000.lfsck_namespace=</span></span><br><span class="line"><span class="string">name: lfsck_namespace</span></span><br><span class="line"><span class="string">magic: 0xa06249ff</span></span><br><span class="line"><span class="string">version: 2</span></span><br><span class="line"><span class="string">status: init</span></span><br><span class="line"><span class="string">flags:</span></span><br><span class="line"><span class="string">param:</span></span><br><span class="line"><span class="string">last_completed_time: N/A</span></span><br><span class="line"><span class="string">time_since_last_completed: N/A</span></span><br><span class="line"><span class="string">latest_start_time: N/A</span></span><br><span class="line"><span class="string">time_since_latest_start: N/A</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ lctl get_param  lo[vd].*-mdtlov.qos_prio_free</span></span><br><span class="line"><span class="string">lod.fsname-MDT0000-mdtlov.qos_prio_free=91%</span></span><br><span class="line"><span class="string">lov.fsname-MDT0000-mdtlov.qos_prio_free=91%</span></span><br><span class="line"><span class="string">#This setting controls how much lfs prioritizes free space (versus location) in allocation. The higher this number, the more lfs takes empty space on an OST into consideration for its allocation. When set to 100%, lfs uses ONLY empty space as the deciding factor for writes. Remember, this setting is only taken into consideration when lfs believes the OSTs to be imbalanced If you have set qos_threshold_rr to 100, this setting will have no effect. </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ lctl get_param *.*MDT0000-mdtlov.qos_threshold_rr</span></span><br><span class="line"><span class="string">lod.fsname-MDT0000-mdtlov.qos_threshold_rr=17% ## set 100 means forces lfs to round-robin because it believes the OSTs are balanced</span></span><br><span class="line"><span class="string">lov.fsname-MDT0000-mdtlov.qos_threshold_rr=17%</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">## open files</span></span><br><span class="line"><span class="string">$ cat /proc/fs/lustre/mdt/*/exports/*/open_files</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>

<h3 id="OSS"><a href="#OSS" class="headerlink" title="OSS"></a>OSS</h3><h4 id="format"><a href="#format" class="headerlink" title="format"></a>format</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### in the over loading storage, increase the value</span></span><br><span class="line">$ lctl get_param -n ost.*.ost_io.timeouts</span><br><span class="line"></span><br><span class="line"><span class="comment">## grant info</span></span><br><span class="line">$ lctl get_param ldlm.namespaces.testfs-MDT0000*.pool.*</span><br><span class="line"></span><br><span class="line">$ lctl get_param obdfilter.*.filesfree</span><br><span class="line">obdfilter.fsname-OST0000.filesfree=4183295296</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n osd-*.*OST0000.kbytesfree</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.namespaces.*.max_nolock_bytes</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.filter-fsname-OST0000_UUID.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.max_nolock_bytes=0</span><br><span class="line">$ lctl get_param ldlm.namespaces.*.contention_seconds</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.contention_seconds=2</span><br><span class="line">ldlm.namespaces.filter-fsname-OST0000_UUID.contention_seconds=2</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.contention_seconds=2</span><br><span class="line">$ lctl get_param ldlm.namespaces.*.contended_locks</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.contended_locks=32</span><br><span class="line">ldlm.namespaces.filter-fsname-OST0000_UUID.contended_locks=32</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.contended_locks=32</span><br><span class="line"></span><br><span class="line"><span class="comment">#contended_locks- If the number of lock conflicts in the scan of granted and waiting queues at contended_locks is exceeded, the resource is considered to be contended.</span></span><br><span class="line"><span class="comment">#contention_seconds- The resource keeps itself in a contended state as set in the parameter.</span></span><br><span class="line"><span class="comment">#max_nolock_bytes- Server-side locking set only for requests less than the blocks set in the max_nolock_bytes parameter. If this tunable is set to zero (0), it disables server-side locking for read/write requests.</span></span><br><span class="line"></span><br><span class="line">$ lctl set_param -n ldlm.namespaces.*.max_nolock_bytes=2000000</span><br><span class="line">$ lctl set_param -n ldlm.namespaces.*.max_nolock_bytes=0</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.lock_reclaim_threshold_mb</span><br><span class="line">ldlm.lock_reclaim_threshold_mb=736</span><br><span class="line"></span><br><span class="line"><span class="comment">#Monitor all OSS</span></span><br><span class="line">$ lctl get_param obdfilter.*.stats obdfilter.*OST*.kbytesfree ldlm.namespaces.filter-*.pool.granted</span><br><span class="line">obdfilter.fsname-OST0000.stats=</span><br><span class="line">snapshot_time             1619839622.706200894 secs.nsecs</span><br><span class="line">read_bytes                130 samples [bytes] 4096 1048576 109461504</span><br><span class="line">write_bytes               115 samples [bytes] 444 1048576 105471940</span><br><span class="line">setattr                   10 samples [reqs]</span><br><span class="line">punch                     4 samples [reqs]</span><br><span class="line">sync                      4 samples [reqs]</span><br><span class="line">destroy                   9 samples [reqs]</span><br><span class="line">create                    3 samples [reqs]</span><br><span class="line">statfs                    27512 samples [reqs]</span><br><span class="line">get_info                  5 samples [reqs]</span><br><span class="line"></span><br><span class="line">$ lctl set_param obdfilter.*.stats=clear</span><br><span class="line"></span><br><span class="line">$ lctl get_param obdfilter.*OST*.exports.*@*.stats</span><br><span class="line"></span><br><span class="line">$ lctl get_param obdfilter.*.exports.*.stats</span><br><span class="line">obdfilter.fsname-OST0000.exports.192.168.0.238@tcp.stats=</span><br><span class="line">snapshot_time             1619839674.573369178 secs.nsecs</span><br><span class="line">setattr                   1 samples [reqs]</span><br><span class="line">destroy                   9 samples [reqs]</span><br><span class="line">create                    3 samples [reqs]</span><br><span class="line">statfs                    27498 samples [reqs]</span><br><span class="line">get_info                  1 samples [reqs]</span><br><span class="line">obdfilter.fsname-OST0000.exports.192.168.0.233@tcp.stats=</span><br><span class="line">snapshot_time             1619839674.573451068 secs.nsecs</span><br><span class="line">read_bytes                130 samples [bytes] 4096 1048576 109461504</span><br><span class="line">write_bytes               115 samples [bytes] 444 1048576 105471940</span><br><span class="line">setattr                   9 samples [reqs]</span><br><span class="line">punch                     4 samples [reqs]</span><br><span class="line">sync                      4 samples [reqs]</span><br><span class="line">statfs                    25 samples [reqs]</span><br><span class="line">get_info                  4 samples [reqs]</span><br><span class="line"></span><br><span class="line">$ lctl set_param obdfilter.*.exports.*.stats=clear</span><br><span class="line"></span><br><span class="line">$ lctl get_param ost.OSS.ost.stats</span><br><span class="line">ost.OSS.ost.stats=</span><br><span class="line">snapshot_time             1619837407.840675602 secs.nsecs</span><br><span class="line">req_waittime              1786 samples [usec] 18 309 235121 35937999</span><br><span class="line">req_qdepth                1786 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                1786 samples [reqs] 1 2 1788 1792</span><br><span class="line">req_timeout               1786 samples [sec] 50 50 89300 4465000</span><br><span class="line">reqbuf_avail              5344 samples [bufs] 61 64 338434 21434154</span><br><span class="line">ldlm_glimpse_enqueue      5 samples [reqs] 1 1 5 5</span><br><span class="line">ldlm_extent_enqueue       3 samples [reqs] 1 1 3 3</span><br><span class="line">ost_create                2 samples [usec] 49 528 577 281185</span><br><span class="line">ost_get_info              1 samples [usec] 1498 1498 1498 2244004</span><br><span class="line">ost_connect               3 samples [usec] 669 1574 3159 3764093</span><br><span class="line">ost_disconnect            1 samples [usec] 305 305 305 93025</span><br><span class="line">obd_ping                  1771 samples [usec] 12 126 78662 3931506</span><br><span class="line"></span><br><span class="line">$ lctl get_param ost.OSS.ost_create.stats</span><br><span class="line">ost.OSS.ost_create.stats=</span><br><span class="line">snapshot_time             1619837439.899464538 secs.nsecs</span><br><span class="line">req_waittime              27072 samples [usec] 23 523 3523516 532341790</span><br><span class="line">req_qdepth                27072 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                27072 samples [reqs] 1 1 27072 27072</span><br><span class="line">req_timeout               27072 samples [sec] 50 50 1353600 67680000</span><br><span class="line">reqbuf_avail              54351 samples [bufs] 63 64 3424493 215767379</span><br><span class="line">ost_statfs                27072 samples [usec] 21 252 1600400 103030806</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.services.ldlm_canceld.stats</span><br><span class="line">ldlm.services.ldlm_canceld.stats=</span><br><span class="line">snapshot_time             1619837467.579931100 secs.nsecs</span><br><span class="line">req_waittime              3 samples [usec] 100 171 423 62345</span><br><span class="line">req_qdepth                3 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                3 samples [reqs] 1 1 3 3</span><br><span class="line">req_timeout               3 samples [sec] 50 50 150 7500</span><br><span class="line">reqbuf_avail              9 samples [bufs] 64 64 576 36864</span><br><span class="line">ldlm_cancel               3 samples [usec] 49 121 248 23126</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.services.ldlm_canceld.stats</span><br><span class="line">ldlm.services.ldlm_canceld.stats=</span><br><span class="line">snapshot_time             1619837467.579931100 secs.nsecs</span><br><span class="line">req_waittime              3 samples [usec] 100 171 423 62345</span><br><span class="line">req_qdepth                3 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                3 samples [reqs] 1 1 3 3</span><br><span class="line">req_timeout               3 samples [sec] 50 50 150 7500</span><br><span class="line">reqbuf_avail              9 samples [bufs] 64 64 576 36864</span><br><span class="line">ldlm_cancel               3 samples [usec] 49 121 248 23126</span><br><span class="line">$ lctl get_param ldlm.services.ldlm_cbd.stats</span><br><span class="line">ldlm.services.ldlm_cbd.stats=</span><br><span class="line">snapshot_time             1619837479.839607820 secs.nsecs</span><br><span class="line"></span><br><span class="line"><span class="comment"># mdt</span></span><br><span class="line">FSNAME=lfs</span><br><span class="line">MDS1NID=1.1.1.1@tcp</span><br><span class="line">MDS2NID=2.2.2.2@tcp</span><br><span class="line"></span><br><span class="line">mkfs.lfs --mgs --backfstype=zfs --fsname=<span class="variable">$&#123;FSNAME&#125;</span> --servicenode=<span class="variable">$&#123;MDS1NID&#125;</span> --servicenode=<span class="variable">$&#123;MDS2NID&#125;</span> mdt_0/mgt_0</span><br><span class="line">mkfs.lfs --mdt --mgsnode=<span class="variable">$&#123;MDS1NID&#125;</span> --mgsnode=<span class="variable">$&#123;MDS2NID&#125;</span> --backfstype=zfs --fsname=<span class="variable">$&#123;FSNAME&#125;</span> --servicenode=<span class="variable">$&#123;MDS1NID&#125;</span> --servicenode=<span class="variable">$&#123;MDS2NID&#125;</span> --index=0 mdt_0/mdt_0</span><br><span class="line"></span><br><span class="line"><span class="comment">## add parameters</span></span><br><span class="line">mkfs.lfs --mkfsoptions=“-E stride=32,stripe_width=256” --ost –mgsnode=192.168.0.22@tcp /dev/sda1</span><br><span class="line"></span><br><span class="line"><span class="comment"># ost</span></span><br><span class="line">FSNAME=lfs</span><br><span class="line">MDS1NID=1.1.1.1@tcp</span><br><span class="line">MDS2NID=2.2.2.2@tcp</span><br><span class="line">OSS1NID=3.3.3.3@tcp </span><br><span class="line">OSS2NID=4.4.4.4@tcp </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;0..6&#125;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> mkfs.lfs --reformat --backfstype=zfs --ost  --index=<span class="variable">$i</span>  --fsname=<span class="variable">$&#123;FSNAME&#125;</span> --servicenode=<span class="variable">$&#123;OSS1NID&#125;</span> --servicenode=<span class="variable">$&#123;OSS2NID&#125;</span> --mgsnode=<span class="variable">$&#123;MDS1NID&#125;</span> --mgsnode=<span class="variable">$&#123;MDS2NID&#125;</span>  ost_<span class="variable">$i</span>/ost_<span class="variable">$i</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<h4 id="change-ipaddr"><a href="#change-ipaddr" class="headerlink" title="change ipaddr"></a>change ipaddr</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tunefs.lfs --erase-params --mgsnode=<span class="variable">$new_mgs_ip</span>@tcp --servicenode=<span class="variable">$new_service_ip</span>@tcp0 --writeconf /dev/md4</span><br></pre></td></tr></table></figure>

<h4 id="Enable-large-dir-feature"><a href="#Enable-large-dir-feature" class="headerlink" title="Enable large_dir feature"></a>Enable large_dir feature</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ tune2fs -O large_dir /dev/nvme0n1p1</span><br><span class="line">$ dumpe2fs -h /dev/nvme0n1p1 | grep feat</span><br><span class="line">$ dumpe2fs 1.45.2.wc1 (27-May-2019)</span><br><span class="line">Filesystem features:      has_journal ext_attr resize_inode dir_index filetype needs_recovery mmp flex_bg ea_inode dirdata large_dir sparse_super large_file huge_file uninit_bg dir_nlink quota</span><br><span class="line">Journal features:         journal_incompat_revoke</span><br></pre></td></tr></table></figure>

<h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><h4 id="set-stripe-size-for-tiny-files"><a href="#set-stripe-size-for-tiny-files" class="headerlink" title="set stripe size for tiny files"></a>set stripe size for tiny files</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br></pre></td><td class="code"><pre><span class="line">client $ lctl get_param -n llite.*.client_type</span><br><span class="line"><span class="built_in">local</span> client</span><br><span class="line"></span><br><span class="line">client $ lctl get_param osc.fsname-OST0000-osc*.resend_count</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.resend_count=10</span><br><span class="line"></span><br><span class="line">client $ lctl get_param osc.*-osc*.rpc_stats</span><br><span class="line"></span><br><span class="line">osc.testfs1-OST0000-osc-ffff9a2736998000.rpc_stats=</span><br><span class="line">snapshot_time:         1647398762.213066047 (secs.nsecs)</span><br><span class="line"><span class="built_in">read</span> RPCs <span class="keyword">in</span> flight:  0</span><br><span class="line">write RPCs <span class="keyword">in</span> flight: 0</span><br><span class="line">pending write pages:  0</span><br><span class="line">pending <span class="built_in">read</span> pages:   0</span><br><span class="line"></span><br><span class="line">                        <span class="built_in">read</span>                    write</span><br><span class="line">pages per rpc         rpcs   % cum % |       rpcs   % cum %</span><br><span class="line">1:                      41   0   0   |         41   1   1</span><br><span class="line">2:                       0   0   0   |         10   0   1</span><br><span class="line">4:                       7   0   1   |         18   0   1</span><br><span class="line">8:                       1   0   1   |         48   1   3</span><br><span class="line">16:                      3   0   1   |         40   1   4</span><br><span class="line">32:                     10   0   1   |         31   0   5</span><br><span class="line">64:                      3   0   1   |         24   0   5</span><br><span class="line">128:                    17   0   1   |         70   1   7</span><br><span class="line">256:                  4279  98 100   |       3343  92 100</span><br><span class="line"></span><br><span class="line">client $ lctl get_param llite.*.read_ahead_stats</span><br><span class="line">llite.ltfs1-ffff9a2736998000.read_ahead_stats=</span><br><span class="line">snapshot_time             1647398871.147899109 secs.nsecs</span><br><span class="line">hits                      857597 samples [pages]</span><br><span class="line">misses                    664 samples [pages]</span><br><span class="line">failed grab_cache_page    10319 samples [pages]</span><br><span class="line"><span class="built_in">read</span> but discarded        82760 samples [pages]</span><br><span class="line">zero size window          468754 samples [pages]</span><br><span class="line">read-ahead to EOF         242 samples [pages]</span><br><span class="line">hit max r-a issue         965 samples [pages]</span><br><span class="line">failed to reach end       11560 samples [pages]</span><br><span class="line"></span><br><span class="line">client $ lctl get_param llite.*.*<span class="built_in">read</span>*</span><br><span class="line">llite.ltfs1-ffff9a2736998000.fast_read=1</span><br><span class="line">llite.ltfs1-ffff9a2736998000.max_read_ahead_mb=64</span><br><span class="line">llite.ltfs1-ffff9a2736998000.max_read_ahead_per_file_mb=64</span><br><span class="line">llite.ltfs1-ffff9a2736998000.max_read_ahead_whole_mb=64</span><br><span class="line">llite.ltfs1-ffff9a2736998000.read_ahead_stats=</span><br><span class="line">snapshot_time             1647398887.020011954 secs.nsecs</span><br><span class="line">hits                      857597 samples [pages]</span><br><span class="line">misses                    664 samples [pages]</span><br><span class="line">failed grab_cache_page    10319 samples [pages]</span><br><span class="line"><span class="built_in">read</span> but discarded        82760 samples [pages]</span><br><span class="line">zero size window          468754 samples [pages]</span><br><span class="line">read-ahead to EOF         242 samples [pages]</span><br><span class="line">hit max r-a issue         965 samples [pages]</span><br><span class="line">failed to reach end       11560 samples [pages]</span><br><span class="line"></span><br><span class="line">client $ lctl get_param osc.*.stats</span><br><span class="line"></span><br><span class="line">osc.testfs1-OST0000-osc-ffff9a2736998000.stats=</span><br><span class="line">snapshot_time             1647398966.425013406 secs.nsecs</span><br><span class="line">req_waittime              10027 samples [usec] 23 5567567 76549428 162550984799808</span><br><span class="line">req_active                10027 samples [reqs] 1 11 36957 214049</span><br><span class="line">ldlm_glimpse_enqueue      796 samples [reqs] 1 1 796 796</span><br><span class="line">ldlm_extent_enqueue       397 samples [reqs] 1 1 397 397</span><br><span class="line">read_bytes                4361 samples [bytes] 0 1048576 3514739172 3636771225513784</span><br><span class="line">write_bytes               3625 samples [bytes] 190 1048576 3514628763 3636770234184707</span><br><span class="line">ost_setattr               435 samples [usec] 40 2063486 2273570 4258904750384</span><br><span class="line">ost_read                  4361 samples [usec] 74 134260 26437092 465273858862</span><br><span class="line">ost_write                 3625 samples [usec] 69 5567567 47628317 157826643363867</span><br><span class="line">ost_connect               1 samples [usec] 3793 3793 3793 14386849</span><br><span class="line">ldlm_cancel               402 samples [usec] 51 5246 86898 115121580</span><br><span class="line">obd_ping                  10 samples [usec] 82 202 1243 168281</span><br><span class="line"></span><br><span class="line"><span class="comment">#deactivate imperative recovery</span></span><br><span class="line">$ lctl set_param mgs.MGS.live.testfs=<span class="string">&quot;state=disabled&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#activate imperative recovery</span></span><br><span class="line">$ lctl set_param mgs.MGS.live.testfs=<span class="string">&quot;state=full&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#disable xattr cache</span></span><br><span class="line">$ lctl set_param llite.*.xattr_cache=0</span><br><span class="line"></span><br><span class="line">$ lctl get_param  llite.*.fast_read</span><br><span class="line">llite.fsname-ffff8dab2cce8800.fast_read=1</span><br><span class="line"></span><br><span class="line"><span class="comment">#stripe info</span></span><br><span class="line">$ lctl get_param  lov.fsname-clilov-\*.stripe*</span><br><span class="line">lov.fsname-clilov-ffff8dab2cce8800.stripecount=1</span><br><span class="line">lov.fsname-clilov-ffff8dab2cce8800.stripeoffset=-1</span><br><span class="line">lov.fsname-clilov-ffff8dab2cce8800.stripesize=1048576</span><br><span class="line">lov.fsname-clilov-ffff8dab2cce8800.stripetype=1</span><br><span class="line"></span><br><span class="line">$ lctl get_param llite.*.xattr_cache</span><br><span class="line">llite.fsname-ffff8dab2cce8800.xattr_cache=1</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n mdc.*MDT0000*.blocksize</span><br><span class="line">4096</span><br><span class="line">$ lctl get_param mdc.*MDT0000*.active</span><br><span class="line">mdc.fsname-MDT0000-mdc-ffff8dab2cce8800.active=1</span><br><span class="line">$ lctl get_param mdc.*MDT0000*.state</span><br><span class="line">$ lctl get_param mdc.*MDT0000*.stats | grep -Ei <span class="string">&#x27;ost_read|ldlm_glimpse&#x27;</span></span><br><span class="line">$ lctl get_param mdc.*MDT0000*.timeouts</span><br><span class="line">mdc.fsname-MDT0000-mdc-ffff8dab2cce8800.timeouts=</span><br><span class="line">last reply : 1619838998, 0s ago</span><br><span class="line">network    : cur  50  worst  50 (at 1619704491, 134507s ago)   1   1   1   1</span><br><span class="line">portal 12  : cur  50  worst  50 (at 1619704491, 134507s ago)  50  50  50  50</span><br><span class="line">portal 17  : cur  50  worst  50 (at 1619708441, 130557s ago)  50  50   0  50</span><br><span class="line">portal 23  : cur  50  worst  50 (at 1619769781, 69217s ago)  50  50  50   0</span><br><span class="line">portal 30  : cur  50  worst  50 (at 1619769796, 69202s ago)  50   0   0   0</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*OST0000*.state</span><br><span class="line">$ lctl get_param osc.*OST0000*.stats</span><br><span class="line">$ lctl get_param osc.*OST0000*.timeouts</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.timeouts=</span><br><span class="line">last reply : 1619838981, 36s ago</span><br><span class="line">network    : cur  50  worst  50 (at 1619704492, 134525s ago)   1   1   1   1</span><br><span class="line">portal 28  : cur  50  worst  50 (at 1619704491, 134526s ago)  50  50  50  50</span><br><span class="line">portal 7   : cur  50  worst  50 (at 1619704492, 134525s ago)  50  50   0   0</span><br><span class="line">portal 6   : cur  50  worst  50 (at 1619769796, 69221s ago)  50   0   0  50</span><br><span class="line">portal 17  : cur  50  worst  50 (at 1619770886, 68131s ago)  50   0   0   0</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*OST0000*.unstable_stats</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.unstable_stats=</span><br><span class="line">unstable_pages:                    0</span><br><span class="line">unstable_mb:                       0</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*OST0000*.blocksize</span><br><span class="line">1048576</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Client-Based I/O Extent Size Survey</span></span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">disabled</span><br><span class="line"> write anything to this file to activate, <span class="keyword">then</span> <span class="string">&#x27;0&#x27;</span> or <span class="string">&#x27;disable&#x27;</span> to deactivate</span><br><span class="line"></span><br><span class="line"><span class="comment"># Monitor client all</span></span><br><span class="line">client $ lctl get_param llite.*.stats</span><br><span class="line">clinet $ lctl get_param llite.*.read_ahead_stat</span><br><span class="line">client $ lctl get_param osc.*-osc*.rpc_stats</span><br><span class="line"></span><br><span class="line">osc.testfs1-OST0000-osc-ffff9a2736998000.rpc_stats=</span><br><span class="line">snapshot_time:         1647398762.213066047 (secs.nsecs)</span><br><span class="line"><span class="built_in">read</span> RPCs <span class="keyword">in</span> flight:  0</span><br><span class="line">write RPCs <span class="keyword">in</span> flight: 0</span><br><span class="line">pending write pages:  0</span><br><span class="line">pending <span class="built_in">read</span> pages:   0</span><br><span class="line"></span><br><span class="line">                        <span class="built_in">read</span>                    write</span><br><span class="line">pages per rpc         rpcs   % cum % |       rpcs   % cum %</span><br><span class="line">1:                      41   0   0   |         41   1   1</span><br><span class="line">2:                       0   0   0   |         10   0   1</span><br><span class="line">4:                       7   0   1   |         18   0   1</span><br><span class="line">8:                       1   0   1   |         48   1   3</span><br><span class="line">16:                      3   0   1   |         40   1   4</span><br><span class="line">32:                     10   0   1   |         31   0   5</span><br><span class="line">64:                      3   0   1   |         24   0   5</span><br><span class="line">128:                    17   0   1   |         70   1   7</span><br><span class="line">256:                  4279  98 100   |       3343  92 100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#The file can be cleared and enabled by issuing the following command:</span></span><br><span class="line">$ lctl set_param llite.testfs-*.extents_stats=1</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=1</span><br><span class="line"></span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">snapshot_time:         1619838099.035887547 (secs.nsecs)</span><br><span class="line">                               <span class="built_in">read</span>       |                write</span><br><span class="line">      extents            calls    % cum%  |          calls    % cum%</span><br><span class="line">   0K -    4K :              0    0    0  |              0    0    0</span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">snapshot_time:         1619838110.284058497 (secs.nsecs)</span><br><span class="line">                               <span class="built_in">read</span>       |                write</span><br><span class="line">      extents            calls    % cum%  |          calls    % cum%</span><br><span class="line">   0K -    4K :              0    0    0  |              0    0    0</span><br><span class="line"></span><br><span class="line">$ dd <span class="keyword">if</span>=/dev/urandom of=test_5 bs=4k count=100</span><br><span class="line">100+0 records <span class="keyword">in</span></span><br><span class="line">100+0 records out</span><br><span class="line">409600 bytes (410 kB) copied, 0.0048456 s, 84.5 MB/s</span><br><span class="line"></span><br><span class="line">$ ls -l test_5</span><br><span class="line">-rw-r--r-- 1 root root 409600 May  1 11:02 test_5</span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">snapshot_time:         1619838165.554498620 (secs.nsecs)</span><br><span class="line">                               <span class="built_in">read</span>       |                write</span><br><span class="line">      extents            calls    % cum%  |          calls    % cum%</span><br><span class="line">   0K -    4K :              0    0    0  |              0    0    0</span><br><span class="line">   4K -    8K :              0    0    0  |            100  100  100</span><br><span class="line">$ dd <span class="keyword">if</span>=test_5 of=test_6 bs=16k count=25</span><br><span class="line">25+0 records <span class="keyword">in</span></span><br><span class="line">25+0 records out</span><br><span class="line">409600 bytes (410 kB) copied, 0.00201437 s, 203 MB/s</span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">snapshot_time:         1619838225.522203239 (secs.nsecs)</span><br><span class="line">                               <span class="built_in">read</span>       |                write</span><br><span class="line">      extents            calls    % cum%  |          calls    % cum%</span><br><span class="line">   0K -    4K :              0    0    0  |              0    0    0</span><br><span class="line">   4K -    8K :              0    0    0  |            100   80   80</span><br><span class="line">   8K -   16K :              0    0    0  |              0    0   80</span><br><span class="line">  16K -   32K :              0    0    0  |             25   20  100</span><br><span class="line"></span><br><span class="line">$ dd <span class="keyword">if</span>=test_5 of=test_7 bs=16k count=25 iflag=direct</span><br><span class="line">25+0 records <span class="keyword">in</span></span><br><span class="line">25+0 records out</span><br><span class="line">409600 bytes (410 kB) copied, 0.010213 s, 40.1 MB/s</span><br><span class="line"></span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">snapshot_time:         1619838404.667431860 (secs.nsecs)</span><br><span class="line">                               <span class="built_in">read</span>       |                write</span><br><span class="line">      extents            calls    % cum%  |          calls    % cum%</span><br><span class="line">   0K -    4K :              0    0    0  |              0    0    0</span><br><span class="line">   4K -    8K :              0    0    0  |            100   66   66</span><br><span class="line">   8K -   16K :              0    0    0  |              0    0   66</span><br><span class="line">  16K -   32K :             25  100  100  |             50   33  100</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*0000-osc-*.stats</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.stats=</span><br><span class="line">snapshot_time             1619795318.676450713 secs.nsecs</span><br><span class="line">req_waittime              1321 samples [usec] 282 81340 2770704 62526834430</span><br><span class="line">req_active                1321 samples [reqs] 1 27 2605 27949</span><br><span class="line">read_bytes                10 samples [bytes] 1048576 1048576 10485760 10995116277760</span><br><span class="line">write_bytes               85 samples [bytes] 32768 1048576 87162880 91210072981504</span><br><span class="line">ost_read                  10 samples [usec] 5584 15595 111076 1326778432</span><br><span class="line">ost_write                 85 samples [usec] 3899 81340 1826891 60616504085</span><br><span class="line">ost_connect               1 samples [usec] 2156 2156 2156 4648336</span><br><span class="line">ost_statfs                16 samples [usec] 282 753 8835 5208451</span><br><span class="line">ldlm_cancel               3 samples [usec] 547 1141 2390 2093894</span><br><span class="line">obd_ping                  1198 samples [usec] 319 1351 815234 569338326</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param osc.testfs-OST0001*.*max*   /  lctl get_param ldlm.namespaces.*.*max*</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.lru_max_age=3900000</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.max_parallel_ast=1024</span><br><span class="line">ldlm.namespaces.filter-testfs1-OST0000_UUID.lru_max_age=3900000</span><br><span class="line">ldlm.namespaces.filter-testfs1-OST0000_UUID.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.filter-testfs1-OST0000_UUID.max_parallel_ast=1024</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.lru_max_age=3900000</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.max_parallel_ast=1024</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param osc.testfs-OST0001*.*grant*  / lctl get_param ldlm.namespaces.*.pool.*grant*</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.grant_plan=115635</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.grant_rate=0</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.grant_speed=0</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.granted=0</span><br><span class="line">ldlm.namespaces.filter-testfs1-OST0000_UUID.pool.grant_plan=4276</span><br><span class="line">ldlm.namespaces.filter-testfs1-OST0000_UUID.pool.grant_rate=0</span><br><span class="line">ldlm.namespaces.filter-testfs1-OST0000_UUID.pool.grant_speed=-426</span><br><span class="line">ldlm.namespaces.filter-testfs1-OST0000_UUID.pool.granted=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.grant_plan=115635</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.grant_rate=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.grant_speed=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.granted=0</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param ldlm.namespaces.testfs1-MDT0000-*.*.*</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.cancel_rate=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.grant_plan=115635</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.grant_rate=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.grant_speed=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.granted=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.limit=1</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.recalc_period=10</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.server_lock_volume=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.state=</span><br><span class="line">LDLM pool state (ldlm-pool-testfs1-MDT0000-lwp-OST0000-1):</span><br><span class="line">  SLV: 0</span><br><span class="line">  CLV: 0</span><br><span class="line">  LVF: 1</span><br><span class="line">  GR:  0</span><br><span class="line">  CR:  0</span><br><span class="line">  GS:  0</span><br><span class="line">  G:   0</span><br><span class="line">  L:   1</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.stats=</span><br><span class="line">snapshot_time             1647398330.677429043 secs.nsecs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param obdfilter.*.brw_stats / lctl get_param osd-zfs.*.brw_stats</span><br><span class="line"></span><br><span class="line">obdfilter.testfs-OST0000.brw_stats=</span><br><span class="line">snapshot_time:         1593407004.726097825 (secs.nsecs)</span><br><span class="line"></span><br><span class="line">                           <span class="built_in">read</span>      |     write</span><br><span class="line">pages per bulk r/w     rpcs  % cum % |  rpcs        % cum %</span><br><span class="line">1:                 6006159   2   2   | 1239974   1   1</span><br><span class="line">2:                45845506  16  18   | 1809852   2   4</span><br><span class="line">4:                 1952464   0  18   | 53814   0   4</span><br><span class="line">8:                33533746  11  30   | 832520   1   5</span><br><span class="line">16:               12859649   4  35   | 91425   0   5</span><br><span class="line">32:                 620131   0  35   | 81660   0   5</span><br><span class="line">64:               38856531  13  49   | 307195   0   5</span><br><span class="line">128:               1697223   0  49   | 223084   0   6</span><br><span class="line">256:             142793845  50 100   | 69219436  93 100</span><br><span class="line"></span><br><span class="line">                           <span class="built_in">read</span>      |     write</span><br><span class="line">discontiguous pages    rpcs  % cum % |  rpcs        % cum %</span><br><span class="line">0:               284165254 100 100   | 1244480   1   1</span><br><span class="line">1:                       0   0 100   | 1810011   2   4</span><br><span class="line">2:                       0   0 100   | 14961   0   4</span><br><span class="line">3:                       0   0 100   | 38853   0   4</span><br><span class="line">4:                       0   0 100   | 10409   0   4</span><br><span class="line">5:                       0   0 100   | 23853   0   4</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param ost.OSS.ost_io.req_history</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#show it in the lnetctl</span></span><br><span class="line">$  lnetctl stats show</span><br><span class="line">statistics:</span><br><span class="line">...</span><br><span class="line">    resend_count: 0</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.<span class="variable">$FSNAME</span>*.checksums</span><br><span class="line"></span><br><span class="line"><span class="comment"># cancel_lru_locks</span></span><br><span class="line">$ lctl set_param -n ldlm.namespaces.*osc*.lru_size=clear</span><br><span class="line">$ lctl set_param -n ldlm.namespaces.*mdc*.lru_size=clear</span><br><span class="line">$ lctl get_param ldlm.namespaces.*.lock_unused_count</span><br><span class="line">ldlm.namespaces.MGC192.168.0.1@tcp.lock_unused_count=0</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffffa05245a8c800.lock_unused_count=488</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-ffffa05245a8c800.lock_unused_count=1</span><br><span class="line">ldlm.namespaces.fsname-OST0001-osc-ffffa05245a8c800.lock_unused_count=0</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.namespaces.*mdc-*.lru_size</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lru_size=1</span><br><span class="line">$ lctl get_param ldlm.namespaces.*osc-*.lru_size</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-ffff8dab2cce8800.lru_size=0</span><br><span class="line"></span><br><span class="line">$ lctl get_param mdc.*.import </span><br><span class="line">$ lctl get_param mdc.*.import | grep <span class="string">&quot;state: FULL&quot;</span></span><br><span class="line">$ lctl get_param mdc.*.import | grep <span class="string">&quot;connect_flags&quot;</span>  </span><br><span class="line">or </span><br><span class="line">$ lctl get_param  mdc.*.connect_flags | grep early_lock_cancel</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n llite.*.sbi_flags</span><br><span class="line">checksum            acl       lru_resize lazy_statfs 64bit_hash agl verbose layout xattr_cache fast_read file_secctx</span><br><span class="line">checksum user_xattr acl flock lru_resize lazy_statfs 64bit_hash agl verbose layout xattr_cache fast_read file_secctx</span><br><span class="line"></span><br><span class="line">$ lfs setstripe -S 65536 /lfs</span><br><span class="line"></span><br><span class="line"><span class="comment"># Flush all of the metadata client (mdc) locks on this node</span></span><br><span class="line">$ lctl set_param ldlm.namespaces.*mdc*.lru_size=clear</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*.grant_shrink_interval</span><br><span class="line">osc.lfs-OST0000-osc-ffff8dab37559000.grant_shrink_interval=1200</span><br><span class="line">osc.lfs-OST0001-osc-ffff8dab37559000.grant_shrink_interval=1200</span><br><span class="line">osc.lfs-OST0002-osc-ffff8dab37559000.grant_shrink_interval=1200</span><br><span class="line"></span><br><span class="line"><span class="comment"># This example reports the amount of space this client has reserved for writeback cache with each OST</span></span><br><span class="line">$ lctl get_param osc.*.cur_grant_bytes</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.cur_grant_bytes=3407872</span><br><span class="line"></span><br><span class="line"><span class="comment">### calculator the max_cur_granted</span></span><br><span class="line">undirty 34209792 + grant_chunk 3407872 = max_cur_granted 37617664</span><br><span class="line"></span><br><span class="line"><span class="comment">#### grant_chunk</span></span><br><span class="line">$  lctl get_param osc.*.import  | grep -Ei <span class="string">&#x27;max_brw_size|grant_extent_tax&#x27;</span></span><br><span class="line">       max_brw_size: 1048576</span><br><span class="line">       grant_extent_tax: 655360</span><br><span class="line">grant_chunk = $(((<span class="number">1048576</span>+<span class="number">655360</span>)*<span class="number">2</span>)) = 3407872 bytes</span><br><span class="line"></span><br><span class="line"><span class="comment">#### undirty</span></span><br><span class="line">nrpages:256 rpc_in_flight:8</span><br><span class="line">rpc_in_flight++</span><br><span class="line">nrpegs 256 x  rpc_in_flight 9 = nrpages 2304</span><br><span class="line">max_dirty_mb:32 MB</span><br><span class="line">MB to pages = dirty_max_pages: 8192</span><br><span class="line"><span class="keyword">if</span> dirty_max_pages 8192 &gt; nrpages 2304; nrpages = dirty_max_pages:8192</span><br><span class="line">nrpages 8192 x 4096 = undirty 33554432 bytes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lctl grant_max_extent_size 1073741824</span><br><span class="line">lctl grant_max_extent_size = max_extent_size/4096 to pages:262144</span><br><span class="line">(nrpages 8192 + max_extent_pages 262144 -1) / max_extent_pages 262144 = nrextents = 1</span><br><span class="line">grant_extent_tax: 655360</span><br><span class="line">undirty 34209792 bytes = undirty bytes: 33554432 + nrextents 1 * grant_extent_tax (bytes):655360</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">client $ lctl get_param osc.*.grant_shrink</span><br><span class="line">client $ lctl set_param osc.*.grant_shrink=0</span><br><span class="line"><span class="comment"># replace</span></span><br><span class="line">$ bpftrace -e <span class="string">&#x27;k:osc_should_shrink_grant &#123; override(0); &#125;&#x27;</span> --unsafe</span><br><span class="line"></span><br><span class="line"><span class="comment">### clear cache info </span></span><br><span class="line">client $ lctl set_param osc.*-osc*.rpc_stats 0 </span><br><span class="line">client $ lctl set_param llite.*.read_ahead_stats 0</span><br><span class="line">client $ lctl set_param -n llite.*.max_cached_mb 128 <span class="comment">## 128MB</span></span><br><span class="line"><span class="comment">## randomly read 1000 of 32K chunks from file large than 1G size</span></span><br><span class="line">find /fsname -<span class="built_in">type</span> f -size +1G | head -n 128 | <span class="keyword">while</span> <span class="built_in">read</span> line; <span class="keyword">do</span> dd <span class="keyword">if</span>=<span class="variable">$line</span> of=/dev/null bs=128K count=1000 skip=$(shuf -i 2000-65000 -n 1) &amp; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## check status</span></span><br><span class="line">client $ lctl get_param osc.*-osc*.rpc_stats</span><br><span class="line"></span><br><span class="line">client $ lctl get_param llite.*.max_cached_mb</span><br><span class="line">llite.fsname-ffff8dab2cce8800.max_cached_mb=</span><br><span class="line">users: 16</span><br><span class="line">max_cached_mb: 128  <span class="comment">##128MB=32768 pages</span></span><br><span class="line">used_mb: 128</span><br><span class="line">unused_mb: 0</span><br><span class="line">reclaim_count: 36</span><br><span class="line"></span><br><span class="line"><span class="comment">## 128M</span></span><br><span class="line">client lctl set_param llite.*.read_ahead_stats=c</span><br><span class="line">llite.testfs-ffff8daf56159000.read_ahead_stats=c</span><br><span class="line"></span><br><span class="line">llite.testfs-ffff8dcc59a15800.fast_read=1</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_async_active=12</span><br><span class="line">llite.testfs-ffff8dcc59a15800.read_ahead_async_file_threshold_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.read_ahead_range_kb=1024</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_per_file_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_whole_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.read_ahead_stats=</span><br><span class="line">snapshot_time             1642131829.087651284 secs.nsecs</span><br><span class="line">hits                      2887533 samples [pages]</span><br><span class="line">misses                    627508 samples [pages] &lt;--------------------</span><br><span class="line">readpage not consecutive  1255012 samples [pages]                    |</span><br><span class="line">zero size window          2887886 samples [pages]                    |</span><br><span class="line">failed to fast <span class="built_in">read</span>       627508 samples [pages]                     |</span><br><span class="line">                                                                     |</span><br><span class="line">lctl get_param llite.testfs*.*<span class="built_in">read</span>*                                  |</span><br><span class="line">llite.testfs-ffff8db8208be800.fast_read=1                            |</span><br><span class="line">llite.testfs-ffff8db8208be800.max_read_ahead_mb=64                   |-------------dff misses samples <span class="keyword">in</span> the same <span class="keyword">case</span></span><br><span class="line">llite.testfs-ffff8db8208be800.max_read_ahead_per_file_mb=64          |</span><br><span class="line">llite.testfs-ffff8db8208be800.max_read_ahead_whole_mb=64             |</span><br><span class="line">llite.testfs-ffff8db8208be800.read_ahead_stats=                      |</span><br><span class="line">snapshot_time             1642130844.639513080 secs.nsecs            |</span><br><span class="line">hits                      19433183 samples [pages]                   |</span><br><span class="line">misses                    5 samples [pages] &lt;-------------------------</span><br><span class="line">readpage not consecutive  4 samples [pages]</span><br><span class="line">zero size window          12 samples [pages]</span><br><span class="line">failed to reach end       76012 samples [pages]</span><br><span class="line"></span><br><span class="line">Hits</span><br><span class="line">Misses</span><br><span class="line">Readpage not consecutive</span><br><span class="line">Miss inside window</span><br><span class="line">Failed grab_cache_page</span><br><span class="line">Failed lock match</span><br><span class="line">Read but discarded</span><br><span class="line">Zero length file</span><br><span class="line">Zero size window</span><br><span class="line">Read-ahead to EOF</span><br><span class="line">Hit max r-a issue</span><br><span class="line">Wrong page from</span><br><span class="line">grab_cache_page</span><br><span class="line"></span><br><span class="line">$ size=1; lctl get_param -n osc.*.rpc_stats | awk <span class="string">&#x27;($1 == &quot;&#x27;</span><span class="variable">$size</span><span class="string">&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2;&#125;&#x27;</span></span><br><span class="line">$ size=2; lctl get_param -n osc.*.rpc_stats | awk <span class="string">&#x27;($1 == &quot;&#x27;</span><span class="variable">$size</span><span class="string">&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2;&#125;&#x27;</span></span><br><span class="line">$ size=4; lctl get_param -n osc.*.rpc_stats | awk <span class="string">&#x27;($1 == &quot;&#x27;</span><span class="variable">$size</span><span class="string">&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2;&#125;&#x27;</span></span><br><span class="line">$ size=8; lctl get_param -n osc.*.rpc_stats | awk <span class="string">&#x27;($1 == &quot;&#x27;</span><span class="variable">$size</span><span class="string">&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2;&#125;&#x27;</span></span><br><span class="line">$ size=16; lctl get_param -n osc.*.rpc_stats | awk <span class="string">&#x27;($1 == &quot;&#x27;</span><span class="variable">$size</span><span class="string">&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2;&#125;&#x27;</span></span><br><span class="line">36 </span><br><span class="line"><span class="keyword">if</span> size 4 * PAGE_SIZE 4096 = 32768 &lt; rsize 131072(means dd bs=128k); means Small 16384 <span class="built_in">read</span> IO 36</span><br><span class="line"></span><br><span class="line">1K 2K 4K ... 256 K</span><br><span class="line"></span><br><span class="line"><span class="comment">## for example, just print first line </span></span><br><span class="line">$ size=1; lctl get_param -n osc.*.rpc_stats | awk <span class="string">&#x27;($1 == &quot;&#x27;</span><span class="variable">$size</span><span class="string">&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2; exit&#125;&#x27;</span></span><br><span class="line">209</span><br><span class="line">the size=1 * PAGE_SIZE 4096 = 4096 &lt; rsize 131072 (means dd bs=128k); means Small 4096 <span class="built_in">read</span> IO 209</span><br><span class="line">$ size=8; lctl get_param -n osc.*.rpc_stats | awk <span class="string">&#x27;($1 == &quot;&#x27;</span><span class="variable">$size</span><span class="string">&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2; exit&#125;&#x27;</span></span><br><span class="line">14</span><br><span class="line">the size=8 * PAGE_SIZE 4096 = 32768 &lt; rsize 131072 (means dd bs=128k); means Small 32768 <span class="built_in">read</span> IO 14</span><br><span class="line"></span><br><span class="line">$ $ lctl set_param osc.*-osc*.rpc_stats 0</span><br><span class="line">$ lctl set_param llite.*.read_ahead_stats 0</span><br><span class="line">$ lctl set_param -n llite.*.max_cached_mb 64 <span class="comment">#default 64</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## read, read_ahead cache</span></span><br><span class="line"><span class="comment">#disable the client read ahead </span></span><br><span class="line">$ lctl set_param llite.*.max_read_ahead_mb=0</span><br><span class="line"></span><br><span class="line"><span class="comment">#limit ldlm threads, ldlm threads will exhaust all CPUs resources like LU-7330</span></span><br><span class="line">options ptlrpc ldlm_num_threads=16</span><br><span class="line"></span><br><span class="line"><span class="comment">## In lfs script, the check stripe_size aligned read-ahead is very simple</span></span><br><span class="line"><span class="built_in">disable</span> readahead and <span class="built_in">set</span> stripe count = 1, compare the <span class="built_in">read</span> performance, <span class="keyword">if</span> not aligned, the result will slow than disabled readahead result.</span><br><span class="line"></span><br><span class="line"><span class="comment">####</span></span><br><span class="line">$ dd <span class="keyword">if</span>=/dev/zero of=test_3 bs=1M count=70</span><br><span class="line">$ lctl set_param -n ldlm.namespaces.*osc*.lru_size=clear</span><br><span class="line">$ lctl set_param -n ldlm.namespaces.*mdc*.lru_size=clear</span><br><span class="line">$ lctl set_param -n llite.*.read_ahead_stats 0</span><br><span class="line">$ dd <span class="keyword">if</span>=test_3 of=/dev/null bs=10M skip=6 count=1</span><br><span class="line">$ lctl  get_param -n llite.*.read_ahead_stats | grep <span class="string">&#x27;misses&#x27;</span></span><br><span class="line">misses                    1 samples [pages]</span><br><span class="line"><span class="keyword">if</span> misses = 1 , it <span class="string">&#x27;s ok, else the misses value is not right</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># setstripe</span></span><br><span class="line"><span class="string">$ lfs setstripe -S 4000M -c 50 /mnt/striped</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#Monitor</span></span><br><span class="line"><span class="string">$ lctl get_param  osc.*OST0000*.stats</span></span><br><span class="line"><span class="string">osc.$FSNAME-OST0000-osc-ffff8dab37559000.stats=</span></span><br><span class="line"><span class="string">snapshot_time             1619689024.314392020 secs.nsecs</span></span><br><span class="line"><span class="string">req_waittime              128 samples [usec] 138 2599 61091 35024171</span></span><br><span class="line"><span class="string">req_active                128 samples [reqs] 1 1 128 128</span></span><br><span class="line"><span class="string">ost_connect               1 samples [usec] 2599 2599 2599 6754801</span></span><br><span class="line"><span class="string">ost_statfs                3 samples [usec] 459 512 1433 686269</span></span><br><span class="line"><span class="string">obd_ping                  124 samples [usec] 138 575 57059 27583101</span></span><br><span class="line"><span class="string">$ lctl get_param mdc.lfs*.stats</span></span><br><span class="line"><span class="string">mdc.lfs-MDT0000-mdc-ffff8dab3afe6000.stats=</span></span><br><span class="line"><span class="string">snapshot_time             1619689557.206806185 secs.nsecs</span></span><br><span class="line"><span class="string">req_waittime              112 samples [usec] 71 1525 18376 7048052</span></span><br><span class="line"><span class="string">req_active                112 samples [reqs] 1 1 112 112</span></span><br><span class="line"><span class="string">mds_getattr               1 samples [usec] 86 86 86 7396</span></span><br><span class="line"><span class="string">mds_connect               1 samples [usec] 734 734 734 538756</span></span><br><span class="line"><span class="string">mds_get_root              1 samples [usec] 101 101 101 10201</span></span><br><span class="line"><span class="string">mds_statfs                3 samples [usec] 71 145 293 31995</span></span><br><span class="line"><span class="string">ldlm_cancel               1 samples [usec] 92 92 92 8464</span></span><br><span class="line"><span class="string">obd_ping                  103 samples [usec] 93 1525 16863 6429731</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">OSS $ llobdstat ost_name 2</span></span><br><span class="line"><span class="string">Read: 9.56529e+13, Write: 2.76168e+13, create/destroy: 1832/171649, stat: 1.1192e+07, punch: 52249</span></span><br><span class="line"><span class="string">[NOTE: cx: create, dx: destroy, st: statfs, pu: punch ]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Timestamp   Read-delta  ReadRate  Write-delta  WriteRate</span></span><br><span class="line"><span class="string">--------------------------------------------------------</span></span><br><span class="line"><span class="string">1637629257    0.00MB    0.00MB/s     0.00MB    0.00MB/s st:13</span></span><br><span class="line"><span class="string">1637629259    0.00MB    0.00MB/s     0.00MB    0.00MB/s st:14</span></span><br><span class="line"><span class="string">1637629261   16.00MB    8.00MB/s     0.00MB    0.00MB/s st:14</span></span><br><span class="line"><span class="string">1637629263    0.00MB    0.00MB/s     0.00MB    0.00MB/s st:12</span></span><br><span class="line"><span class="string">1637629265    0.00MB    0.00MB/s     0.00MB    0.00MB/s st:16</span></span><br><span class="line"><span class="string">1637629267    0.00MB    0.00MB/s     0.00MB    0.00MB/s st:16</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">OSS $ llstat -i 1 ost </span></span><br><span class="line"><span class="string">MDS $ llstat -i 1 mds</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">napshot_time             1637629207.425948584</span></span><br><span class="line"><span class="string">req_waittime              33338215</span></span><br><span class="line"><span class="string">req_qdepth                33338215</span></span><br><span class="line"><span class="string">req_active                33338215</span></span><br><span class="line"><span class="string">req_timeout               33338215</span></span><br><span class="line"><span class="string">reqbuf_avail              67080504</span></span><br><span class="line"><span class="string">ldlm_glimpse_enqueue      13497741</span></span><br><span class="line"><span class="string">ldlm_extent_enqueue       787370</span></span><br><span class="line"><span class="string">ost_setattr               388196</span></span><br><span class="line"><span class="string">ost_create                6643</span></span><br><span class="line"><span class="string">ost_destroy               690443</span></span><br><span class="line"><span class="string">ost_get_info              3191</span></span><br><span class="line"><span class="string">ost_connect               1721355</span></span><br><span class="line"><span class="string">ost_disconnect            1714914</span></span><br><span class="line"><span class="string">ost_sync                  79</span></span><br><span class="line"><span class="string">ost_set_info              5901</span></span><br><span class="line"><span class="string">ost_quotactl              2214221</span></span><br><span class="line"><span class="string">obd_ping                  12308161</span></span><br><span class="line"><span class="string">/proc/fs/lustre/ost/OSS/ost/stats @ 1637629208.426168645</span></span><br><span class="line"><span class="string">Name                      Cur.Count  Cur.Rate   #Events   Unit           last        min          avg        max    stddev</span></span><br><span class="line"><span class="string">req_waittime              6          6          33338221  [usec]          178          4       683.36    1846733  10378.79 </span></span><br><span class="line"><span class="string">req_qdepth                6          6          33338221  [reqs]            0          0         0.02       2958      3.50 </span></span><br><span class="line"><span class="string">req_active                6          6          33338221  [reqs]            8          1         2.20         79      7.54 </span></span><br><span class="line"><span class="string">req_timeout               6          6          33338221  [sec]           300         50        50.01         92      0.52 </span></span><br><span class="line"><span class="string">reqbuf_avail              12         12         67080516  [bufs]          739          0        62.16        117      2.89 </span></span><br><span class="line"><span class="string">ldlm_glimpse_enqueue      2          2          13497743  [reqs]            2          1         1.00          1      0.00 </span></span><br><span class="line"><span class="string">ldlm_extent_enqueue       0          0          787370    [reqs]            0          1         1.00          1      0.00 </span></span><br><span class="line"><span class="string">ost_setattr               0          0          388196    [usec]            0          2      9493.19   78450702 241902.62 </span></span><br><span class="line"><span class="string">ost_create                0          0          6643      [usec]            0     150442    994329.00   34318893 1112213.09 </span></span><br><span class="line"><span class="string">ost_destroy               0          0          690443    [usec]            0         74     29416.49   21468510 104767.69 </span></span><br><span class="line"><span class="string">ost_get_info              0          0          3191      [usec]            0          9      4283.79     447398  21558.91 </span></span><br><span class="line"><span class="string">ost_connect               0          0          1721355   [usec]            0          7      3217.33   20717452  29239.94 </span></span><br><span class="line"><span class="string">ost_disconnect            0          0          1714914   [usec]            0         34      5274.31   13105121  33557.39 </span></span><br><span class="line"><span class="string">ost_sync                  0          0          79        [usec]            0          9    555830.46    1691862 476579.36 </span></span><br><span class="line"><span class="string">ost_set_info              0          0          5901      [usec]            0          9        18.16         65      4.85 </span></span><br><span class="line"><span class="string">ost_quotactl              0          0          2214221   [usec]            0          8       798.77    7068501  10658.14 </span></span><br><span class="line"><span class="string">obd_ping                  4          4          12308165  [usec]           39          3         9.30        304      3.42 </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">## All OST available bytes</span></span><br><span class="line"><span class="string">$ lctl get_param lov.zfsz2-clilov-*.kbytesavail</span></span><br><span class="line"><span class="string">lov.zfsz2-clilov-ffff8dab3afe6000.kbytesavail=139702738176</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ lctl get_param lov.zfsz2-clilov-*.kbytestotal</span></span><br><span class="line"><span class="string">lov.zfsz2-clilov-ffff8dab3afe6000.kbytestotal=1038674946560</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ lctl get_param osc.*.rpc_stats</span></span><br><span class="line"><span class="string">snapshot_time:         1619688798.673860765 (secs.nsecs)</span></span><br><span class="line"><span class="string">read RPCs in flight:  0</span></span><br><span class="line"><span class="string">write RPCs in flight: 0</span></span><br><span class="line"><span class="string">pending write pages:  0</span></span><br><span class="line"><span class="string">pending read pages:   0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">			read			write</span></span><br><span class="line"><span class="string">pages per rpc         rpcs   % cum % |       rpcs   % cum %</span></span><br><span class="line"><span class="string">1:		         0   0   0   |          0   0   0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">			read			write</span></span><br><span class="line"><span class="string">rpcs in flight        rpcs   % cum % |       rpcs   % cum %</span></span><br><span class="line"><span class="string">0:		         0   0   0   |          0   0   0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">			read			write</span></span><br><span class="line"><span class="string">offset                rpcs   % cum % |       rpcs   % cum %</span></span><br><span class="line"><span class="string">0:		         0   0   0   |          0   0   0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ lctl lov_getconfig /mnt</span></span><br><span class="line"><span class="string">default_stripe_count: 1</span></span><br><span class="line"><span class="string">default_stripe_size: 1048576</span></span><br><span class="line"><span class="string">default_stripe_offset: 18446744073709551615</span></span><br><span class="line"><span class="string">default_stripe_pattern: 1</span></span><br><span class="line"><span class="string">obd_count: 15</span></span><br><span class="line"><span class="string">OBDS:	obdidx		obdgen		 obduuid</span></span><br><span class="line"><span class="string">	     0	             1		 lfs-OST0000_UUID</span></span><br><span class="line"><span class="string">	     1	             1		 lfs-OST0001_UUID</span></span><br><span class="line"><span class="string">	     2	             1		 lfs-OST0002_UUID</span></span><br><span class="line"><span class="string">	     3	             1		 lfs-OST0003_UUID</span></span><br><span class="line"><span class="string">	     4	             1		 lfs-OST0004_UUID</span></span><br><span class="line"><span class="string">	     5	             1		 lfs-OST0005_UUID</span></span><br><span class="line"><span class="string">	     6	             1		 lfs-OST0006_UUID</span></span><br><span class="line"><span class="string">	     7	             1		 lfs-OST0007_UUID</span></span><br><span class="line"><span class="string">	     8	             1		 lfs-OST0008_UUID</span></span><br><span class="line"><span class="string">	     9	             1		 lfs-OST0009_UUID</span></span><br><span class="line"><span class="string">	    10	             1		 lfs-OST000a_UUID</span></span><br><span class="line"><span class="string">	    11	             1		 lfs-OST000b_UUID</span></span><br><span class="line"><span class="string">	    12	             1		 lfs-OST000c_UUID</span></span><br><span class="line"><span class="string">	    13	             1		 lfs-OST000d_UUID</span></span><br><span class="line"><span class="string">	    14	             1		 lfs-OST000e_UUID</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ lctl get_param osc.*OST0000-osc-[^mM]*.cur_grant_bytes</span></span><br><span class="line"><span class="string">osc.fsname-OST0000-osc-ffff8dab37559000.cur_grant_bytes=2097152</span></span><br><span class="line"><span class="string">osc.zfsz2-OST0000-osc-ffff8dab3afe6000.cur_grant_bytes=3407872</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">###$ </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ lctl get_param -n osc.fsname-OST0000-osc*.import  | grep &quot;target:&quot;</span></span><br><span class="line"><span class="string">    target: fsname-OST0000_UUID</span></span><br><span class="line"><span class="string">obdfilter_name=fsname-OST0000_UUID</span></span><br><span class="line"><span class="string">### ???</span></span><br><span class="line"><span class="string">$ lctl --device $&#123;obdfilter_name&#125;_osc cleanup</span></span><br><span class="line"><span class="string">$ lctl --device $&#123;obdfilter_name&#125;_osc detach</span></span><br><span class="line"><span class="string">$ lctl attach echo_client ec ec_uuid</span></span><br><span class="line"><span class="string">$ lctl --device ec create 1 | awk &#x27;</span>/object id/ &#123;<span class="built_in">print</span> <span class="variable">$6</span>&#125;<span class="string">&#x27; </span></span><br><span class="line"><span class="string">## Get the id</span></span><br><span class="line"><span class="string">$ lctl --device ec getattr $id</span></span><br><span class="line"><span class="string">$ lctl --device ec getattr $id</span></span><br><span class="line"><span class="string">$ lctl --device ec</span></span><br><span class="line"><span class="string">$ lctl --device ec destroy $id 1</span></span><br><span class="line"><span class="string">$ lctl --device ec cleanup</span></span><br><span class="line"><span class="string">$ lctl --device ec detach</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># re-compile the lfs 2.13.0 client, you must import openmpi PATH</span></span><br><span class="line"><span class="string">$ export PATH=/usr/lib64/openmpi/bin/:$PATH</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#or you can delele all openmpi info from lfs.spec, and you could got these rpms</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#if you has the openmpi error when you rpmbuild, find the lfs.spec and delete all openmpi info</span></span><br><span class="line"><span class="string">$ rpmbuild -bb -v --without servers lfs.spec</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ rpmbuild --rebuild --without servers  lfs-2.10.3-1.src.rpm</span></span><br><span class="line"><span class="string">$ rpmbuild  --rebuild --without servers --with lnet-dlc  lfs-2.10.3-1.src.rpm</span></span><br><span class="line"><span class="string">$ rpmbuild --rebuild --without servers --without lfs-tests lfs-2.10.3-1.src.rpm</span></span><br><span class="line"><span class="string">$ rpmbuild --define &#x27;</span>kversion 2.6.32-220.4.1.el6.x86_64<span class="string">&#x27; --define &#x27;</span>kdir /usr/src/kernels/2.6.32-220.4.1.el6.x86_64/<span class="string">&#x27; --rebuild lustre-client-2.1.0-2.6.32_131.6.1.el6.x86_64_g9d71fe8.src.rpm</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># centos 7.7</span></span><br><span class="line"><span class="string"># /root/rpmbuild/BUILD/lfs-2.10.8/lnet/klnds/o2iblnd/o2iblnd.h:69:27: fatal error: linux/pci-dma.h: No such file or directory</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># copy the file from the old kernel</span></span><br><span class="line"><span class="string">copy /usr/src/kernels/$&#123;the old version kernel&#125;/include/linux/pci-dma.h /usr/src/kernels/$&#123;the centos 7.7 kernel&#125;/include/linux/pci-dma.h</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># New stupid bug when you compile lfs 2.10.3-1, if you are not export $PATH with openmpi, the compile will failed.</span></span><br><span class="line"><span class="string">If you want it pass, I was clear /tmp/tmp.* rpmbuild not help I guess maybe the old config in some tmpfs path.</span></span><br><span class="line"><span class="string">after you reboot and re-export the env, the compile will be successful.</span></span><br><span class="line"><span class="string">#Is real the realease production ? `too stupid` bug. just waste my time to type these words.</span></span><br><span class="line"><span class="string">There is no test team in lfs develop team, All users was the test team except you are going to buy DDN.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># from source</span></span><br><span class="line"><span class="string">$ ./configure --enable-client --disable-server --with-linux=/usr/src/kernels/$(uname -r) --with-linux-obj=/usr/src/kernels/$(uname -r);make rpms/deps</span></span><br><span class="line"><span class="string">$ ./configure --enable-client --disable-server --with-linux=/usr/src/kernels/linux-5.4.123 --with-linux-obj=/usr/src/kernels/linux-5.4.123;make rpms/deps</span></span><br><span class="line"><span class="string">## install in ubuntu 18.04</span></span><br><span class="line"><span class="string">$ apt install uuid-dev libblkid-dev dietlibc-dev</span></span><br><span class="line"><span class="string">$ apt install build-essential debhelper devscripts fakeroot kernel-wedge libudev-dev pciutils-dev</span></span><br><span class="line"><span class="string">$ apt install module-assistant libreadline-dev dpatch libsnmp-dev quilt</span></span><br><span class="line"><span class="string">$ apt install linux-headers-$(uname -r)</span></span><br><span class="line"><span class="string">$ cd $&#123;BUILDPATH&#125;/lfs-release</span></span><br><span class="line"><span class="string">$ git reset --hard &amp;&amp; git clean -dfx</span></span><br><span class="line"><span class="string">$ sh autogen.sh</span></span><br><span class="line"><span class="string">$ ./configure --disable-server --with-linux=/usr/src/linux-headers-4.15.0-64-generic</span></span><br><span class="line"><span class="string">$ make install</span></span><br><span class="line"><span class="string">$ rm -rf  /lib/modules/4.15.0-64-generic/kernel/drivers/staging/lfs/</span></span><br><span class="line"><span class="string">$ depmod -a</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># LU-14116 cause could not support rhel 8.3, you could replace krb5 rpms by rhel 8.2</span></span><br><span class="line"><span class="string">### set lfs client for a lot metadata ops</span></span><br><span class="line"><span class="string">Otherwise, the file attributes will be dropped from the client cache if the file has not been accessed before the LDLM lock timeout. The timeout is stored via lctl get_param ldlm.namespaces.*mdc*.lru_max_age</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#restricting the number of locks kept on the client (10000 locks, 10 minutes age)</span></span><br><span class="line"><span class="string">$ lctl set_param ldlm.namespaces.*.lru_size=10000 ldlm.namespaces.*.lru_max_age=600000</span></span><br><span class="line"><span class="string">$ lctl set_param ldlm.namespaces.*.lru_size=10000 ldlm.namespaces.*.lru_max_age=3900000</span></span><br><span class="line"><span class="string">$ lctl get_param ldlm.namespaces.*.lru_size ldlm.namespaces.*.lru_max_age</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># default</span></span><br><span class="line"><span class="string">MDS$ lctl get_param ldlm.namespaces.*.lru_size</span></span><br><span class="line"><span class="string">ldlm.namespaces.MGC192.168.0.1@tcp.lru_size=400</span></span><br><span class="line"><span class="string">ldlm.namespaces.MGS.lru_size=400</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.lru_size=0</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-OST0000-osc-MDT0000.lru_size=0</span></span><br><span class="line"><span class="string">ldlm.namespaces.mdt-fsname-MDT0000_UUID.lru_size=400</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">OSS$ lctl get_param ldlm.namespaces.*.lru_size</span></span><br><span class="line"><span class="string">ldlm.namespaces.MGC192.168.0.1@tcp.lru_size=400</span></span><br><span class="line"><span class="string">ldlm.namespaces.filter-fsname-OST0000_UUID.lru_size=400</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.lru_size=0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">client $ lctl get_param ldlm.namespaces.*.lru_size</span></span><br><span class="line"><span class="string">ldlm.namespaces.MGC192.168.0.1@tcp.lru_size=2400</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lru_size=0</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-OST0000-osc-ffff8dab2cce8800.lru_size=0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># The lru_size parameter is used to control the number of client-side locks in the LRU cached locks queue</span></span><br><span class="line"><span class="string">client $ lctl get_param ldlm.namespaces.*mdc-*.lru_size #default dynamic</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lru_size=0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#disable </span></span><br><span class="line"><span class="string">client$ lctl set_param ldlm.namespaces.*osc*.lru_size=5000</span></span><br><span class="line"><span class="string">#The total number of locks available is a function of the server RAM. The default limit is 50 locks/1 MB of RAM. If memory pressure is too high, the LRU size is shrunk. The number of locks on the server is limited tonum_osts_per_oss * num_clients * lru_size as followsa</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">To determine the number of locks being granted with dynamic LRU resizing, run:</span></span><br><span class="line"><span class="string">client$ lctl get_param ldlm.namespaces.*.pool.limit</span></span><br><span class="line"><span class="string">ldlm.namespaces.MGC192.168.0.1@tcp.pool.limit=0</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.pool.limit=195699</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-OST0000-osc-ffff8dab2cce8800.pool.limit=183850</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">OSS $ lctl get_param ldlm.namespaces.*.pool.limit</span></span><br><span class="line"><span class="string">ldlm.namespaces.MGC192.168.0.1@tcp.pool.limit=0</span></span><br><span class="line"><span class="string">ldlm.namespaces.filter-fsname-OST0000_UUID.pool.limit=183850</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.pool.limit=1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">MDS $ lctl get_param ldlm.namespaces.*.pool.limit</span></span><br><span class="line"><span class="string">ldlm.namespaces.MGC192.168.0.1@tcp.pool.limit=0</span></span><br><span class="line"><span class="string">ldlm.namespaces.MGS.pool.limit=1</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.pool.limit=1</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-OST0000-osc-MDT0000.pool.limit=1</span></span><br><span class="line"><span class="string">ldlm.namespaces.mdt-fsname-MDT0000_UUID.pool.limit=195699</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">The lru_max_age parameter is used to control the age of client-side locks in the LRU cached locks queue. This limits how long unused locks are cached on the client, and avoids idle clients from holding locks for an excessive time, which reduces memory usage on both the client and server, as well as reducing work during server recovery.</span></span><br><span class="line"><span class="string">Client$ lctl set_param ldlm.namespaces.*MDT*.lru_max_age=900s</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lru_max_age=900s</span></span><br><span class="line"><span class="string">Client$ lctl get_param ldlm.namespaces.*MDT*.lru_max_age</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lru_max_age=900000</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#modify client not change mds and oss</span></span><br><span class="line"><span class="string">MDS $ lctl get_param ldlm.namespaces.*MDT*.lru_max_age</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.lru_max_age=3900000</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-OST0000-osc-MDT0000.lru_max_age=3900000</span></span><br><span class="line"><span class="string">ldlm.namespaces.mdt-fsname-MDT0000_UUID.lru_max_age=3900000</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">OSS $ lctl get_param ldlm.namespaces.*MDT*.lru_max_age</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.lru_max_age=3900000 </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">MDS $ lctl get_param ldlm.namespaces.*.pool.lock_volume_factor</span></span><br><span class="line"><span class="string">ldlm.namespaces.MGC192.168.0.238@tcp.pool.lock_volume_factor=1</span></span><br><span class="line"><span class="string">ldlm.namespaces.MGS.pool.lock_volume_factor=1</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.pool.lock_volume_factor=1</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-OST0000-osc-MDT0000.pool.lock_volume_factor=1</span></span><br><span class="line"><span class="string">ldlm.namespaces.mdt-fsname-MDT0000_UUID.pool.lock_volume_factor=1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">OSS $ lctl get_param ldlm.namespaces.*.pool.lock_volume_factor</span></span><br><span class="line"><span class="string">ldlm.namespaces.MGC192.168.0.238@tcp.pool.lock_volume_factor=1</span></span><br><span class="line"><span class="string">ldlm.namespaces.filter-fsname-OST0000_UUID.pool.lock_volume_factor=1</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.pool.lock_volume_factor=1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Client $  lctl get_param ldlm.namespaces.*.pool.lock_volume_factor</span></span><br><span class="line"><span class="string">ldlm.namespaces.MGC192.168.0.238@tcp.pool.lock_volume_factor=1</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.pool.lock_volume_factor=1</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-OST0000-osc-ffff8dab2cce8800.pool.lock_volume_factor=1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Only client$ lctl get_param ldlm.namespaces.*-MDT0000-mdc-*.lock_unused_count; </span></span><br><span class="line"><span class="string">Only client$ lctl get_param ldlm.namespaces.*-MDT0000-mdc-*.pool.recalc_period</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lock_unused_count=0</span></span><br><span class="line"><span class="string">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.pool.recalc_period=10</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># check object server status</span></span><br><span class="line"><span class="string">client $ lfs osts</span></span><br><span class="line"><span class="string">client $ cat /proc/fs/lfs/lov/$fsname-clilov-fffff882037467800/target_obd</span></span><br><span class="line"><span class="string">mds    $ cat /proc/fs/lfs/lov/$fsname-MDT0000-mdtlov/target_obd</span></span><br><span class="line"><span class="string">client $ lctl get_param osc.*-OST*.active</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># check object server status in my production env</span></span><br><span class="line"><span class="string">$ (lfs osts | awk -F &#x27;</span>[ :_]+<span class="string">&#x27; &#x27;</span><span class="variable">$0</span>~/OST/&#123;<span class="built_in">print</span> <span class="variable">$2</span>&#125;<span class="string">&#x27; | while read line; do grep &quot;FULL&quot; /proc/fs/lfs/osc/$&#123;line&#125;-*/state &gt;/dev/null 2&gt;&amp;1 || echo -e &quot;Got these bad OSTs:&quot; $&#123;RED&#125;$line$&#123;NC&#125; ; done) &amp;&amp; exit 0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># show ost ip  addr</span></span><br><span class="line"><span class="string">$ lctl dl -t</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># list nids</span></span><br><span class="line"><span class="string">$ lctl lst_nids</span></span><br><span class="line"><span class="string">$ lctl which_nid $your_ipaddr@tcp</span></span><br><span class="line"><span class="string">$ lctl ping $your_ipaddr@tcp </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># mount namespace &quot;D&quot; and clear mds info from client</span></span><br><span class="line"><span class="string">client $ echo 0 &gt; cat /sys/fs/lfs/mdc/$FNAME-*/active</span></span><br><span class="line"><span class="string">client $ lctl set_param mdc.$FNAME-*.active=0 #better than degarde</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># lfs rpc</span></span><br><span class="line"><span class="string">## 2.12.6 default</span></span><br><span class="line"><span class="string">$ lctl get_param -n osc.*OST0000-osc-[^mM]*.max_rpcs_in_flight</span></span><br><span class="line"><span class="string">64</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ lctl get_param mdc.*.max_rpcs_in_flight</span></span><br><span class="line"><span class="string">mdc.test0-MDT0000-mdc-ffff95067b45a000.max_rpcs_in_flight=8</span></span><br><span class="line"><span class="string">$ lctl set_param mdc.*.max_rpcs_in_flight=16</span></span><br><span class="line"><span class="string">mdc.test0-MDT0000-mdc-ffff95067b45a000.max_rpcs_in_flight=16</span></span><br><span class="line"><span class="string">$ cat /sys/module/mdt/parameters/max_mod_rpcs_per_client</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#In lfs 2.13 client; </span></span><br><span class="line"><span class="string">#direct IO could reach high 120k, but the buffered IO only 4k. there are read </span></span><br><span class="line"><span class="string">#When I disable</span></span><br><span class="line"><span class="string">$ lctl set_param llite.*.read_ahead_async_file_threshold_mb=0</span></span><br><span class="line"><span class="string">echo 0 &gt; /sys/fs/lfs/llite/lfs-ffff9455c0a4b800/read_ahead_async_file_threshold_mb</span></span><br><span class="line"><span class="string">#I could got the high IOPS too.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">## 2.12.X readahead setting</span></span><br><span class="line"><span class="string">$ ls -1 /sys/kernel/debug/lfs/llite/lfs-ffff92fab9235800</span></span><br><span class="line"><span class="string">dump_page_cache</span></span><br><span class="line"><span class="string">extents_stats</span></span><br><span class="line"><span class="string">extents_stats_per_process</span></span><br><span class="line"><span class="string">max_cached_mb</span></span><br><span class="line"><span class="string">max_read_ahead_mb</span></span><br><span class="line"><span class="string">max_read_ahead_per_file_mb</span></span><br><span class="line"><span class="string">max_read_ahead_whole_mb</span></span><br><span class="line"><span class="string">nosquash_nids</span></span><br><span class="line"><span class="string">offset_stats</span></span><br><span class="line"><span class="string">read_ahead_stats</span></span><br><span class="line"><span class="string">root_squash</span></span><br><span class="line"><span class="string">sbi_flags</span></span><br><span class="line"><span class="string">site</span></span><br><span class="line"><span class="string">statahead_stats</span></span><br><span class="line"><span class="string">stats</span></span><br><span class="line"><span class="string">unstable_stats</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#readahead</span></span><br><span class="line"><span class="string">$ lctl set_param llite.*.read_ahead_stats=c llite.testfs-ffff8daf56159000.read_ahead_stats=c</span></span><br><span class="line"><span class="string">$ lctl get_param llite.*.*read*</span></span><br><span class="line"><span class="string">$ lctl get_param llite.$&#123;fsname&#125;*.*read*</span></span><br><span class="line"><span class="string">llite.testfs-ffff8dcc59a15800.fast_read=1</span></span><br><span class="line"><span class="string">llite.testfs-ffff8dcc59a15800.max_read_ahead_async_active=12</span></span><br><span class="line"><span class="string">llite.testfs-ffff8dcc59a15800.read_ahead_async_file_threshold_mb=64</span></span><br><span class="line"><span class="string">llite.testfs-ffff8dcc59a15800.read_ahead_range_kb=1024</span></span><br><span class="line"><span class="string">llite.testfs-ffff8dcc59a15800.max_read_ahead_mb=64</span></span><br><span class="line"><span class="string">llite.testfs-ffff8dcc59a15800.max_read_ahead_per_file_mb=64</span></span><br><span class="line"><span class="string">llite.testfs-ffff8dcc59a15800.max_read_ahead_whole_mb=64</span></span><br><span class="line"><span class="string">llite.testfs-ffff8dcc59a15800.read_ahead_stats=</span></span><br><span class="line"><span class="string">snapshot_time             1642131829.087651284 secs.nsecs</span></span><br><span class="line"><span class="string">hits                      9999533 samples [pages]</span></span><br><span class="line"><span class="string">misses                    838588 samples [pages]  ---&gt; too high, readahead invalid</span></span><br><span class="line"><span class="string">readpage not consecutive  1293010 samples [pages] ---&gt; too high, readahead invalid</span></span><br><span class="line"><span class="string">zero size window          2837081 samples [pages] ---&gt; too high, readahead invalid</span></span><br><span class="line"><span class="string">failed to fast read       592502 samples [pages]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#Many system commands, such as ls –l, du, and find, traverse a directory sequentially. To make these commands run efficiently, the directory statahead can be enabled to improve the performance of directory traversal</span></span><br><span class="line"><span class="string"># Controls the maximum number of file attributes (metadata) that will be prefetched by the statahead thread. By default, statahead is enabled and statahead_max is 32 files, The maximum statahead_max is 8192 files</span></span><br><span class="line"><span class="string">$ lctl get_param llite.*.statahead_max</span></span><br><span class="line"><span class="string">llite.fsname-ffff8dab2cce8800.statahead_max=32</span></span><br><span class="line"><span class="string">$ lctl set_param llite.*.statahead_max=128</span></span><br><span class="line"><span class="string">$ lctl set_param -P llite.*.statahead_max=128</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">The directory statahead thread will also prefetch the file size/block attributes from the OSTs, so that all file attributes are available on the client when requested by an application. This is controlled by the asynchronous glimpse lock (AGL) setting. The AGL behaviour can be disabled by setting:</span></span><br><span class="line"><span class="string">$ lctl set_param llite.*.statahead_agl=0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#default enable</span></span><br><span class="line"><span class="string">$ lctl set_param llite.*.statahead_agl=1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ lctl get_param -n llite.*.statahead_stats</span></span><br><span class="line"><span class="string">statahead total: 4</span></span><br><span class="line"><span class="string">statahead wrong: 0 ## Does it increase, monitor</span></span><br><span class="line"><span class="string">agl total: 4</span></span><br><span class="line"><span class="string">#A read-only interface that provides current statahead and AGL statistics, such as how many times statahead/AGL has been triggered since the last mount, how many statahead/AGL failures have occurred due to an incorrect prediction or other causes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># client max read ahead size</span></span><br><span class="line"><span class="string">cat /sys/kernel/debug/lfs/llite/lfs-ffff91b225941800/max_read_ahead_mb</span></span><br><span class="line"><span class="string">64</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ cat /proc/fs/lfs/osc/fsname-*/state</span></span><br><span class="line"><span class="string">$ grep -Ri current_state  /proc/fs/lfs/osc/fsname-*/state</span></span><br><span class="line"><span class="string">current_state: FULL</span></span><br><span class="line"><span class="string">state_history:</span></span><br><span class="line"><span class="string"> - [ 1616015179, CONNECTING ]</span></span><br><span class="line"><span class="string"> - [ 1616015179, IDLE ]</span></span><br><span class="line"><span class="string"> - [ 1616018598, CONNECTING ]</span></span><br><span class="line"><span class="string"> - [ 1616018598, FULL ]</span></span><br><span class="line"><span class="string"> - [ 1616019092, CONNECTING ]</span></span><br><span class="line"><span class="string"> - [ 1616019092, IDLE ]</span></span><br><span class="line"><span class="string"> - [ 1616019285, CONNECTING ]</span></span><br><span class="line"><span class="string"> - [ 1616019285, FULL ]</span></span><br><span class="line"><span class="string"> - [ 1616021199, CONNECTING ]</span></span><br><span class="line"><span class="string"> - [ 1616021199, IDLE ]</span></span><br><span class="line"><span class="string"> - [ 1616023761, CONNECTING ]</span></span><br><span class="line"><span class="string"> - [ 1616023761, FULL ]</span></span><br><span class="line"><span class="string"> - [ 1616023908, CONNECTING ]</span></span><br><span class="line"><span class="string"> - [ 1616023908, IDLE ]</span></span><br><span class="line"><span class="string"> - [ 1616147184, CONNECTING ]</span></span><br><span class="line"><span class="string"> - [ 1616147184, FULL ]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ grep generation /proc/fs/lfs/osc/*-OST0000*/import</span></span><br><span class="line"><span class="string">       generation: 11</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">or </span></span><br><span class="line"><span class="string">client $ lctl get_param osc.fsname-OST0000*.import</span></span><br><span class="line"><span class="string">client $ lctl get_param osc.fsname-OST0000*.state</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ lctl set_param osc.*.idle_timeout=20</span></span><br></pre></td></tr></table></figure>

<h3 id="mds"><a href="#mds" class="headerlink" title="mds"></a>mds</h3><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line">$ cat /sys/module/mdt/parameters/max_mod_rpcs_per_client</span><br><span class="line"><span class="number">8</span></span><br><span class="line">$ echo <span class="number">16</span> &gt; /sys/module/mdt/parameters/max_mod_rpcs_per_client</span><br><span class="line"></span><br><span class="line"><span class="comment"># oss, lfs is extended to support RPCs up to 16MB in size</span></span><br><span class="line"><span class="comment"># Lfs is extended to support RPCs up to 16MB in size. By enabling a larger RPC size, fewer RPCs will be required to transfer the same amount of data between clients and servers. With a larger RPC size, the OSS can submit more data to the underlying disks at once, therefore it can produce larger disk I/Os to fully utilize the increasing bandwidth of disks.</span></span><br><span class="line"><span class="comment">#At client connection time, clients will negotiate with servers what the maximum RPC size it is possible to use, but the client can always send RPCs smaller than this maximum.</span></span><br><span class="line"><span class="comment">#The parameter brw_size is used on the OST to tell the client the maximum (preferred) IO size. All clients that talk to this target should never send an RPC greater than this size. Clients can individually set a smaller RPC size limit via the osc.*.max_pages_per_rpc tunable.</span></span><br><span class="line"><span class="comment">#The smallest brw_size that can be set for ZFS OSTs is the recordsize of that dataset. This ensures that the client can always write a full ZFS file block if it has enough dirty data, and does not otherwise force it to do read- modify-write operations for every RPC. </span></span><br><span class="line"></span><br><span class="line">$ lctl get_param obdfilter.test0-OST*.brw_size</span><br><span class="line">obdfilter.test0-OST0000.brw_size=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">$ lctl set_param obdfilter.*.brw_size=<span class="number">16</span> #<span class="number">16</span>M</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get all client info from mds</span></span><br><span class="line">$ cat /<span class="keyword">proc</span>/fs/lfs/nodemap/default/exports</span><br><span class="line"></span><br><span class="line">##client</span><br><span class="line">$<span class="title"> lctl</span> get_param<span class="title"> osc.*.max_pages_per_rpc</span> osc.*.max_rpcs_in_flight<span class="title"> osc.*.max_dirty_mb</span> llite.*.max_read_ahead_mb</span><br><span class="line">#<span class="title"> In</span> order<span class="title"> to</span> enable<span class="title"> a</span> larger<span class="title"> RPC</span> size,<span class="title"> brw_size</span> must<span class="title"> be</span> changed<span class="title"> to</span> an<span class="title"> IO</span> size<span class="title"> value</span> up<span class="title"> to</span> 16MB.<span class="title"> To</span> temporarily<span class="title"> change</span> brw_size,<span class="title"> the</span> following<span class="title"> command</span> should<span class="title"> be</span> run<span class="title"> on</span> the<span class="title"> OSS:</span></span><br><span class="line"><span class="title">oss#</span> lctl<span class="title"> set_param</span> obdfilter.fsname-OST*.brw_size=16</span><br><span class="line"></span><br><span class="line">To<span class="title"> persistently</span> change<span class="title"> brw_size,</span> the<span class="title"> following</span> command<span class="title"> should</span> be<span class="title"> run:</span></span><br><span class="line"><span class="title">oss#</span> lctl<span class="title"> set_param</span> -P<span class="title"> obdfilter.fsname-OST*.brw_size=16</span></span><br><span class="line"><span class="title">When</span> a<span class="title"> client</span> connects<span class="title"> to</span> an<span class="title"> OST</span> target,<span class="title"> it</span> will<span class="title"> fetch</span> brw_size<span class="title"> from</span> the<span class="title"> target</span> and<span class="title"> pick</span> the<span class="title"> maximum</span> value<span class="title"> of</span> brw_size<span class="title"> and</span> its<span class="title"> local</span> setting<span class="title"> for</span> max_pages_per_rpc<span class="title"> as</span> the<span class="title"> actual</span> RPC<span class="title"> size.</span> Therefore,<span class="title"> the</span> max_pages_per_rpc<span class="title"> on</span> the<span class="title"> client</span> side<span class="title"> would</span> have<span class="title"> to</span> be<span class="title"> set</span> to 16M,<span class="title"> or</span> 4096<span class="title"> if</span> the<span class="title"> PAGESIZE</span> is 4KB,<span class="title"> to</span> enable<span class="title"> a</span> 16MB<span class="title"> RPC.</span> To<span class="title"> temporarily</span> make<span class="title"> the</span> change,<span class="title"> the</span> following<span class="title"> command</span> should<span class="title"> be</span> run<span class="title"> on</span> the<span class="title"> client</span> to<span class="title"> setmax_pages_per_rpc:</span></span><br><span class="line"><span class="title">client$</span> lctl<span class="title"> set_param</span> osc.fsname-OST*.max_pages_per_rpc=16M</span><br><span class="line">client$<span class="title"> lctl</span> set_param -P<span class="title"> obdfilter.fsname-OST*.osc.max_pages_per_rpc=16M</span></span><br><span class="line"><span class="title"></span></span><br><span class="line"><span class="title"></span></span><br><span class="line"><span class="title">client$</span> lctl<span class="title"> set_param</span> osc.*.max_pages_per_rpc=256<span class="title"> osc.*.max_rpcs_in_flight=64</span> osc.*.max_dirty_mb=256<span class="title"> osc.*.grant_shrink=0</span></span><br><span class="line"><span class="title">client$</span> lctl<span class="title"> get_param</span> osc.*.max_pages_per_rpc</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.max_pages_per_rpc=256</span><br><span class="line"></span><br><span class="line">#<span class="title"> cancel_lru_locks</span></span><br><span class="line"><span class="title">$</span> lctl<span class="title"> set_param</span> -n<span class="title"> ldlm.namespaces.*osc*.lru_size=clear</span></span><br><span class="line"><span class="title">$</span> lctl<span class="title"> set_param</span> -n<span class="title"> ldlm.namespaces.*mdc*.lru_size=clear</span></span><br><span class="line"><span class="title">$</span> lctl<span class="title"> set_param</span> -n<span class="title"> osc.*.rpc_stats=0</span></span><br><span class="line"><span class="title">$</span> dd<span class="title"> if=/dev/zero</span> of=/lfs/test<span class="title"> bs=1M</span> count=10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###<span class="title"> example</span> for<span class="title"> single</span> OST</span><br><span class="line">$<span class="title"> lctl</span> get_param -n &#x27;osc.*.rpc_stats&#x27; |<span class="title"> sed</span> -n &#x27;/pages<span class="title"> per</span> rpc/,/^$/p&#x27;</span><br><span class="line">pages<span class="title"> per</span> rpc<span class="title">         rpcs</span>   %<span class="title"> cum</span> % |<span class="title">       rpcs</span>   %<span class="title"> cum</span> %</span><br><span class="line">1:		         0   0   0   |          0   0   0</span><br><span class="line">2:		         0   0   0   |          0   0   0</span><br><span class="line">4:		         0   0   0   |          0   0   0</span><br><span class="line">8:		         0   0   0   |          0   0   0</span><br><span class="line">16:		         0   0   0   |          0   0   0</span><br><span class="line">32:		         0   0   0   |          0   0   0</span><br><span class="line">64:		         0   0   0   |          0   0   0</span><br><span class="line">128:		         0   0   0   |          0   0   0</span><br><span class="line">256:		         0   0   0   |         10 100 100  ##<span class="title"> why</span> 256 ?<span class="title"> in</span> my<span class="title"> test</span> env,<span class="title"> the</span> brw_size=1 * 1048576 / 4096<span class="title"> PAGE_SIZE</span> = 256</span><br><span class="line">                         |                       |</span><br><span class="line">                         |                      ----------<span class="title"> means</span> dd<span class="title"> bs=1M</span> count=10 , 10<span class="title"> x</span> read<span class="title"> rpcs</span></span><br><span class="line"><span class="title"></span>                         ---------------------------------write<span class="title"> rpcs</span></span><br><span class="line"><span class="title">###</span> if<span class="title"> you</span> set<span class="title"> brw_size</span> in<span class="title"> OSS,</span> and<span class="title"> make</span> sure<span class="title"> it</span> by<span class="title"> the</span> client </span><br><span class="line"></span><br><span class="line">#<span class="title"> The</span> parameter<span class="title"> brw_size</span> is<span class="title"> used</span> on<span class="title"> the</span> OST<span class="title"> to</span> tell<span class="title"> the</span> client<span class="title"> the</span> maximum (preferred)<span class="title"> IO</span> size.<span class="title"> All</span> clients<span class="title"> that</span> talk<span class="title"> to</span> this<span class="title"> target</span> should<span class="title"> never</span> send<span class="title"> an</span> RPC<span class="title"> greater</span> than<span class="title"> this</span> size.<span class="title"> Clients</span> can<span class="title"> individually</span> set<span class="title"> a</span> smaller<span class="title"> RPC</span> size<span class="title"> limit</span> via<span class="title"> the</span> osc.*.max_pages_per_rpc<span class="title"> tunable.</span></span><br><span class="line"><span class="title">#</span> The<span class="title"> smallest</span> brw_size<span class="title"> that</span> can<span class="title"> be</span> set<span class="title"> for</span> ZFS<span class="title"> OSTs</span> is<span class="title"> the</span> recordsize<span class="title"> of</span> that<span class="title"> dataset.</span> This<span class="title"> ensures</span> that<span class="title"> the</span> client<span class="title"> can</span> always<span class="title"> write</span> a<span class="title"> full</span> ZFS<span class="title"> file</span> block<span class="title"> if</span> it<span class="title"> has</span> enough<span class="title"> dirty</span> data,<span class="title"> and</span> does<span class="title"> not</span> otherwise<span class="title"> force</span> it<span class="title"> to</span> do<span class="title"> read-</span> modify-write<span class="title"> operations</span> for<span class="title"> every</span> RPC. </span><br><span class="line">$<span class="title"> lctl</span> set_param<span class="title"> obdfilter.fsname-OST*.brw_size=16</span></span><br><span class="line"><span class="title"></span></span><br><span class="line"><span class="title">#</span> No<span class="title"> root</span> squash</span><br><span class="line">mds $<span class="title"> lctl</span> set_param $fsname.mdt.root_squash=108:108</span><br><span class="line">or</span><br><span class="line">mds $<span class="title"> lctl</span> set_param<span class="title"> mdt.$</span>&#123;fsname&#125;-MDT0000.root_squash=<span class="number">108</span>:<span class="number">108</span></span><br><span class="line">mds $ lctl set_param mdt.$&#123;fsname&#125;-MDT0000.nosquash_nids=<span class="string">&quot;ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">error</span>: set_param: param_path &#x27;$fsname/mdt/root_squash&#x27;: No such <span class="keyword">file</span> or directory</span><br><span class="line">mds $ lctl conf_param $fsname.mdt.root_squash=<span class="number">108</span>:<span class="number">108</span></span><br><span class="line">mds $ lctl conf_param $fsname.mdt.nosquash_nids=<span class="string">&quot;ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp&quot;</span></span><br><span class="line">mds $ cat /<span class="keyword">proc</span>/fs/lf/mdt/$FNAME-MDT0000/nosquash_nids</span><br><span class="line">ip.ip.ip.ip@tcp<span class="title"> ip1.ip1.ip1.ip@tcp</span></span><br><span class="line"><span class="title"></span></span><br><span class="line"><span class="title">client</span> $<span class="title"> lctl</span> set_param<span class="title"> llite.$FNAME-*.nosquash_nids=&quot;ip.ip.ip.ip@tcp</span> ip1.ip1.ip1.ip@tcp&quot;</span><br><span class="line"></span><br><span class="line">#<span class="title"> MDS</span> to<span class="title"> totally</span> avoid<span class="title"> new</span> object<span class="title"> creation</span> on<span class="title"> that</span> OST</span><br><span class="line">$<span class="title"> lctl</span> set_param<span class="title"> osp.$FNAME-OST00XX-osc-MDT*.max_create_count=0</span></span><br><span class="line"><span class="title">$</span> lctl<span class="title"> set_param</span> osp.$FNAME-OST00XX*.max_create_count=0</span><br><span class="line"></span><br><span class="line">#degrade<span class="title"> will</span> only<span class="title"> prefer</span> to<span class="title"> skip</span> the<span class="title"> OST</span></span><br><span class="line"><span class="title">OSS</span> $<span class="title"> lctl</span> set_param<span class="title"> obdfilter.$FNAME-OST0000.degraded=1</span></span><br><span class="line"><span class="title"></span></span><br><span class="line"><span class="title">#disable</span> pre-create</span><br><span class="line">OSS $<span class="title"> lctl</span> set_param<span class="title"> obdfilter.$FNAME-OST0000*.no_precreate=1</span></span><br><span class="line"><span class="title"></span></span><br><span class="line"><span class="title">$</span> lctl<span class="title"> set_param</span> -P<span class="title"> timeout=300</span></span><br><span class="line"><span class="title">$</span> lctl<span class="title"> set_param</span> timeout=300 </span><br><span class="line">#<span class="title"> if</span> you<span class="title"> want</span> it<span class="title"> work</span> ,you<span class="title"> have</span> set<span class="title"> it</span> twice...</span><br><span class="line"></span><br><span class="line">#<span class="title"> Monitor</span> status</span><br><span class="line">$<span class="title"> cat</span> /<span class="keyword">proc</span>/fs/lfs/mdc/$&#123;fname&#125;-MDT0000-mdc-ffff88091eff0800/state </span><br><span class="line">$ lctl get_param mdc.$fname-MDT*.state</span><br><span class="line"></span><br><span class="line">$ watch -d lctl get_param mdt.*.md_stats</span><br><span class="line">snapshot_time             <span class="number">1556726087.189561170</span> secs.nsecs</span><br><span class="line"><span class="keyword">open</span>                      <span class="number">3412130101</span> samples [reqs]</span><br><span class="line"><span class="keyword">close</span>                     <span class="number">2926922120</span> samples [reqs]</span><br><span class="line">mknod                     <span class="number">293730475</span> samples [reqs]</span><br><span class="line">link                      <span class="number">20713305</span> samples [reqs]</span><br><span class="line">unlink                    <span class="number">316042257</span> samples [reqs]</span><br><span class="line">mkdir                     <span class="number">3275032</span> samples [reqs]</span><br><span class="line">rmdir                     <span class="number">2731821</span> samples [reqs]</span><br><span class="line"><span class="keyword">rename</span>                    <span class="number">7687699</span> samples [reqs]</span><br><span class="line">getattr                   <span class="number">2060900881</span> samples [reqs]</span><br><span class="line">setattr                   <span class="number">320658776</span> samples [reqs]</span><br><span class="line">getxattr                  <span class="number">1080139037</span> samples [reqs]</span><br><span class="line">setxattr                  <span class="number">222105</span> samples [reqs]</span><br><span class="line">statfs                    <span class="number">11587278</span> samples [reqs]</span><br><span class="line">sync                      <span class="number">20670980</span> samples [reqs]</span><br><span class="line">samedir_rename            <span class="number">7199107</span> samples [reqs]</span><br><span class="line">crossdir_rename           <span class="number">488592</span> samples [reqs]</span><br></pre></td></tr></table></figure>

<h3 id="OSS-1"><a href="#OSS-1" class="headerlink" title="OSS"></a>OSS</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">obdfilter.lfsfs-OST*.brw_size=16</span></span><br><span class="line"></span><br><span class="line"><span class="string">osc.lfsfs-OST*.max_pages_per_rpc=4096</span></span><br><span class="line"><span class="string">lite.lfs*.max_read_aqhead_mb=1024</span></span><br><span class="line"><span class="string">osc.lfsfs-OST*.max_rpcs_in_flight=16</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># osd_sync_destroy_max_size &quot;Maximum object size to use synchronous destroy</span></span><br><span class="line"><span class="string">options</span> <span class="string">osd_zfs</span> <span class="string">osd_sync_destroy_max_size=1048576</span></span><br><span class="line"></span><br><span class="line"><span class="string">options</span> <span class="string">ost</span> <span class="string">oss_num_threads=0</span></span><br><span class="line"><span class="comment"># you can limit the server performance by num_threads </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#default ost io threads</span></span><br><span class="line"><span class="string">$</span> <span class="string">lctl</span> <span class="string">get_param</span> <span class="string">ost.OSS.ost_io.threads_max</span></span><br><span class="line"><span class="string">ost.OSS.ost_io.threads_max=128</span></span><br><span class="line"><span class="string">$</span> <span class="string">lctl</span> <span class="string">get_param</span> <span class="string">ost.OSS.ost_io.threads_started</span></span><br><span class="line"><span class="string">ost.OSS.ost_io.threads_started=21</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># rpc info</span></span><br><span class="line"><span class="string">$</span> <span class="string">cat</span> <span class="string">/proc/fs/lfs/osc/lfs-OST0000-osc-ffff88103a993000/import</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get status</span></span><br><span class="line"><span class="string">$</span> <span class="string">lctl</span> <span class="string">get_param</span> <span class="string">osc.*OST0000*.&#123;state,timeouts&#125;</span></span><br><span class="line"><span class="string">$</span> <span class="string">lctl</span> <span class="string">get_param</span> <span class="string">at_*</span> <span class="string">timeout</span></span><br><span class="line"><span class="string">$</span> <span class="string">lctl</span> <span class="string">get_param</span> <span class="string">llite.*$FNAME*.stats</span></span><br><span class="line"><span class="comment">### clear stats, lctl set_param llite.*.stats=c</span></span><br><span class="line"><span class="string">$</span> <span class="string">lctl</span> <span class="string">get_param</span> <span class="string">obdfilter.*OST005e*.brw_stats</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Read and print the last_rcvd file from a device</span></span><br><span class="line"><span class="comment">#display client information</span></span><br><span class="line"><span class="string">$</span> <span class="string">lr_reader</span> <span class="string">-c</span> <span class="string">/dev/sdh</span></span><br><span class="line"><span class="attr">last_rcvd:</span></span><br><span class="line"><span class="attr">uuid:</span> <span class="string">fsms-MDT0000_UUID</span></span><br><span class="line"> <span class="attr">feature_compat:</span> <span class="number">0x8</span></span><br><span class="line"> <span class="attr">feature_incompat:</span> <span class="number">0x61c</span></span><br><span class="line"> <span class="attr">feature_rocompat:</span> <span class="number">0x1</span></span><br><span class="line"> <span class="attr">last_transaction:</span> <span class="number">4294967298</span></span><br><span class="line"> <span class="attr">target_index:</span> <span class="number">0</span></span><br><span class="line"> <span class="attr">mount_count:</span> <span class="number">1</span></span><br><span class="line"> <span class="attr">client_area_start:</span> <span class="number">8192</span></span><br><span class="line"> <span class="attr">client_area_size:</span> <span class="number">128</span></span><br><span class="line"> <span class="attr">79136f3b-7d85-e265-37aa-dbb40ec5a30c:</span></span><br><span class="line"> <span class="attr">generation:</span> <span class="number">2</span></span><br><span class="line"> <span class="attr">last_transaction:</span> <span class="number">0</span></span><br><span class="line"> <span class="attr">last_xid:</span> <span class="number">0</span></span><br><span class="line"> <span class="attr">last_result:</span> <span class="number">0</span></span><br><span class="line"> <span class="attr">last_data:</span> <span class="number">0</span></span><br><span class="line"><span class="comment">#display reply data information</span></span><br><span class="line"><span class="string">$</span> <span class="string">lr_reader</span> <span class="string">-r</span> <span class="string">/dev/sdh</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">reply_data:</span></span><br><span class="line"> <span class="attr">0:</span></span><br><span class="line"> <span class="attr">client_generation:</span> <span class="number">2</span></span><br><span class="line"> <span class="attr">last_transaction:</span> <span class="number">4426736549</span></span><br><span class="line"> <span class="attr">last_xid:</span> <span class="number">1511845291497772</span></span><br><span class="line"> <span class="attr">last_result:</span> <span class="number">0</span></span><br><span class="line"> <span class="attr">last_data:</span> <span class="number">0</span></span><br><span class="line"> <span class="attr">1:</span></span><br><span class="line"> <span class="attr">client_generation:</span> <span class="number">2</span></span><br><span class="line"> <span class="attr">last_transaction:</span> <span class="number">4426736566</span></span><br><span class="line"> <span class="attr">last_xid:</span> <span class="number">1511845291498048</span></span><br><span class="line"> <span class="attr">last_result:</span> <span class="number">0</span></span><br><span class="line"> <span class="attr">last_data:</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># disable ost cache</span></span><br><span class="line"><span class="string">$</span> <span class="string">lctl</span> <span class="string">get_param</span> <span class="string">osd-ldiskfs.*.read_cache_enable</span></span><br><span class="line"><span class="string">osd-ldiskfs.MGS.read_cache_enable=0</span></span><br><span class="line"><span class="string">osd-ldiskfs.$&#123;fsname&#125;-MDT0000.read_cache_enable=0</span></span><br><span class="line"></span><br><span class="line"><span class="string">$</span> <span class="string">lctl</span> <span class="string">get_param</span> <span class="string">ldlm.namespaces.*.lru_size</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#make sure ost mount parameters and flag</span></span><br><span class="line"><span class="string">$</span> <span class="string">cat</span> <span class="string">/proc/fs/ldiskfs/dm-xx/options</span></span><br><span class="line"><span class="string">rw</span></span><br><span class="line"><span class="string">barrier</span></span><br><span class="line"><span class="string">no_mbcache</span></span><br><span class="line"><span class="string">user_xattr</span></span><br><span class="line"><span class="string">acl</span></span><br><span class="line"><span class="string">resuid=0</span></span><br><span class="line"><span class="string">resgid=0</span></span><br><span class="line"><span class="string">errors=remount-ro</span></span><br><span class="line"><span class="string">commit=5</span></span><br><span class="line"><span class="string">min_batch_time=0</span></span><br><span class="line"><span class="string">max_batch_time=15000</span></span><br><span class="line"><span class="string">stripe=0</span></span><br><span class="line"><span class="string">data=ordered</span></span><br><span class="line"><span class="string">inode_readahead_blks=32</span></span><br><span class="line"><span class="string">init_itable=10</span></span><br><span class="line"><span class="string">max_dir_size_kb=0</span></span><br></pre></td></tr></table></figure>

<h4 id="TCP-Ethernet-network"><a href="#TCP-Ethernet-network" class="headerlink" title="TCP Ethernet network"></a>TCP Ethernet network</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">options lnet networks=<span class="string">&quot;tcp0(myri10ge),tcp1(eth0)&quot;</span></span><br><span class="line">lctl</span><br><span class="line">lctl &gt; network tcp1</span><br><span class="line">lctl &gt; peer_list</span><br><span class="line">12345-xx.xx.xx.xx@tcp [0]0.0.0.0-&gt;0.0.0.0:0 <span class="comment">#0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ldlm_enqueue_min = max(2*net_latency, net_latency + quiescent_time) +\\ 2*service_time</span></span><br><span class="line"><span class="comment"># ldlm_enqueue_min = max(2*50, 50 + 140) + 2*50 = 50+140 + 100 = 290</span></span><br><span class="line"><span class="comment"># Minimum lock enqueue time (in seconds). The default is 100. The time it takes to enqueue a lock, ldlm_enqueue, is the maximum of the measured enqueue estimate (influenced by at_min and at_max parameters), multiplied by a weighting factor and the value of ldlm_enqueue_min.lfs Distributed Lock Manager (LDLM) lock enqueues have a dedicated minimum value for ldlm_enqueue_min. Lock enqueue timeouts increase as the measured enqueue times increase (similar to adaptive timeouts).</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#at_max The largest potential RPC timeout that a client can set is 2*at_max. By lowering at_max from 600 to 400 seconds we reduce the worst case I/O delay from 1200 seconds, or 20 minutes, to 800 seconds or just over 13 minutes.</span></span><br><span class="line"><span class="comment">#at_min The 40 second value factors into our calculation for an appropriate LDLM timeout as discussed in section LDLM Timeouts. Our recommendation for Lfs servers is also 40 seconds</span></span><br><span class="line"><span class="comment">#Adaptive Timeouts: In a Lfs file system servers keep track of the time it takes for RPCs to be completed</span></span><br><span class="line"><span class="comment">#The quiescent_time in this formula is to account for the time it takes all Lfs clients to reestablish connections with all Lfs targets following an HSN quiesce. We&#x27;ve experimentally determined an average time to be approximately 140 seconds, but it is possible that this value may vary based on different factors such as the number of Lfs clients, the number of Lfs targets, the number of Lfs file systems mounted on each client, etc. Thus, given an at_min of 40 seconds</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">options ptlrpc ldlm_enqueue_min=250</span><br><span class="line"></span><br><span class="line"><span class="comment"># readonly mount</span></span><br><span class="line">$ mount.lfs <span class="variable">$zpool</span> /ost0 -o rdonly_dev</span><br><span class="line"></span><br><span class="line"><span class="comment"># mount a network</span></span><br><span class="line">$ mount.lfs -o device=192.168.1.1@tcp1:/lfs1 /mnt </span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="http://wiki.lfs.org/Lfs_Resiliency:_Understanding_Lfs_Message_Loss_and_Tuning_for_Resiliency#Tuning_Lfs_for_Resiliency">Understanding Lfs Message Loss and Tuning for Resiliency</a></p>
<p>I think in some the bad Ethernet tcp quality env, you must improve them</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /sys/module/lnet/parameters</span><br><span class="line">$ chmod 755 ./*</span><br><span class="line">$ <span class="built_in">echo</span> 1024 &gt; accept_backlog</span><br><span class="line">$ <span class="built_in">echo</span> 5 &gt; lnet_retry_count</span><br><span class="line">$ <span class="built_in">echo</span> 1 &gt; use_tcp_bonding <span class="comment"># has been </span></span><br><span class="line">$ cat accept_backlog lnet_retry_count use_tcp_bonding</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /sys/module/ksocklnd/parameters</span><br><span class="line">chmod 755 ./*</span><br><span class="line"><span class="built_in">echo</span> 128 &gt; peer_credits</span><br><span class="line"><span class="built_in">echo</span> 1024 &gt; credits</span><br><span class="line"><span class="built_in">echo</span> 80 &gt; sock_timeout</span><br><span class="line">cat credits peer_credits sock_timeout</span><br><span class="line"></span><br><span class="line"><span class="comment">#What is the mean about Peer ?</span></span><br><span class="line"><span class="comment">#Dynamic Peer Discovery</span></span><br><span class="line"><span class="comment">#Dynamic Peer Discovery (&quot;Discovery&quot; for short) is the process by which a node can discover the network interfaces it can reach a peer on without being pre-configured. This involves sending a ping to the peer. The ping response carries a flag bit to indicate that the peer is multi-rail capable. If it is the node then pushes its own network interface information to the peer. This protocol distributes the network interface information to both nodes and subsequently the nodes can excercise the peer network interfaces as well as its own, as described in further detail in this section. Discovery can be enabled, disabled or in verification mode. If it is in verification mode, then it will cross reference the discovered peer NIDs with the configured NIDs and complain if there is a discrepancy, but will continue to use the configured NIDs</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Peer Credits</span></span><br><span class="line"><span class="comment">#Governs the number of concurrent sends to a single peer, End-to-end flow control accomplished at higher layer. e.g. max_rpcs_in_flight</span></span><br><span class="line"></span><br><span class="line">$ cat /proc/sys/lnet/peers </span><br><span class="line">$ cat /sys/kernel/debug/lnet/peers</span><br><span class="line">nid                      refs state  last   max   rtr   min    tx   min queue </span><br><span class="line">xx.xx.xx.xx@o2ib            3    up    -1   126   126   126   126   110 0</span><br><span class="line">tx is the number of peer credits currently available <span class="keyword">for</span> this peer</span><br><span class="line">min is the smallest number of peer credits seen </span><br><span class="line">Negative credit count indicates the number of messages awaiting a credit</span><br><span class="line"></span><br><span class="line"><span class="comment">#lnet network Interface Credits</span></span><br><span class="line">$ cat /proc/sys/lnet/nis</span><br><span class="line">nid                      status alive refs peer  rtr   max    tx   min </span><br><span class="line">xx.xx.xx.xx@o2ib              up    -1    9  126    0  2048  2048  1796 </span><br><span class="line">max is total available (i.e. value of ko2iblnd credits) </span><br><span class="line">tx is the number currently available, Negative number indicates number of messages awaiting a credit</span><br><span class="line">min is the low water mark</span><br><span class="line"></span><br><span class="line"><span class="comment">#Lctl conn_list–List active TCP connections, type (bulk/control), tx_buffer_size, rx_buffer_size</span></span><br><span class="line">$ lctl --net tcp conn_list</span><br><span class="line"></span><br><span class="line">chmod a+w /sys/module/ksocklnd/parameters/*</span><br><span class="line"><span class="built_in">echo</span> 2000 &gt; /sys/module/ksocklnd/parameters/credits</span><br><span class="line"><span class="comment"># the number of concurrent sends (to all peers), defaults:64</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 240 &gt; /sys/module/ksocklnd/parameters/peer_timeout</span><br><span class="line"><span class="comment">## default 180</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## not test</span></span><br><span class="line"><span class="built_in">echo</span> 256 &gt; /sys/module/ksocklnd/parameters/peer_buffer_credits</span><br><span class="line"><span class="comment">#default: 0; peer_buffer_credits=256 # per-peer router buffer credits</span></span><br><span class="line"><span class="comment"># concurrent_sends=256 - send work-queue sizing # not test</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 32 &gt; /sys/module/ksocklnd/parameters/peer_credits</span><br><span class="line"><span class="comment">## the number of concurrent sends to a single peer, #default:8</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 70 &gt; /sys/module/ksocklnd/parameters/sock_timeout</span><br><span class="line"><span class="comment">## default: 50 sec</span></span><br><span class="line"></span><br><span class="line">chmod a+w /sys/module/lnet/parameters/*</span><br><span class="line"><span class="built_in">echo</span> 15 &gt; /sys/module/lnet/parameters/accept_timeout</span><br><span class="line"><span class="comment">##default: 5  Acceptor&#x27;s timeout (seconds)</span></span><br><span class="line">options lnet accept_timeout=15</span><br><span class="line"><span class="comment">## Specifies the number of seconds the server waits for data to arrive from the client. If data does not arrive before the timeout expires then the connection is closed. By setting it to less than the default 30 seconds, you can free up threads sooner. However, you may also disconnect users with slower connections.</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 2000 &gt; /sys/module/lnet/parameters/accept_backlog</span><br><span class="line"><span class="comment">##default: 127 Acceptor&#x27;s listen backlog</span></span><br><span class="line">options lnet accept_backlog=2000</span><br><span class="line"><span class="comment">## Acceptor&#x27;s listen backlog, the number of the connections the server instance can buffer in the wait queue.</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 6 &gt; /sys/module/lnet/parameters/lnet_retry_count</span><br><span class="line"><span class="comment">## default: 3 lnet_retry_count:Maximum number of times to retry transmitting a message</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/module/lnet/parameters/use_tcp_bonding</span><br><span class="line"><span class="comment">## default: 1  use_tcp_bonding:Set to 1 to use socklnd bonding. 0 to use Multi-Rail</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## default lfs.conf</span></span><br><span class="line">options lnet networks=tcp(bond0)</span><br><span class="line">options lnet use_tcp_bonding=1</span><br><span class="line">options lnet accept_backlog=2000</span><br><span class="line">options lnet accept_timeout=15</span><br><span class="line">options lnet lnet_retry_count=6</span><br><span class="line">options ksocklnd credits=2000</span><br><span class="line">options ksocklnd peer_credits=32</span><br><span class="line">options ksocklnd sock_timeout=70</span><br><span class="line">options ptlrpc at_max=320</span><br><span class="line">options ptlrpc at_min=50</span><br><span class="line">options ptlrpc ldlm_enqueue_min=240</span><br><span class="line"></span><br><span class="line"><span class="comment"># The maximum number of pages that will be sent in a single RPC request to the OST</span></span><br><span class="line">$ lctl get_param osc.*.max_pages_per_rpc</span><br><span class="line">$ lctl set_param osc.*.max_pages_per_rpc=1024 <span class="comment"># 1024 = 1024*4KB =4MB per RPC</span></span><br><span class="line"><span class="comment">#Max RPCS in flight between OSC and OST</span></span><br><span class="line">$ lctl set_param -P <span class="variable">$FNAME</span>.osc.max_pages_per_rpc=1024</span><br><span class="line"></span><br><span class="line">MDS $ lctl set_param -P osc.*.max_pages_per_rpc=1024 osc.*.max_rpcs_in_flight=128  mdc.*.max_rpcs_in_flight=128 osc.*.max_dirty_mb=1024 llite.*.max_read_ahead_mb=1024 osc.*.grant_shrink=0  osc.*.brw_size=16 osc.*.checksums=0</span><br><span class="line"></span><br><span class="line">$ lctl set_param osc.*.max_rpcs_in_flight=64;</span><br><span class="line"><span class="comment"># Max number of 4K pages per RPC</span></span><br><span class="line"><span class="comment"># Increase for small IO or long fast network paths (high BDP), May want to decrease to preempt TCP congestion</span></span><br><span class="line"></span><br><span class="line">256 = 1MB per RPC</span><br><span class="line"><span class="comment"># max_pages_per_rpc*4*max_rpcs_in_flight*2=max_dirty_mb</span></span><br><span class="line">1024*4KB/1024(KB to MB)*64*2=512</span><br><span class="line">$ lctl set_param osc.*.max_dirty_mb=512</span><br><span class="line"><span class="comment"># Maximum MBs of dirty data that can be written and queued on a client</span></span><br><span class="line"><span class="comment">## Got the current dirty bytes</span></span><br><span class="line">$ lctl get_param osc.*.cur_dirty_bytes</span><br><span class="line"><span class="comment">##  reports the amount of space this client has reserved for writeback cache with each OST</span></span><br><span class="line">$ lctl get_param osc.*.cur_grant_bytes</span><br><span class="line"></span><br><span class="line">Set per OST or each clients</span><br><span class="line">256*4/1024*64*2=128 </span><br><span class="line">lctl set_param osc.*.max_pages_per_rpc=256; lctl set_param osc.*.max_rpcs_in_flight=64;lctl set_param osc.*.max_dirty_mb=128</span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> the write cache</span><br><span class="line">512*4/1024*128*2=512</span><br><span class="line">lctl set_param osc.*.max_pages_per_rpc=512; lctl set_param osc.*.max_rpcs_in_flight=128;lctl set_param osc.*.max_dirty_mb=512</span><br><span class="line"></span><br><span class="line">$ modinfo mdt | grep max_mod_rpcs_per_client</span><br><span class="line">parm:           max_mod_rpcs_per_client:maximum number of modify RPCs <span class="keyword">in</span> flight allowed per client (uint)</span><br><span class="line"></span><br><span class="line">mds $ <span class="built_in">echo</span> 16 &gt; /sys/module/mdt/parameters/max_mod_rpcs_per_client</span><br><span class="line"></span><br><span class="line"><span class="comment"># config</span></span><br><span class="line">lctl network up/down</span><br><span class="line">lctl list_nids</span><br><span class="line">lctl ping xxxxx@tcp</span><br><span class="line">lctl network unconfigure</span><br><span class="line"><span class="comment">### from lfs 2.7</span></span><br><span class="line">lnetctl lnet configure/unconfigure</span><br><span class="line">lnetctl net show -v</span><br><span class="line">lnetctl peer show -v</span><br><span class="line">lnetctl net add --net LNET --<span class="keyword">if</span> eth0</span><br><span class="line">lnetctl net del --net LNET</span><br><span class="line">// To <span class="built_in">export</span> the current configuration to a YAML file</span><br><span class="line">lnetctl <span class="built_in">export</span> FILE.yaml</span><br><span class="line">lnetctl <span class="built_in">export</span> &gt; FILE.yaml</span><br><span class="line">// To import the configuration from a YAML file</span><br><span class="line">lnetctl import FILE.yaml</span><br><span class="line">lnetctl import &lt; FILE.yaml </span><br><span class="line"></span><br><span class="line"><span class="comment">### Lnet multiple-plane</span></span><br><span class="line">options lnet networks=<span class="string">&quot;tcp1(eth1),tcp2(eth2),o2ib0(ib0)&quot;</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">options lnet ip2nets=<span class="string">&quot;tcp1(eth0) 192.168.0.[2,4] \</span></span><br><span class="line"><span class="string"> tcp1 192.168.0.*; o2ib1 132.6.[1-3],[2-8/2]&quot;</span></span><br><span class="line"><span class="comment">### [2-8/2] means 2,4,6,8</span></span><br></pre></td></tr></table></figure>

<h3 id="Trace-log"><a href="#Trace-log" class="headerlink" title="Trace log"></a>Trace log</h3><h4 id="lfs-log"><a href="#lfs-log" class="headerlink" title="lfs log"></a>lfs log</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># F_SETPIPE_SZ F_GETPIPE_SZ</span></span><br><span class="line"><span class="comment"># echo 104857600 &gt; /proc/sys/fs/pipe-max-size</span></span><br><span class="line"><span class="comment"># fs.pipe-max-size=104857600</span></span><br><span class="line"><span class="comment"># ulimit pipe size            (512 bytes, -p) 8 = 4096 bytes, it &#x27;s pipe buffer size in the ulimit, not pipe size</span></span><br><span class="line"><span class="comment">## POSIX.1-2001 says that write(2)s of less than PIPE_BUF  bytes  must  be atomic:  the  output  data  is  written  to  the  pipe  as a contiguous sequence.  Writes of more than PIPE_BUF bytes may  be  non-atomic:  the kernel  may  interleave the data with data written by other processes.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># here 2 tools for pipe</span></span><br><span class="line"><span class="comment"># pv - Pipe Viewer - is a terminal-based tool for monitoring the progress of data through a pipeline.</span></span><br><span class="line"><span class="comment"># process1 | pv -pterbTCB 1G | process2</span></span><br><span class="line"></span><br><span class="line">$ pv -cN sources linux-image-unsigned-4.15.0-65-generic-dbgsym_4.15.0-65.74_amd64.ddeb | dd of=/tmp/tmp bs=512 | pv -cN cat</span><br><span class="line">      cat: 0.00 B 0:00:00 [0.00 B/s] [&lt;=&gt;                                                                                                                                                                                                    ]</span><br><span class="line">  sources:  751MiB 0:00:03 [ 191MiB/s] [===================================================================================================================================================================================&gt;] 100%            </span><br><span class="line">1538323+1 records <span class="keyword">in</span></span><br><span class="line">1538323+1 records out</span><br><span class="line">787621648 bytes (788 MB, 751 MiB) copied, 3.92424 s, 201 MB/s</span><br><span class="line"></span><br><span class="line">$ lctl set_param debug_mb=500</span><br><span class="line">$ mkfifo -m 777 /tmp/lfs.log; lctl debug_daemon start /tmp/lfs.log; tail -f /tmp/lfs.log | strings</span><br><span class="line">or</span><br><span class="line"><span class="comment">## 500M</span></span><br><span class="line">$ lctl debug_daemon start /tmp/lfs.log 500</span><br><span class="line"></span><br><span class="line">$ trace-cmd record -p <span class="keyword">function</span> mount <span class="variable">$ipaddr</span>@tcp:/lfs /mnt</span><br><span class="line"><span class="comment"># it &#x27;ll create trace.dat</span></span><br><span class="line">$ trace-cmd report</span><br><span class="line"></span><br><span class="line">$ lctl get_param debug_mb</span><br><span class="line">debug_mb=41</span><br><span class="line"></span><br><span class="line">$ lctl set_param debug_mb=512</span><br><span class="line">$ lctl set_param debug=-1</span><br><span class="line">$ lctl set_param debug=<span class="string">&quot;ioctl neterror warning error emerg ha config console lfsck malloc cache dentry mmap page info console rpctrace reada vfstrace rpctrace&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#default</span></span><br><span class="line">$ lctl set_param debug=<span class="string">&quot;ioctl neterror warning error emerg ha config console lfsck&quot;</span></span><br><span class="line">trace	Function entry/<span class="built_in">exit</span> markers</span><br><span class="line">dlmtrace	Distributed locking-related information</span><br><span class="line">inode	</span><br><span class="line">super	</span><br><span class="line">malloc	Memory allocation or free information</span><br><span class="line">cache	Cache-related information</span><br><span class="line">info	Non-critical general information</span><br><span class="line">dentry	kernel namespace cache handling</span><br><span class="line">mmap	Memory-mapped IO interface</span><br><span class="line">page	Page cache and bulk data transfers</span><br><span class="line">info	Miscellaneous informational messages</span><br><span class="line">net	LNet network related debugging</span><br><span class="line">console	Significant system events, printed to console</span><br><span class="line">warning	Significant but non-fatal exceptions, printed to console</span><br><span class="line">error	Critical error messages, printed to console</span><br><span class="line">neterror	Significant LNet error messages</span><br><span class="line">emerg	Fatal system errors, printed to console</span><br><span class="line">config	Configuration and setup, enabled by default</span><br><span class="line">ha	Failover and recovery-related information, enabled by default</span><br><span class="line">hsm	Hierarchical space management/tiering</span><br><span class="line">ioctl	IOCTL-related information, enabled by default</span><br><span class="line">layout	File layout handling (PFL, FLR, DoM)</span><br><span class="line">lfsck	Filesystem consistency checking, enabled by default</span><br><span class="line">other	Miscellaneious other debug messages</span><br><span class="line">quota	Space accounting and management</span><br><span class="line">reada	Client readahead management</span><br><span class="line">rpctrace	Remote request/reply tracing and debugging</span><br><span class="line">sec	Security, Kerberos, Shared Secret Key handling</span><br><span class="line">snapshot	Filesystem snapshot management</span><br><span class="line">vfstrace	Kernel VFS interface operations</span><br><span class="line"></span><br><span class="line"><span class="comment">### Got the mds process with client</span></span><br><span class="line">mds $ lctl set_param debug=+rpctrace</span><br><span class="line">mds $ lctl dk &gt; dk</span><br><span class="line">00000100:00100000:3.0:1621645680.953924:0:5370:0:(service.c:2089:ptlrpc_server_handle_request()) Handling RPC pname:cluuid+ref:pid:xid:nid:opc ll_mgs_0002:781f6010-0ac2-a476-4567-56bb7b70d013+9:86470:x1700165046065472:12345-<span class="variable">$client_IP</span>@tcp:400</span><br><span class="line"></span><br><span class="line"><span class="comment">### Got the client process info</span></span><br><span class="line">client $ lctl set_param debug=+rpctrace</span><br><span class="line">client $ lctl dk &gt; dk</span><br><span class="line">00000100:00100000:15.0:1621645914.531574:0:77490:0:(client.c:1682:ptlrpc_send_new_req()) Sending RPC pname:cluuid:pid:xid:nid:opc vim:8c1a0181-62fb-78ee-8651-31b8edfd4244:77490:1700165046080640:<span class="variable">$MDS_IP</span>@tcp:101</span><br></pre></td></tr></table></figure>

<h4 id="kdump"><a href="#kdump" class="headerlink" title="kdump"></a>kdump</h4><p>yum -y install kexec-tools<br>cat /etc/kdump.conf<br>nfs my.nfsserver.example.org:/path/to/expor<br>core_collector makedumpfile -d 16 -c<br>#-c Compress dump data by each page<br>#core_collector makedumpfile -d 16 -c message_level 16</p>
<h1 id="1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages"><a href="#1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages" class="headerlink" title="1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages"></a>1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages</h1><h1 id="1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages"><a href="#1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages" class="headerlink" title="1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages"></a>1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages</h1><p>#ssh <a href="mailto:&#x75;&#x73;&#x65;&#114;&#x40;&#109;&#121;&#46;&#115;&#101;&#x72;&#118;&#101;&#x72;&#x2e;&#x65;&#x78;&#97;&#x6d;&#x70;&#x6c;&#101;&#46;&#x6f;&#x72;&#103;">&#x75;&#x73;&#x65;&#114;&#x40;&#109;&#121;&#46;&#115;&#101;&#x72;&#118;&#101;&#x72;&#x2e;&#x65;&#x78;&#97;&#x6d;&#x70;&#x6c;&#101;&#46;&#x6f;&#x72;&#103;</a>:/dest/path<br>#By default, uses ssh key at /root/.ssh/kdump_id_rsa<br>#core_collector makedumpfile <options></p>
<h3 id="changelog"><a href="#changelog" class="headerlink" title="changelog"></a>changelog</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">* MARK – Internal record keeping</span><br><span class="line">* CREAT – Regular file creation</span><br><span class="line">* MKDIR – Directory creation</span><br><span class="line">* HLINK – Hard link</span><br><span class="line">* SLINK – Soft link</span><br><span class="line">* OPEN – open file</span><br><span class="line">* CLOSE – close file</span><br><span class="line">* MKNOD – Other file creation</span><br><span class="line">* UNLNK – Regular file removal</span><br><span class="line">* RMDIR – Directory removal</span><br><span class="line">* RNMFM – Rename, original</span><br><span class="line">* RNMTO – Rename, final</span><br><span class="line">* IOCTL – ioctl on file or directory</span><br><span class="line">* TRUNC – Regular file truncated</span><br><span class="line">* SATTR – Attribute change</span><br><span class="line">* XATTR – Extended Attribute change</span><br><span class="line">* HSM – HSM action</span><br><span class="line">* UNKNW – Unkown operation</span><br></pre></td></tr></table></figure>

<h4 id="Enable"><a href="#Enable" class="headerlink" title="Enable"></a>Enable</h4><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param mdt.$FNAME-MDT000<span class="number">0</span>.hsm_control=enabled</span><br><span class="line">$ lctl set_param -P  mdt.$FNAME-MDT000<span class="number">0</span>.hsm_control=enabled</span><br><span class="line">$ lctl set_param mdt.$FNAME-MDT000<span class="number">0</span>.hsm.max_requests=<span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create user</span></span><br><span class="line">$ lctl --device $FNAME-MDT000<span class="number">0</span> changelog_register</span><br><span class="line"><span class="comment"># del user</span></span><br><span class="line">$ lctl --device fsname-MDT000<span class="number">0</span> changelog_deregister cl1</span><br><span class="line"><span class="comment"># Get the size</span></span><br><span class="line">$ lctl get_param mdd.$FNAME-MDT000<span class="number">0</span>.changelog_users mdd.$FNAME-MDT000<span class="number">0</span>.changelog_size</span><br><span class="line"></span><br><span class="line"><span class="comment"># changelog mask</span></span><br><span class="line">$ lctl set_param mdd.$FNAME-MDT*.changelog_mask=MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RNMFM RNMTO OPEN LYOUT TRUNC CLOSE IOCTL TRUNC SATTR XATTR HSM MTIME CTIME</span><br><span class="line">$ lctl get_param mdd.$FNAME-MDT*.changelog_mask</span><br><span class="line">MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RENME RNMTO OPEN LYOUT TRUNC SATTR XATTR HSM MTIME CTIME</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the changelog</span></span><br><span class="line">$ lfs changelog $FNAME-MDT000<span class="number">0</span> &gt; lfs-changelog</span><br><span class="line">$ fs changelog $fsname-MDT000<span class="number">0</span> [startrec [endrec]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># clear all</span></span><br><span class="line">$ lctl changelog_clear mdt_name userid endrec</span><br><span class="line"><span class="string">``</span><span class="string">``</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Disable </span></span><br></pre></td></tr></table></figure>
<p>#Notify a device that user cl1 no longer needs records (up toand including 3)<br>$ lfs changelog_clear $FNAME-MDT0000 cl1 3</p>
<p>#To stop changelogs, changelog_mask should be set to MARK only<br>$ lctl set_param mdd.$FNAME-MDT0000.changelog_mask=MARK<br>mdd.lfs-MDT0000.changelog_mask=MARK</p>
<p>#or youcan set it -all<br>$ lctl set_param mdd.$FNAME-MDT0000.changelog_mask=-all</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### FSCK</span><br><span class="line">```bash</span><br><span class="line">Dec <span class="number">29</span> <span class="number">14</span>:<span class="number">11</span>:<span class="number">32</span> mookie kernel: LDISKFS-fs error (device sdz): ldiskfs_lookup: unlinked inode <span class="number">5384166</span> <span class="keyword">in</span> dir #<span class="number">145170469</span></span><br><span class="line">Dec <span class="number">29</span> <span class="number">14</span>:<span class="number">11</span>:<span class="number">32</span> mookie kernel: Remounting filesystem read-only</span><br></pre></td></tr></table></figure>

<h4 id="Flush-the-journal"><a href="#Flush-the-journal" class="headerlink" title="Flush the journal"></a>Flush the journal</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ umount /lfs</span><br><span class="line">$ mount -t ldiskfs /dev/sdx /lfs</span><br><span class="line">$ umount /lfs</span><br></pre></td></tr></table></figure>

<ul>
<li><p>Ensure e2fsprogs version ,it ‘s not default linux version ,it ‘s lfs version</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa | grep e2fsprogs</span><br><span class="line">e2fsprogs-1.42.12.wc1-7.el6.x86_64</span><br><span class="line">e2fsprogs-libs-1.42.12.wc1-7.el6.x86_64</span><br></pre></td></tr></table></figure>
</li>
<li><p>Before fsck，make sure the mount point has been <font color=red>umount</font></p>
</li>
<li><p>Can check multiple MDT/OSTs in parallel</p>
</li>
</ul>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check only mode</span></span><br><span class="line">$ e2fsck -fn <span class="regexp">/dev/</span>sdx</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prudent mode</span></span><br><span class="line">$ e2fsck -fp <span class="regexp">/dev/</span>sdx</span><br><span class="line"></span><br><span class="line"><span class="comment"># Answer yes</span></span><br><span class="line">$ e2fsck -fy <span class="regexp">/dev/</span>sdx</span><br></pre></td></tr></table></figure>

<h4 id="re-writeconf"><a href="#re-writeconf" class="headerlink" title="re-writeconf"></a>re-writeconf</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mds$ tunefs.lfs --writeconf /dev/sdx</span><br><span class="line">oss$ tunefs.lfs --writeconf /dev/ost0</span><br></pre></td></tr></table></figure>
<p>If MGS and MDT in single block device, you can add “-o nosvc” to avoid mount MDT</p>
<h3 id="User-group-quota"><a href="#User-group-quota" class="headerlink" title="User group quota"></a>User group quota</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## you can &#x27;t use lctl set_param, it &#x27;s not work</span></span><br><span class="line">mds $ lctl conf_param/set_param -P fsname.quota.ost|mdt=u|g|p|ugp|none</span><br><span class="line"></span><br><span class="line"><span class="comment">#enable</span></span><br><span class="line">mds $ lctl conf_param  <span class="variable">$FNAME</span>.quota.ost=ugp</span><br><span class="line">mds $ lctl conf_param  <span class="variable">$FNAME</span>.quota.mdt=ugp</span><br><span class="line"></span><br><span class="line">mds $ cat /proc/fs/lfs/osd-ldiskfs/<span class="variable">$FNAME</span>-MDT0000/quota_slave/info</span><br><span class="line">quota enabled:  <span class="string">&quot;ug&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#disable quota</span></span><br><span class="line">mds $ lctl set_param -P <span class="variable">$FNAME</span>.quota.ost=none</span><br><span class="line">mds $ lctl set_param -P <span class="variable">$FNAME</span>.quota.mdt=none</span><br><span class="line"></span><br><span class="line"><span class="comment">### lctl set_param -P must reboot, no -P not work, if you don&#x27; t reboot ,you have too use conf_param</span></span><br><span class="line"></span><br><span class="line">client $ lfs setquota -u user1 -b 307200 -B 309200 -i 10000 -I 11000 /mnt/lfs</span><br><span class="line">client $ lfs setquota -g group1 -b 5120000 -B 5150000 -i 100000 -I 101000 /mnt/lfs</span><br><span class="line"></span><br><span class="line">client $ lfs quota -u user1 -v /mnt/lfs</span><br><span class="line">client $ lfs quota -t -p /mnt/lfs</span><br><span class="line">Block grace time: 1w; Inode grace time: 1w</span><br></pre></td></tr></table></figure>

<h3 id="Disable-the-ost"><a href="#Disable-the-ost" class="headerlink" title="Disable the ost"></a>Disable the ost</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mds $ mds lctl dl | grep osc</span><br><span class="line">8 UP osp lfs-OST0000-osc-MDT0000 lfs-MDT0000-mdtlov_UUID 5</span><br><span class="line"></span><br><span class="line">mds $ lctl --device 8 deactivate</span><br><span class="line">or </span><br><span class="line">mds $ lctl conf_param fsname-OST000.osc.active=0</span><br><span class="line">mds $ lctl --device MGS llog_print fsname-MDT0000</span><br><span class="line"></span><br><span class="line">mds $ lctl --device 8 activate</span><br></pre></td></tr></table></figure>

<h3 id="Skip-recovery"><a href="#Skip-recovery" class="headerlink" title="Skip recovery"></a>Skip recovery</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">mds</span> $ mds lctl dl | grep osc</span><br><span class="line"><span class="attribute">8</span> UP mdt testfs<span class="number">0</span>-MDT<span class="number">0000</span> testfs<span class="number">0</span>-MDT<span class="number">0000</span>_UUID <span class="number">10</span></span><br><span class="line"><span class="attribute">mds</span> $ lctl dl | grep <span class="string">&quot;UP mdt&quot;</span> | awk &#x27;&#123;print $<span class="number">1</span>&#125;&#x27;</span><br><span class="line"><span class="attribute">8</span></span><br><span class="line"><span class="attribute">mds</span> $ lctl --device <span class="number">8</span> abort_recovery</span><br><span class="line"></span><br><span class="line"><span class="attribute">or</span> </span><br><span class="line"></span><br><span class="line"><span class="attribute">mount</span>.lfs xxx xxx -o abort_recov</span><br></pre></td></tr></table></figure>

<h3 id="lfs-migarate"><a href="#lfs-migarate" class="headerlink" title="lfs migarate"></a>lfs migarate</h3><p>Strong not recommand this command, because the command will cause loss the data, I suggest you copy data by index and checksum the copy file</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">$ lfs setstripe -c 1  -i 4 /lfs/dir1</span><br><span class="line">$ copy /lfs/old_dir1/file1 /lfs/dir1</span><br><span class="line">$ md5sum /lfs/old_dir1/file1 /lfs/dir1/file1</span><br><span class="line"></span><br><span class="line"><span class="comment"># dont &#x27;t use lfs migrate, it &#x27;s too dangerous, it will cause data loss</span></span><br><span class="line"><span class="comment">## lfs find /opt/lfswh -obd lfswh-OST000c_UUID -size +4G | lfs_migrate -y</span></span><br><span class="line"><span class="comment">## lfs migrate -c 1  -i 4 filepath</span></span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">### Job status</span></span><br><span class="line">```bash</span><br><span class="line">client $  lctl get_param jobid_var</span><br><span class="line">client $  jobid_var=<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line">SLURM: jobid_var=SLURM_JOB_ID</span><br><span class="line">SGE: jobid_var=JOB_ID</span><br><span class="line">LSF: jobid_var=LSB_JOBID</span><br><span class="line">Loadleveler: jobid_var=LOADL_STEP_ID</span><br><span class="line">PBS: jobid_var=PBS_JOBID</span><br><span class="line">Maui/MOAB: jobid_var=PBS_JOBID</span><br><span class="line"><span class="comment"># Enable for sge</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=JOB_ID</span><br><span class="line"></span><br><span class="line"><span class="comment"># disable</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If there isn&#x27;t any job scheduler is running over the system, or user just want to collect the stats for process &amp; uid:</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=procname_uid</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check Job status</span></span><br><span class="line">oss $ lctl get_param obdfilter.testfs5-OST0004.job_stats</span><br><span class="line">job_stats:</span><br><span class="line">- job_id:          9158530</span><br><span class="line">  snapshot_time:   1503038800</span><br><span class="line">  read_bytes:      &#123; samples:           0, unit: bytes, min:       0, max:       0, sum:               0 &#125;</span><br><span class="line">  write_bytes:     &#123; samples:       32452, unit: bytes, min:  262144, max: 1048576, sum:     34009513984 &#125;</span><br><span class="line">  getattr:         &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  setattr:         &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># get mdt ops</span></span><br><span class="line">mds $ lctl get_param mdt.*.job_stats</span><br><span class="line">mds $ lctl get_param  mdt.testfs5-MDT0000.job_stats</span><br><span class="line">mdt.testfs5-MDT0000.job_stats=</span><br><span class="line">job_stats:</span><br><span class="line">- job_id:          278685</span><br><span class="line">  snapshot_time:   1503068243</span><br><span class="line">  open:            &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  close:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  mknod:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  link:            &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  unlink:          &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  mkdir:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># clear stats for all job on testfs-OST0001</span></span><br><span class="line">oss $ lctl set_param obdfilter.testfs-OST0001.job_stats=clear</span><br><span class="line"></span><br><span class="line"><span class="comment"># Clear stats for job &quot;dd.0&quot; on lfs-MDT0000</span></span><br><span class="line">mds $ lctl set_param mdt.lfs-MDT0000.job_stats=dd.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># cleanup interval (seconds)</span></span><br><span class="line">lctl set_param -P testfs5.mdt.job_cleanup_interval=604800</span><br><span class="line">lctl set_param  testfs5.mdt.job_cleanup_interval=604800</span><br><span class="line">mds $  cat /proc/fs/lfs/mdt/testfs5-MDT0000/job_cleanup_interval</span><br></pre></td></tr></table></figure>

<h3 id="lfs-fid-and-path"><a href="#lfs-fid-and-path" class="headerlink" title="lfs fid and path"></a>lfs fid and path</h3><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[client]<span class="comment"># lfs fid2path /mnt      [0x200000400:0x1:0x0]</span></span><br><span class="line">                                       |<span class="string">         </span>|<span class="string">   </span>|</span><br><span class="line">                                       |<span class="string">         </span>|<span class="string">   -- version</span></span><br><span class="line"><span class="string">                                       </span>|<span class="string">         ---- object id</span></span><br><span class="line"><span class="string">                                       ----------Sequence</span></span><br><span class="line"><span class="string">[client]# lfs path2fid /mnt</span></span><br><span class="line"><span class="string">[0x200000007:0x1:0x0]</span></span><br></pre></td></tr></table></figure>

<h3 id="increase-openzfs-sync-performance-in-test-env"><a href="#increase-openzfs-sync-performance-in-test-env" class="headerlink" title="increase openzfs sync performance in test env"></a>increase openzfs sync performance in test env</h3><p><code>this setting will cause data loss, if client roll back log failed</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">lctl set_param osd-zfs.*.osd_obj_sync_delay_us=0</span><br><span class="line">or</span><br><span class="line">lctl set_param osd-zfs.*.osd_object_sync_delay_us=0</span><br><span class="line">or</span><br><span class="line"><span class="built_in">echo</span> 0 &gt; /sys/module/osd_zfs/parameters/osd_object_sync_delay_us</span><br><span class="line"></span><br><span class="line"><span class="comment">## to default</span></span><br><span class="line"><span class="built_in">echo</span> -1 &gt; /sys/module/osd_zfs/parameters/osd_object_sync_delay_us</span><br><span class="line">osd_object_sync_delay_us</span><br><span class="line">To improve fsync() performance until ZIL device,it is possible <span class="built_in">disable</span> the code <span class="built_in">which</span> causes Lfs to block waiting on a TXG to sync</span><br></pre></td></tr></table></figure>

<h3 id="Lfs-issues"><a href="#Lfs-issues" class="headerlink" title="Lfs issues"></a>Lfs issues</h3><h4 id="Don-‘t-use-openzfs-with-lfs-file-system"><a href="#Don-‘t-use-openzfs-with-lfs-file-system" class="headerlink" title="Don ‘t use openzfs with lfs file system"></a>Don ‘t use openzfs with lfs file system</h4><ul>
<li>No bility to support openzfs</li>
<li>If your zpool brain split, they will not show you any help</li>
<li>If you enable openzfs MMP, if single HDD will broken, your zpool have to suspend once<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/issues/7045">https://github.com/zfsonlinux/zfs/issues/7045</a><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/issues/7118">https://github.com/zfsonlinux/zfs/issues/7118</a></li>
<li>this issue impact from zfs 0.7 to 0.7.9, I ‘m not sure 0.8 has the same issue, the best way is disable MMP and use single link for your SAS devices</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Multipath-failed-cause"><a href="#Multipath-failed-cause" class="headerlink" title="Multipath failed cause"></a>Multipath failed cause</h4><ul>
<li>Found error log; <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line">Sep 22 03:16:00 lfs-node21 kernel: scsi 14:0:5:31: attempting task abort! scmd(ffff97ac51784a80)</span><br><span class="line">Sep 22 03:16:00 lfs-node21 kernel: scsi 14:0:5:31: [sg64] tag<span class="comment">#76 CDB: Read(6) 08 00 02 61 01 00</span></span><br><span class="line">Sep 22 03:16:00 lfs-node21 kernel: scsi target14:0:5: _scsih_tm_display_info: handle(0x000a), sas_address(0x500a098b4c66c014), phy(4)</span><br><span class="line">Sep 22 03:16:00 lfs-node21 kernel: scsi target14:0:5: enclosurelogical id(0x51866da0a8a62200), slot(4)</span><br><span class="line">Sep 22 03:16:00 lfs-node21 kernel: scsi target14:0:5: enclosure level(0x0000), connector name( 1   )</span><br><span class="line">Sep 22 03:16:01 lfs-node21 systemd: Started Session 361698 of user root.</span><br><span class="line">Sep 22 03:16:01 lfs-node21 systemd: Starting Session 361698 of user root.</span><br><span class="line">Sep 22 03:16:09 lfs-node21 kernel: sd 14:0:5:6: attempting task abort! scmd(ffff97b2ae507b80)</span><br><span class="line">Sep 22 03:16:09 lfs-node21 kernel: sd 14:0:5:6: [sdp] tag<span class="comment">#8 CDB: Write(16) 8a 00 00 00 00 07 c6 2a 89 e8 00 00 08 00 00 00</span></span><br><span class="line">Sep 22 03:16:09 lfs-node21 kernel: scsi target14:0:5: _scsih_tm_display_info: handle(0x000a), sas_address(0x500a098b4c66c014), phy(4)</span><br><span class="line">Sep 22 03:16:09 lfs-node21 kernel: scsi target14:0:5: enclosurelogical id(0x51866da0a8a62200), slot(4)</span><br><span class="line">Sep 22 03:16:09 lfs-node21 kernel: scsi target14:0:5: enclosure level(0x0000), connector name( 1   )</span><br><span class="line">Sep 22 03:16:30 lfs-node21 kernel: mpt3sas_cm3: Command Timeout</span><br><span class="line">Sep 22 03:16:30 lfs-node21 kernel: mf:<span class="comment">#012#0110100000a 00000100 00000000 00001f00 00000000 00000000 00000000 00000000 #012#01100000000 00000000 00000000 00000000 0000004d</span></span><br><span class="line">Sep 22 03:16:32 lfs-node21 kernel: scsi 13:0:0:31: attempting task abort! scmd(ffff97bc6aad1c00)</span><br><span class="line">Sep 22 03:16:32 lfs-node21 kernel: scsi 13:0:0:31: [sg45] tag<span class="comment">#197 CDB: Read(6) 08 00 02 00 01 00</span></span><br><span class="line">Sep 22 03:16:32 lfs-node21 kernel: scsi target13:0:0: _scsih_tm_display_info: handle(0x0009), sas_address(0x500a098b4c6c3014), phy(0)</span><br><span class="line">Sep 22 03:16:32 lfs-node21 kernel: scsi target13:0:0: enclosurelogical id(0x51866da0a8a62100), slot(0)</span><br><span class="line">Sep 22 03:16:32 lfs-node21 kernel: scsi target13:0:0: enclosure level(0x0000), connector name( 0   )</span><br><span class="line">Sep 22 03:16:39 lfs-node21 kernel: sd 13:0:1:6: attempting task abort! scmd(ffff97b2ae506680)</span><br><span class="line">Sep 22 03:16:39 lfs-node21 kernel: sd 13:0:1:6: [sdap] tag<span class="comment">#4 CDB: Write(16) 8a 00 00 00 00 07 c5 fd 27 10 00 00 08 00 00 00</span></span><br><span class="line">Sep 22 03:16:39 lfs-node21 kernel: scsi target13:0:1: _scsih_tm_display_info: handle(0x000a), sas_address(0x500a098b4c5d9014), phy(4)</span><br><span class="line">Sep 22 03:16:39 lfs-node21 kernel: scsi target13:0:1: enclosurelogical id(0x51866da0a8a62100), slot(4)</span><br><span class="line">Sep 22 03:16:39 lfs-node21 kernel: scsi target13:0:1: enclosure level(0x0000), connector name( 1   )</span><br><span class="line">Sep 22 03:16:40 lfs-node21 kernel: mpt3sas_cm3: sending diag reset !!</span><br><span class="line">Sep 22 03:16:41 lfs-node21 kernel: mpt3sas_cm3: diag reset: SUCCESS</span><br><span class="line">Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: IOC Number : 0</span><br><span class="line">Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: CurrentHostPageSize is 0: Setting default host page size to 4k</span><br><span class="line">Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: FW Package Version(16.17.00.03)</span><br><span class="line">Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: LSISAS3008: FWVersion(16.00.04.00), ChipRevision(0x02), BiosVersion(18.00.00.00)</span><br><span class="line">Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: Dell 12Gbps SAS HBA</span><br><span class="line">Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: Protocol=(Initiator,Target), Capabilities=(TLR,EEDP,Snapshot Buffer,Diag Trace Buffer,Task Set Full,NCQ)</span><br><span class="line">Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: sending port <span class="built_in">enable</span> !!</span><br><span class="line"></span><br><span class="line">Sep 22 03:32:04 lfs-node21 multipathd: 3600a098000b4c3060000024359244375: sdbb - rdac checker reports path is down: inquiry failed</span><br><span class="line">Sep 22 03:32:04 lfs-node21 multipathd: checker failed path 67:80 <span class="keyword">in</span> map 3600a098000b4c3060000024359244375</span><br><span class="line">Sep 22 03:32:04 lfs-node21 multipathd: 3600a098000b4c3060000024359244375: remaining active paths: 1</span><br><span class="line">Sep 22 03:32:04 lfs-node21 multipathd: 8:240: reinstated</span><br><span class="line">Sep 22 03:32:04 lfs-node21 multipathd: sdbg: mark as failed</span><br><span class="line">Sep 22 03:32:04 lfs-node21 multipathd: 3600a098000b4c66c0000020d59243745: remaining active paths: 1</span><br><span class="line">Sep 22 03:32:04 lfs-node21 multipathd: sdbi: mark as failed</span><br><span class="line">Sep 22 03:32:04 lfs-node21 multipathd: 3600a098000b4c66c00000213592437d2: remaining active paths: 1</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdu, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdv, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdw, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdx, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdy, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdar, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdas, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdat, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdau, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdav, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdaw, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdax, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sday, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdaz, open() failed: No such device</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c3060000024359244375: sdbb - rdac checker reports path is upa</span><br><span class="line"></span><br><span class="line"><span class="comment">### All devices single link offline at the same time, I have 4 x 9300-8E, impossible offline at the same time</span></span><br><span class="line"><span class="comment">### Filter all error devices and make sure just single HBA status not correct, and you can found mpt3sas_cm0 in the system message</span></span><br><span class="line"></span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c3060000024359244375: sdbb - rdac checker reports path is up</span><br><span class="line">Sep 22 03:32:14 lfs-node21 kernel: device-mapper: multipath: Reinstating path 67:80.</span><br><span class="line">Sep 22 03:32:14 lfs-node21 kernel: device-mapper: multipath: Reinstating path 67:160.</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 67:80: reinstated</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c3060000024359244375: remaining active paths: 2</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c66c0000020d59243745: sdbg - rdac checker reports path is up</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 67:160: reinstated</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c66c0000020d59243745: remaining active paths: 2</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c66c00000213592437d2: sdbi - rdac checker reports path is up</span><br><span class="line">Sep 22 03:32:14 lfs-node21 kernel: device-mapper: multipath: Reinstating path 67:192.</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 67:192: reinstated</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c66c00000213592437d2: remaining active paths: 2</span><br><span class="line"></span><br><span class="line">Sep 22 03:34:16 lfs-node21 kernel: scsi 1:0:0:31: attempting task abort! scmd(ffff97b265c7b2c0)</span><br><span class="line">Sep 22 03:34:16 lfs-node21 kernel: scsi 1:0:0:31: [sg9] tag<span class="comment">#17 CDB: Read(6) 08 00 02 00 01 00</span></span><br><span class="line">Sep 22 03:34:16 lfs-node21 kernel: scsi target1:0:0: _scsih_tm_display_info: handle(0x0009), sas_address(0x500a098b4c834014), phy(0)</span><br><span class="line">Sep 22 03:34:16 lfs-node21 kernel: scsi target1:0:0: enclosurelogical id(0x51866da09fad6700), slot(0)</span><br><span class="line">Sep 22 03:34:16 lfs-node21 kernel: scsi target1:0:0: enclosure level(0x0000), connector name( 0   )</span><br><span class="line">Sep 22 03:34:46 lfs-node21 kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">Sep 22 03:34:46 lfs-node21 kernel: mf:<span class="comment">#012#01101000009 00000100 00000000 00001f00 00000000 00000000 00000000 00000000 #012#01100000000 00000000 00000000 00000000 00000012</span></span><br><span class="line">Sep 22 03:34:55 lfs-node21 kernel: sd 1:0:1:24: attempting task abort! scmd(ffff97a35d8c9500)</span><br><span class="line">Sep 22 03:34:55 lfs-node21 kernel: sd 1:0:1:24: [sdk] tag<span class="comment">#42 CDB: Read(16) 88 00 00 00 00 09 1f 1d 30 00 00 00 00 08 00 00</span></span><br><span class="line">Sep 22 03:34:55 lfs-node21 kernel: scsi target1:0:1: _scsih_tm_display_info: handle(0x000a), sas_address(0x500a098b4c810014), phy(4)</span><br><span class="line">Sep 22 03:34:55 lfs-node21 kernel: scsi target1:0:1: enclosurelogical id(0x51866da09fad6700), slot(4)</span><br><span class="line">Sep 22 03:34:55 lfs-node21 kernel: scsi target1:0:1: enclosure level(0x0000), connector name( 1   )</span><br><span class="line">Sep 22 03:34:56 lfs-node21 kernel: mpt3sas_cm0: sending diag reset !!</span><br><span class="line">Sep 22 03:34:57 lfs-node21 kernel: mpt3sas_cm0: diag reset: SUCCESS</span><br><span class="line">Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: IOC Number : 0</span><br><span class="line">Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: CurrentHostPageSize is 0: Setting default host page size to 4k</span><br><span class="line">Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: FW Package Version(16.17.00.03)</span><br><span class="line">Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: LSISAS3008: FWVersion(16.00.04.00), ChipRevision(0x02), BiosVersion(18.00.00.00)</span><br><span class="line">Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: Dell 12Gbps SAS HBA</span><br><span class="line">Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: Protocol=(Initiator,Target), Capabilities=(TLR,EEDP,Snapshot Buffer,Diag Trace Buffer,Task Set Full,NCQ)</span><br><span class="line">Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: sending port <span class="built_in">enable</span> !!</span><br><span class="line"></span><br><span class="line"><span class="comment"># del the others</span></span><br><span class="line"></span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: port <span class="built_in">enable</span>: SUCCESS</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="keyword">for</span> end-devices: start</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:0: handle(0x0009), sas_address(0x500a098b4c834014), port: 255</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:0: enclosure logical id(0x51866da09fad6700), slot(0)</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:1: handle(0x000a), sas_address(0x500a098b4c810014), port: 255</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:1: enclosure logical id(0x51866da09fad6700), slot(4)</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: <span class="comment">#011break from _scsih_search_responding_sas_devices: ioc_status(0x0022), loginfo(0x310f0400)</span></span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="keyword">for</span> end-devices: complete</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="keyword">for</span> end-devices: start</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="keyword">for</span> PCIe end-devices: complete</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="keyword">for</span> expanders: start</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="keyword">for</span> expanders: complete</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: removing unresponding devices: start</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: scsi 1:0:0:31: task abort: SUCCESS scmd(ffff97b265c7b2c0)</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: removing unresponding devices: sas end-devices</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: removing unresponding devices: pcie end-devices</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: removing unresponding devices: expanders</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: removing unresponding devices: complete</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: scan devices: start</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: sd 1:0:0:13: attempting task abort! scmd(ffff97b37ad6a4c0)</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: sd 1:0:0:13: [sdg] tag<span class="comment">#89 CDB: Inquiry 12 01 c9 00 30 00</span></span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:0: _scsih_tm_display_info: handle(0x0009), sas_address(0x500a098b4c834014), phy(0)</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:0: enclosurelogical id(0x51866da09fad6700), slot(0)</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:0: enclosure level(0x0000), connector name( 0   )</span><br></pre></td></tr></table></figure>
Single link offline, after reset the link attch back, and try to recovery the connnection</li>
</ul>
<p>But lfs has a <a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-10510">bug, LU-10510</a>, lfs will modify the value when it mount, but the designer not consider link/HBA/IO expander offline or some reason cause the value back to default value. what a pity, the operate (modify to 16383) only trigger on mount.lfs operation, the link back to recovery the connection ? no way.</p>
<p>It ‘s a simple logic. Why the desiger not consider the link offline ? I can ‘t understand. it must be an intern.<br>When the link recovery, the max_sectors_kb recovery to default value (512)<br>You have to use echo 16383 to improve it, why 16383, sorry , I don ‘t know.<br>Does the lfs system rigorous enough ? hahahahaahaha……………….<br>Does is it for Enterprise file system ? hahahahahahaha……………….</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">$ grep . /sys/block/*/queue/max_sectors_kb</span><br><span class="line">/sys/block/sdak/queue/max_sectors_kb 16383</span><br><span class="line">/sys/block/sdal/queue/max_hw_sectors_kb 16383</span><br><span class="line">/sys/block/sdal/queue/max_sectors_kb 512</span><br><span class="line">/sys/block/sda/queue/max_hw_sectors_kb 256</span><br><span class="line">/sys/block/sda/queue/max_sectors_kb 256</span><br><span class="line">/sys/block/sdba/queue/max_hw_sectors_kb 16383</span><br><span class="line">/sys/block/sdba/queue/max_sectors_kb 16383</span><br><span class="line">/sys/block/sdbb/queue/max_hw_sectors_kb 16383</span><br><span class="line">/sys/block/sdbb/queue/max_sectors_kb 16383</span><br><span class="line">/sys/block/sdbc/queue/max_hw_sectors_kb 16383</span><br><span class="line">/sys/block/sdbc/queue/max_sectors_kb 512</span><br><span class="line">/sys/block/sdbd/queue/max_hw_sectors_kb 16383</span><br><span class="line">/sys/block/sdbd/queue/max_sectors_kb 16383</span><br><span class="line">/sys/block/sdbe/queue/max_hw_sectors_kb 16383</span><br><span class="line">/sys/block/sdbe/queue/max_sectors_kb 512</span><br><span class="line">/sys/block/sdbf/queue/max_hw_sectors_kb 16383</span><br><span class="line"></span><br><span class="line">it could cause the multipath error too.</span><br><span class="line">blk_cloned_rq_check_limits: over max size <span class="built_in">limit</span>.</span><br><span class="line">device-mapper: multipath: Failing path 65:176.</span><br><span class="line">device-mapper: multipath: Reinstating path 65:160.</span><br><span class="line">device-mapper: multipath: Reinstating path 65:240.</span><br><span class="line">blk_cloned_rq_check_limits: over max size <span class="built_in">limit</span>.</span><br><span class="line">device-mapper: multipath: Failing path 65:240.</span><br><span class="line">device-mapper: multipath: Reinstating path 65:144.</span><br><span class="line">blk_cloned_rq_check_limits: over max size <span class="built_in">limit</span>.</span><br><span class="line">device-mapper: multipath: Failing path 65:144.</span><br><span class="line">device-mapper: multipath: Reinstating path 65:176.</span><br><span class="line">blk_cloned_rq_check_limits: over max size <span class="built_in">limit</span>.</span><br><span class="line">device-mapper: multipath: Failing path 65:176.</span><br><span class="line">device-mapper: multipath: Reinstating path 65:240.</span><br><span class="line">blk_cloned_rq_check_limits: over max size <span class="built_in">limit</span>.</span><br><span class="line">device-mapper: multipath: Failing path 65:240.</span><br><span class="line">device-mapper: multipath: Reinstating path 65:144.</span><br><span class="line">device-mapper: multipath: Reinstating path 65:176.</span><br><span class="line">device-mapper: multipath: Reinstating path 65:240.</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           0.00    0.00    0.02   47.92    0.00   52.06</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sda               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdb               0.00     0.00    0.00    0.00     0.00     0.00     0.00    67.00    0.00    0.00    0.00   0.00 100.00</span><br><span class="line">sdc               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdi               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdf               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdd               0.00     0.00    0.00    0.00     0.00     0.00     0.00   104.00    0.00    0.00    0.00   0.00 100.00</span><br><span class="line">sdh               0.00     0.00    0.00    0.00     0.00     0.00     0.00    31.00    0.00    0.00    0.00   0.00 100.00</span><br><span class="line">sdj               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdk               0.00     0.00    0.00    0.00     0.00     0.00     0.00    63.00    0.00    0.00    0.00   0.00 100.00</span><br><span class="line">sdl               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdm               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdn               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdo               0.00     0.00    0.00    0.00     0.00     0.00     0.00    64.00    0.00    0.00    0.00   0.00 100.00</span><br></pre></td></tr></table></figure>


<h4 id="disable-OSS-read-cache"><a href="#disable-OSS-read-cache" class="headerlink" title="disable OSS read cache"></a>disable OSS read cache</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#disable read cache on all the OSTs of an OSS</span></span><br><span class="line">$ lctl set_param osd-ldiskfs.*.read_cache_enable=0</span><br><span class="line"></span><br><span class="line"><span class="comment">#re-enable read cache </span></span><br><span class="line">$ lctl set_param osd-ldiskfs.&#123;OST_name&#125;.read_cache_enable=1</span><br><span class="line">$ lctl get_param osd-ldiskfs.*.read_cache_enable</span><br></pre></td></tr></table></figure>

<p>writethrough_cache_enable - Controls whether data sent to the OSS as a write request is kept in the read cache and available for later reads, or if it is discarded from cache when the write is completed. By default, the writethrough cache is enabled (writethrough_cache_enable=1).</p>
<p>If the writethrough cache is disabled (writethrough_cache_enabled=0), the OSS discards the data after the write request from the client is completed. For subsequent read requests, or partial-page write requests, the OSS must re-read the data from disk.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#disable writethrough_cache</span></span><br><span class="line">$ lctl get_param osd-ldiskfs.*.read_cache_enable=0 osd-ldiskfs.*.writethrough_cache_enable=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># re-enable </span></span><br><span class="line">$ lctl set_param osd-ldiskfs.*.read_cache_enable=1 osd-ldiskfs.*.writethrough_cache_enable=1</span><br><span class="line">$ lctl get_param osd-ldiskfs.*.read_cache_enable osd-ldiskfs.*.writethrough_cache_enable=1</span><br></pre></td></tr></table></figure>

<p>readcache_max_filesize - Controls the maximum size of a file that both the read cache and writethrough cache will try to keep in memory. Files larger than readcache_max_filesize will not be kept in cache for either reads or writes.</p>
<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ lctl <span class="keyword">set</span>_param obdfilter.*<span class="string">.readcache_max_filesize=32M</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#disable the maxinum cached file size on the OST</span></span><br><span class="line">$ lctl <span class="keyword">set</span>_param obdfilter.&#123;OST_name&#125;<span class="string">.readcache_max_filesize=-1</span></span><br><span class="line">$ lctl get_param obdfilter.*<span class="string">.readcache_max_filesize</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># worked</span></span><br><span class="line">$ lctl get_param osd-ldiskfs.*<span class="string">.readcache_max_filesize</span></span><br><span class="line">$ lctl <span class="keyword">set</span>_param osd-ldiskfs.*<span class="string">.readcache_max_filesize=0</span> <span class="comment"># -1 out of range</span></span><br></pre></td></tr></table></figure>

<h3 id="client-metadata-setting"><a href="#client-metadata-setting" class="headerlink" title="client metadata setting"></a>client metadata setting</h3><p>The MDC max_rpcs_in_flight parameter defines the maximum number of metadata RPCs, both modifying and non-modifying RPCs, that can be sent in parallel by a client to a MDT target. This includes every file system metadata operations, such as file or directory stat, creation, unlink. The default setting is 8, minimum setting is 1 and maximum setting is 256.</p>
<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">client $ lctl <span class="keyword">set</span>_param mdc.*<span class="string">.max_rpcs_in_flight=16</span></span><br></pre></td></tr></table></figure>

<p>The MDC max_mod_rpcs_in_flight parameter defines the maximum number of file system modifying RPCs that can be sent in parallel by a client to a MDT target. For example, the Lfs client sends modify RPCs when it performs file or directory creation, unlink, access permission modification or ownership modification. The default setting is 7, minimum setting is 1 and maximum setting is 256.</p>
<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clinet $  lctl <span class="keyword">set</span>_param mdc.*<span class="string">.max_mod_rpcs_in_flight=12</span></span><br></pre></td></tr></table></figure>
<p>The max_mod_rpcs_in_flight value must be strictly less than the max_rpcs_in_flight value. It must also be less or equal to the MDT max_mod_rpcs_per_client value. If one of theses conditions is not enforced, the setting fails and an explicit message is written in the Lfs log.</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mds $ echo <span class="number">12</span> &gt; <span class="regexp">/sys/m</span>odule<span class="regexp">/mdt/</span>parameters/max_mod_rpcs_per_client</span><br></pre></td></tr></table></figure>

<h3 id="record-several-years-ago-trace-the-file-by-debugfs"><a href="#record-several-years-ago-trace-the-file-by-debugfs" class="headerlink" title="record several years ago trace the file by debugfs"></a>record several years ago trace the file by debugfs</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">$ dd <span class="keyword">if</span>=RHEL5.5_x86_64.iso of=rhel5-1 bs=1M count=1 oflag=sync</span><br><span class="line"></span><br><span class="line">$ cat $(blktrace.log)</span><br><span class="line">  8,32   0     2591  1720.406537446 14972  Q   W 19144704 + 2048 [ll_ost_io00_002]</span><br><span class="line">  8,32   0     2592  1720.406539742 14972  G   W 19144704 + 2048 [ll_ost_io00_002]</span><br><span class="line">  8,32   0     2593  1720.406542136 14972  P   N [ll_ost_io00_002]</span><br><span class="line">  8,32   0     2594  1720.406543690 14972  I   W 19144704 + 2048 [ll_ost_io00_002]</span><br><span class="line"></span><br><span class="line">$ line=19144704</span><br><span class="line">$ debugfs -c -R <span class="string">&quot;icheck <span class="subst">$(($line/8)</span>)&quot;</span> /dev/sdc</span><br><span class="line">debugfs 1.42.12.wc1 (15-Sep-2014)</span><br><span class="line">/dev/sdc: catastrophic mode - not reading inode or group bitmaps</span><br><span class="line">Block	Inode number</span><br><span class="line">2393088	2623</span><br><span class="line">$ debugfs -R <span class="string">&quot;ncheck 2623&quot;</span> /dev/sdc</span><br><span class="line">debugfs 1.42.12.wc1 (15-Sep-2014)</span><br><span class="line">Inode	Pathname</span><br><span class="line">2623	/O/0/d13/2669</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ lfs getstripe rhel5-1</span><br><span class="line">rhel5-1</span><br><span class="line">lmm_stripe_count:   1</span><br><span class="line">lmm_stripe_size:    1048576</span><br><span class="line">lmm_pattern:        1</span><br><span class="line">lmm_layout_gen:     0</span><br><span class="line">lmm_stripe_offset:  3</span><br><span class="line">lmm_pool:           OST0003</span><br><span class="line">	obdidx		 objid		 objid		 group</span><br><span class="line">	     3	          2669	        0xa6d	             0</span><br><span class="line"></span><br><span class="line">$ debugfs -c -R <span class="string">&quot;stat &lt;2623&gt;&quot;</span> /dev/sdc</span><br><span class="line">debugfs 1.42.12.wc1 (15-Sep-2014)</span><br><span class="line">/dev/sdc: catastrophic mode - not reading inode or group bitmaps</span><br><span class="line">Inode: 2623   Type: regular    Mode:  0666   Flags: 0x80000</span><br><span class="line">Generation: 2790813459    Version: 0x00000005:00002bf8</span><br><span class="line">User:     0   Group:     0   Size: 1048576</span><br><span class="line">File ACL: 0    Directory ACL: 0</span><br><span class="line">Links: 1   Blockcount: 2048</span><br><span class="line">Fragment:  Address: 0    Number: 0    Size: 0</span><br><span class="line"> ctime: 0x5715e344:00000000 -- Tue Apr 19 15:50:28 2016</span><br><span class="line"> atime: 0x00000000:00000000 -- Thu Jan  1 08:00:00 1970</span><br><span class="line"> mtime: 0x5715e344:00000000 -- Tue Apr 19 15:50:28 2016</span><br><span class="line">crtime: 0x57148057:dfa2ff44 -- Mon Apr 18 14:36:07 2016</span><br><span class="line">Size of extra inode fields: 28</span><br><span class="line">Extended attributes stored <span class="keyword">in</span> inode body:</span><br><span class="line">  lma = <span class="string">&quot;08 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 6d 0a 00 00 00 00 00 00 &quot;</span> (24)</span><br><span class="line">  lma: fid=[0x100000000:0xa6d:0x0] compat=8 incompat=0</span><br><span class="line">  fid = <span class="string">&quot;d2 0b 00 00 02 00 00 00 40 00 00 00 00 00 00 00 &quot;</span> (16)</span><br><span class="line">  fid: parent=[0x200000bd2:0x40:0x0] stripe=0</span><br><span class="line">EXTENTS:</span><br><span class="line">(0-255):2393088-2393343</span><br><span class="line"></span><br><span class="line">$ lfs fid2path /opt/ [0x200000bd2:0x40:0x0]</span><br><span class="line">/opt/OST0003/rhel5-1</span><br></pre></td></tr></table></figure>

<h3 id="lfs-zfs-direct-IO-support"><a href="#lfs-zfs-direct-IO-support" class="headerlink" title="lfs zfs direct IO support"></a><a target="_blank" rel="noopener" href="https://lfs-discuss.lfs.narkive.com/S7kbvnG2/lfs-on-zfs-pooer-direct-i-o-performance">lfs zfs direct IO support</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">John, with newer Lfs clients it is possible <span class="keyword">for</span> multiple threads to submit non-overlapping writes concurrently (also not conflicting within a single page), see LU-1669 <span class="keyword">for</span> details.</span><br><span class="line"></span><br><span class="line">Even so, O_DIRECT writes need to be synchronous to disk on the OSS, as Patrick reports, because <span class="keyword">if</span> the OSS fails before the write is on disk there is no cached copy of the data on the client that can be used to resend the RPC.</span><br><span class="line"></span><br><span class="line">The problem is that the ZFS OSD has very long transaction commit <span class="built_in">times</span> <span class="keyword">for</span> synchronous writes because it does not yet have support <span class="keyword">for</span> the ZIL. Using buffered writes, or having very large O_DIRECT writes (e.g. 40MB or larger) and large RPCs (4MB, or up to 16MB <span class="keyword">in</span> 2.9.0) to amortize the sync overhead may be beneficial <span class="keyword">if</span> you really want to use O_DIRECT.</span><br></pre></td></tr></table></figure>

<h3 id="Got-the-client-uuid-map-nid"><a href="#Got-the-client-uuid-map-nid" class="headerlink" title="Got the client uuid map nid"></a>Got the client uuid map nid</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/fs/lfs/nodemap/default/exports</span><br><span class="line"><span class="comment"># too many....          </span></span><br><span class="line">&#123; nid: 192.168.11.1@tcp, uuid: b19f57a2-d206-41ea-0c1e-e13650c4de6d &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ost/OSS/ost/timeouts</span><br><span class="line">  <span class="built_in"> service </span>: cur  50  worst  92 (at 1616674042, 8947s ago)   3  18   1   3</span><br><span class="line">  <span class="built_in"> service </span>: cur  50  worst  50 (at 1616674036, 8953s ago)   1   1   1   1</span><br></pre></td></tr></table></figure>

<h3 id="lnet"><a href="#lnet" class="headerlink" title="lnet"></a><a target="_blank" rel="noopener" href="http://wiki.lfsfs.cn/index.php?title=LNet%E8%B7%AF%E7%94%B1%E5%99%A8%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97">lnet</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Servers:</span><br><span class="line">options lnet networks=<span class="string">&quot;o2ib1(ib0)&quot;</span> routes=<span class="string">&quot;o2ib2 10.10.0.20@o2ib1&quot;</span></span><br><span class="line">Routers:</span><br><span class="line">options lnet networks=<span class="string">&quot;o2ib1(ib0),o2ib2(ib1)&quot;</span> <span class="string">&quot;forwarding=enabled&quot;</span></span><br><span class="line">Clients:</span><br><span class="line">options lnet networks=<span class="string">&quot;o2ib2(ib0)&quot;</span> routes=<span class="string">&quot;o2ib1 10.20.0.29@o2ib2&quot;</span> </span><br><span class="line"></span><br><span class="line">Servers:</span><br><span class="line">lnetctl net add --net o2ib1 --<span class="keyword">if</span> ib0,ib1</span><br><span class="line">lnetctl route add --net o2ib2 --gateway 10.10.0.20@o2ib1</span><br><span class="line">lnetctl peer add --nid 10.10.0.20@o2ib1,10.10.0.21@o2ib1</span><br><span class="line"> </span><br><span class="line">Routers:</span><br><span class="line">lnetctl net add --net o2ib1 --<span class="keyword">if</span> ib0,ib1</span><br><span class="line">lnetctl net add --net o2ib2 --<span class="keyword">if</span> ib2,ib3</span><br><span class="line">lnetctl peer add --nid 10.10.0.1@o2ib1,10.10.0.2@o2ib1</span><br><span class="line">lnetctl peer add --nid 10.20.0.1@o2ib2,10.20.0.2@o2ib2</span><br><span class="line">lnetctl <span class="built_in">set</span> routing 1</span><br><span class="line">   </span><br><span class="line">Clients:</span><br><span class="line">lnetctl net add --net o2ib2 --<span class="keyword">if</span> ib0,ib1</span><br><span class="line">lnetctl route add --net o2ib1 --gateway 10.20.0.29@o2ib2</span><br><span class="line">lnetctl peer add --nid 10.20.0.29@o2ib2,10.20.0.30@o2ib2</span><br><span class="line"></span><br><span class="line">$ lnetctl <span class="built_in">export</span> FILE.yaml</span><br><span class="line">$ lnetctl <span class="built_in">export</span> &gt; FILE.yaml</span><br><span class="line">$ lnetctl import FILE.yaml</span><br><span class="line">$ lnetctl import &lt; FILE.yaml </span><br></pre></td></tr></table></figure>

<p>Parameters</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mark the route status, set 0 means disable</span></span><br><span class="line">options lnet auto_down=1 </span><br><span class="line"></span><br><span class="line"><span class="comment"># avoid_asym_router_failure, avoid push data to the block route cause data loss</span></span><br><span class="line">options lnet avoid_asym_router_failure=1 </span><br><span class="line"></span><br><span class="line"><span class="comment"># ping check the active route time interval, default: 60s</span></span><br><span class="line">options lnet live_router_check_interval=50  </span><br><span class="line"></span><br><span class="line"><span class="comment"># dead_router_check_interval, default:60s</span></span><br><span class="line">options lnet dead_router_check_interval=50 </span><br><span class="line"></span><br><span class="line"><span class="comment"># ping timeout</span></span><br><span class="line">options lnet router_ping_timeout=60</span><br><span class="line"></span><br><span class="line"><span class="comment"># check the route status from</span></span><br><span class="line">$ cat /sys/kernel/debug/lnet/stats</span><br><span class="line">0 23 0 49912 49912 0 0 18468672 27625832 0 0</span><br><span class="line"></span><br><span class="line">$ lnetctl stats show</span><br><span class="line">statistics:</span><br><span class="line">    msgs_alloc: 0</span><br><span class="line">    msgs_max: 23</span><br><span class="line">    rst_alloc: 0</span><br><span class="line">    errors: 0</span><br><span class="line">    send_count: 49912</span><br><span class="line">    resend_count: 0</span><br><span class="line">    response_timeout_count: 0</span><br><span class="line">    local_interrupt_count: 0</span><br><span class="line">    local_dropped_count: 0</span><br><span class="line">    local_aborted_count: 0</span><br><span class="line">    local_no_route_count: 0</span><br><span class="line">    local_timeout_count: 0</span><br><span class="line">    local_error_count: 0</span><br><span class="line">    remote_dropped_count: 0</span><br><span class="line">    remote_error_count: 0</span><br><span class="line">    remote_timeout_count: 0</span><br><span class="line">    network_timeout_count: 0</span><br><span class="line">    recv_count: 49912</span><br><span class="line">    route_count: 0</span><br><span class="line">    drop_count: 0</span><br><span class="line">    send_length: 18468672</span><br><span class="line">    recv_length: 27625832</span><br><span class="line">    route_length: 0</span><br><span class="line">    drop_length: 0</span><br></pre></td></tr></table></figure>

<p>OPA setting</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> ko2iblnd-opa ko2iblnd</span><br><span class="line">options ko2iblnd-opa peer_credits=128 peer_credits_hiw=64 credits=1024 concurrent_sends=256 ntx=2048 map_on_demand=32 fmr_pool_size=2048 fmr_flush_trigger=512 fmr_cache=1</span><br><span class="line"></span><br><span class="line">//The default value is 8, /sys/kernel/debug/lnet/peers, LNet使用peer_credits和network_interface_credits通过网络发送块大小固定为1MB的数据。peer_credits参数管理可以同时发送到单个对等节点的并行数量</span><br><span class="line">ko2iblnd-opa peer_credits=128  </span><br><span class="line"></span><br><span class="line">增加peer_credits的数量并不一定能获得良好的性能，因为在大型的配置中，增加数量会导致网络过载并增加OFED堆栈的内存利用率。可调参数network interface credits(credits)能够限制并行发送到单个网络的数量，并通过proc/sys/lnet/nis接口进行监控。可以通过特定lfs网络驱动程序(LND)的模块参数来增加network interface credits的数量:</span><br><span class="line">// The default value is 64 and is shared across all the CPU partitions (CPTs).</span><br><span class="line">ko2iblnd-opa credits=1024 </span><br><span class="line"></span><br><span class="line">// The default value is 0</span><br><span class="line">ko2iblnd-opa map_on_demand=32 </span><br></pre></td></tr></table></figure>

<h4 id="lnet-health"><a href="#lnet-health" class="headerlink" title="lnet health"></a><a target="_blank" rel="noopener" href="http://wiki.lfsfs.cn/index.php?title=LNet%E5%81%A5%E5%BA%B7%E5%8A%9F%E8%83%BD%E7%94%A8%E6%88%B7%E6%89%8B%E5%86%8C">lnet health</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the more value, the long recovery time, the default is disabled means lnet_health_sensitivity and lnet_retry_count set to 0</span></span><br><span class="line">$ lnetctl <span class="built_in">set</span> health_sensitivity: sensitivity to failure</span><br><span class="line">        0 - turn off health evaluation</span><br><span class="line">        &gt;0 - sensitivity value not more than 1000</span><br><span class="line"></span><br><span class="line">$ lnetctl <span class="built_in">set</span> recovery_interval: interval to ping unhealthy interfaces</span><br><span class="line">        &gt;0 - timeout <span class="keyword">in</span> seconds</span><br><span class="line"></span><br><span class="line">$ lnetctl <span class="built_in">set</span> retry_count: number of retries</span><br><span class="line">        0 - turn of retries</span><br><span class="line">        &gt;0 - number of retries</span><br><span class="line"></span><br><span class="line"><span class="comment">###Important</span></span><br><span class="line">$ lnetctl <span class="built_in">set</span> transaction_timeout: Message/Response timeout</span><br><span class="line">        &gt;0 - timeout <span class="keyword">in</span> seconds</span><br><span class="line"></span><br><span class="line">lnet_lnd_timeout = lnet_transaction_timeout / retry_count</span><br><span class="line"></span><br><span class="line"><span class="comment">### dump all config</span></span><br><span class="line">$ lnetctl global show</span><br><span class="line">global:</span><br><span class="line">    numa_range: 0</span><br><span class="line">    max_intf: 200</span><br><span class="line">    discovery: 1</span><br><span class="line">    drop_asym_route: 0</span><br><span class="line">    retry_count: 2</span><br><span class="line">    transaction_timeout: 50</span><br><span class="line">    health_sensitivity: 100</span><br><span class="line">    recovery_interval: 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># show local health</span></span><br><span class="line">$ lnetctl net show -v 3</span><br><span class="line"></span><br><span class="line"><span class="comment"># show remote health</span></span><br><span class="line">$ lnetctl peer show -v 3</span><br><span class="line"></span><br><span class="line"><span class="comment"># show status</span></span><br><span class="line">$ lnetctl stats show</span><br></pre></td></tr></table></figure>

<h3 id="ban-the-lfs-client"><a href="#ban-the-lfs-client" class="headerlink" title="ban the lfs client"></a>ban the lfs client</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;192.168.1.1@tcp1&#x27;</span> &gt; /proc/fs/lfs/obdfilter/fsname-OST0000/evict_client</span><br><span class="line"></span><br><span class="line"><span class="comment"># MDS</span></span><br><span class="line">/proc/fs/lfs/mdt/lfs1-MDT0000/evict_client</span><br><span class="line">/proc/fs/lfs/mgs/MGS/evict_client</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="rocev2-test"><a href="#rocev2-test" class="headerlink" title="rocev2 test"></a>rocev2 test</h3><p><a target="_blank" rel="noopener" href="http://blog.sysu.tech/RoCE/OpenMPI%E4%BD%BF%E7%94%A8RoCEv2%E7%9A%84%E6%96%B9%E6%B3%95/">http://blog.sysu.tech/RoCE/OpenMPI%E4%BD%BF%E7%94%A8RoCEv2%E7%9A%84%E6%96%B9%E6%B3%95/</a><br><a target="_blank" rel="noopener" href="https://wiki.whamcloud.com/display/LNet/Setting+Type+of+Service+%28ToS%29+from+o2iblnd">https://wiki.whamcloud.com/display/LNet/Setting+Type+of+Service+%28ToS%29+from+o2iblnd</a><br><a target="_blank" rel="noopener" href="https://download.lenovo.com/servers/mig/2020/05/27/21909/mlnx-lnvgy_dd_nic_cx.ib-5.0-2.1.8.0-0_sles12_x86-64.pdf">https://download.lenovo.com/servers/mig/2020/05/27/21909/mlnx-lnvgy_dd_nic_cx.ib-5.0-2.1.8.0-0_sles12_x86-64.pdf</a><br><a target="_blank" rel="noopener" href="http://wiki.lustrefs.cn/index.php?title=%E4%BB%8Eo2iblnd%E8%AE%BE%E7%BD%AE%E6%9C%8D%E5%8A%A1%E7%B1%BB%E5%9E%8B(ToS)">http://wiki.lustrefs.cn/index.php?title=%E4%BB%8Eo2iblnd%E8%AE%BE%E7%BD%AE%E6%9C%8D%E5%8A%A1%E7%B1%BB%E5%9E%8B(ToS)</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/burningTheStar/p/8563347.html">https://www.cnblogs.com/burningTheStar/p/8563347.html</a><br><a target="_blank" rel="noopener" href="https://community.mellanox.com/s/article/roce-configuration-for-linux-drivers-in-dscp-based-qos-mode">https://community.mellanox.com/s/article/roce-configuration-for-linux-drivers-in-dscp-based-qos-mode</a><br><a target="_blank" rel="noopener" href="https://community.mellanox.com/s/article/howto-set-egress-tos-dscp-on-rdma-cm-qps">https://community.mellanox.com/s/article/howto-set-egress-tos-dscp-on-rdma-cm-qps</a><br><a target="_blank" rel="noopener" href="https://community.mellanox.com/s/article/howto-setup-rdma-connection-using-inbox-driver--rhel--ubuntu-x">https://community.mellanox.com/s/article/howto-setup-rdma-connection-using-inbox-driver--rhel--ubuntu-x</a><br><a target="_blank" rel="noopener" href="https://community.mellanox.com/s/article/howto-configure-resilient-roce-end-to-end-using-connectx-4-and-spectrum--no-qos-x">https://community.mellanox.com/s/article/howto-configure-resilient-roce-end-to-end-using-connectx-4-and-spectrum--no-qos-x</a><br><a target="_blank" rel="noopener" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_infiniband_and_rdma_networks/index">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_infiniband_and_rdma_networks/index</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">options lnet networks=o2ib0(p4p1)</span><br></pre></td></tr></table></figure>

<h3 id="lfs-magic-num"><a href="#lfs-magic-num" class="headerlink" title="lfs magic num"></a>lfs magic num</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">magic: 0xa0629d03 </span><br></pre></td></tr></table></figure>

<h3 id="build-lfs"><a href="#build-lfs" class="headerlink" title="build lfs"></a><a target="_blank" rel="noopener" href="http://hmli.ustc.edu.cn/doc/linux/lustre/">build lfs</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">$ rpm -ivh http://download.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-9.noarch.rpm</span><br><span class="line">$ yum -y groupinstall <span class="string">&quot;Development Tools&quot;</span></span><br><span class="line">$ yum -y install automake xmlto asciidoc elfutils-libelf-devel zlib-devel binutils-devel newt-devel python-devel libyaml-devel hmaccalc perl-ExtUtils-Embed rpm-build make gcc redhat-rpm-config patchutils git libtool net-tools elfutils-devel bison audit-libs-devel  pesign numactl-devel pciutils-devel ncurses-devel libselinux-devel</span><br><span class="line">$ useradd -m build</span><br><span class="line">$ su - build</span><br><span class="line">$ git <span class="built_in">clone</span> git://git.hpdd.intel.com/fs/lustre-release.git</span><br><span class="line">$ <span class="built_in">cd</span> lustre-release; sh autogen.sh </span><br><span class="line">$ mkdir -p ~/kernel/rpmbuild/&#123;BUILD,RPMS,SOURCES,SPECS,SRPMS&#125;</span><br><span class="line">$ <span class="built_in">cd</span> ~/kernel</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;%_topdir %(echo $HOME)/kernel/rpmbuild&#x27;</span> &gt; ~/.rpmmacros</span><br><span class="line">$ rpm -ivh http://vault.centos.org/7.3.1611/updates/Source/SPackages/kernel-3.10.0-514.16.1.el7.src.rpm</span><br><span class="line">$ <span class="built_in">cd</span> ~/kernel/rpmbuild</span><br><span class="line">$ rpmbuild -bp --target=`uname -m` ./SPECS/kernel.spec</span><br><span class="line">$ rm -f ~/lustre-kernel-x86_64-lustre.patch</span><br><span class="line">$ <span class="built_in">cd</span> ~/lustre-release/lustre/kernel_patches/series</span><br><span class="line">$ <span class="keyword">for</span> patch <span class="keyword">in</span> $(&lt;<span class="string">&quot;3.10-rhel7.series&quot;</span>); <span class="keyword">do</span> \</span><br><span class="line">    patch_file=<span class="string">&quot;<span class="variable">$HOME</span>/lustre-release/lustre/kernel_patches/patches/<span class="variable">$&#123;patch&#125;</span>&quot;</span>;\</span><br><span class="line">    cat <span class="string">&quot;<span class="variable">$&#123;patch_file&#125;</span>&quot;</span> &gt;&gt; <span class="variable">$HOME</span>/lustre-kernel-x86_64-lustre.patch; \</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">$ cp ~/lustre-kernel-x86_64-lustre.patch ~/kernel/rpmbuild/SOURCES/patch-3.10.0-lustre.patch</span><br><span class="line">$ vim ~/kernel/rpmbuild/SPECS/kernel.spec</span><br><span class="line"></span><br><span class="line"><span class="comment"># insert the two lines under &quot;&#x27;find $RPM_BUILD_ROOT/lib/modules/$KernelVer&#x27;&quot;</span></span><br><span class="line">cp -a fs/ext3/* <span class="variable">$RPM_BUILD_ROOT</span>/lib/modules/<span class="variable">$KernelVer</span>/build/fs/ext3 </span><br><span class="line">cp -a fs/ext4/* <span class="variable">$RPM_BUILD_ROOT</span>/lib/modules/<span class="variable">$KernelVer</span>/build/fs/ext4</span><br><span class="line"></span><br><span class="line"><span class="comment"># insert the two lines under &quot;# empty final patch to facilitate testing of kernel patches&quot;</span></span><br><span class="line"><span class="comment"># adds lfs patches</span></span><br><span class="line">Patch99995: patch-%&#123;version&#125;-lustre.patch</span><br><span class="line"></span><br><span class="line"><span class="comment"># insert the two lines under &quot;&#x27;ApplyOptionalPatch linux-kernel-test.patch&#x27;&quot;</span></span><br><span class="line"><span class="comment"># lustre patch</span></span><br><span class="line">ApplyOptionalPatch patch-%&#123;version&#125;-lustre.patch</span><br><span class="line"></span><br><span class="line"><span class="comment"># change the 0 to 1</span></span><br><span class="line">%define listnewconfig_fail 1 ---&gt; %define listnewconfig_fail 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># modify the line, to disable kABI check</span></span><br><span class="line">%define with_kabichk <span class="comment">#%&#123;?_without_kabichk: 0&#125; %&#123;?!_without_kabichk: 1&#125;</span></span><br><span class="line">to</span><br><span class="line">%define with_kabichk 0 </span><br><span class="line"><span class="comment">#%&#123;?_without_kabichk:   0&#125; %&#123;?!_without_kabichk:   1&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># replace the config file</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;# x86_64&#x27;</span> &gt; ~/kernel/rpmbuild/SOURCES/kernel-3.10.0-x86_64.config</span><br><span class="line">$ cat ~/lustre-release/lustre/kernel_patches/kernel_configs/kernel-3.10.0-3.10-rhel7-x86_64.config &gt;&gt; ~/kernel/rpmbuild/SOURCES/kernel-3.10.0-x86_64.config</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> ~/kernel/rpmbuild</span><br><span class="line">buildid=<span class="string">&quot;_lustre&quot;</span> <span class="comment"># Note: change to any string that identify your work</span></span><br><span class="line">rpmbuild -ba --with firmware --target x86_64 --with baseonly \</span><br><span class="line">         --define <span class="string">&quot;buildid <span class="variable">$&#123;buildid&#125;</span>&quot;</span> \</span><br><span class="line">         ~/kernel/rpmbuild/SPECS/kernel.spec</span><br><span class="line"></span><br><span class="line">$ yum localinstall -y /home/build/kernel/rpmbuild/RPMS/x86_64/&#123;kernel,kernel-devel&#125;-3.10.0-514.16.1.el7_lustre.x86_64.rpm</span><br><span class="line">$ reboot</span><br><span class="line">$ <span class="built_in">cd</span> ~/lustre-release/</span><br><span class="line">$ ./configure --enable-quota</span><br><span class="line"></span><br><span class="line">IB:</span><br><span class="line">$ ./configure --with-o2ib=/usr/src/ofa_kernel/default</span><br><span class="line"></span><br><span class="line">OPA:</span><br><span class="line">$ ./configure --with-o2ib=yes</span><br><span class="line"></span><br><span class="line">$ make rpms</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; /etc/yum.repos.d/e2fsprogs.repo</span><br><span class="line">[e2fsprogs-el7-x86_64]</span><br><span class="line">name=e2fsprogs-el7-x86_64</span><br><span class="line">baseurl=https://downloads.hpdd.intel.com/public/e2fsprogs/latest/el7/</span><br><span class="line">enabled=1</span><br><span class="line">priority=1</span><br><span class="line">EOF</span><br><span class="line">  </span><br><span class="line">$ yum update e2fsprogs</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> ~build/lustre-release/</span><br><span class="line">$ yum -y localinstall &#123;kmod-lustre-osd-ldiskfs,kmod-lustre,lustre,lustre-osd-ldiskfs-mount,lustre-iokit,lustre-tests,kmod-lustre-tests&#125;-2.9.55_45_g04fb37c-1.el7.centos.x86_64.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment">## disable selinux</span></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;options lnet networks=tcp0(eth0)&#x27;</span> &gt; /etc/modprobe.d/lustre.conf</span><br><span class="line">$ depmod -a</span><br><span class="line">$ modprobe lustre</span><br><span class="line">$ systemctl restart lustre</span><br><span class="line"></span><br><span class="line"><span class="comment"># test</span></span><br><span class="line">$ /usr/lib64/lustre/tests/llmount.sh</span><br></pre></td></tr></table></figure>

<h3 id="sub-cmd"><a href="#sub-cmd" class="headerlink" title="sub cmd"></a>sub cmd</h3><h4 id="llog-reader"><a href="#llog-reader" class="headerlink" title="llog_reader"></a>llog_reader</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ mount -t ldiskfs /dev/sda /mnt/mgs</span><br><span class="line">$ llog_reader /mnt/mgs/CONFIGS/tfs-client</span><br><span class="line"></span><br><span class="line">$ debugfs -c -R <span class="string">&#x27;dump CONFIGS/tfs-client /tmp/tfs-client&#x27;</span> /dev/sda</span><br><span class="line">$ llog_reader /tmp/tfs-client</span><br></pre></td></tr></table></figure>

<h3 id="SPEC"><a href="#SPEC" class="headerlink" title="SPEC"></a>SPEC</h3><h4 id="Ethernet"><a href="#Ethernet" class="headerlink" title="Ethernet"></a>Ethernet</h4><ul>
<li>2.12.8, single OSS, 1x 40GbE, 86 HDD = 43x mirrors<ul>
<li>write<ul>
<li>single client 1.7GB/s write</li>
<li>3 x clients 3~3.3GB/s ,1x 1x40GbE, 2x 2x10GbE</li>
</ul>
</li>
<li>read</li>
<li>rw</li>
<li>randread</li>
<li>randwrite</li>
<li>randrw<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Average</span>:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s</span><br><span class="line"><span class="attribute">Average</span>:      ens<span class="number">17</span>f<span class="number">0</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span></span><br><span class="line"><span class="attribute">Average</span>:      ens<span class="number">17</span>f<span class="number">1</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span></span><br><span class="line"><span class="attribute">Average</span>:           lo      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span></span><br><span class="line"><span class="attribute">Average</span>:       ens<span class="number">5</span>f<span class="number">0</span> <span class="number">2201898</span>.<span class="number">21</span>  <span class="number">53342</span>.<span class="number">64</span> <span class="number">3240923</span>.<span class="number">01</span>   <span class="number">5305</span>.<span class="number">02</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">54</span></span><br><span class="line"><span class="attribute">Average</span>:       ens<span class="number">5</span>f<span class="number">1</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">Average</span>:     atmptf/s  estres/s retrans/s isegerr/s   orsts/s</span><br><span class="line"><span class="attribute">Average</span>:         <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">62</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## there are some network jitter</span></span><br><span class="line"><span class="attribute">Average</span>:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s</span><br><span class="line"><span class="attribute">Average</span>:      ens<span class="number">17</span>f<span class="number">0</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span></span><br><span class="line"><span class="attribute">Average</span>:      ens<span class="number">17</span>f<span class="number">1</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span></span><br><span class="line"><span class="attribute">Average</span>:           lo      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span></span><br><span class="line"><span class="attribute">Average</span>:       ens<span class="number">5</span>f<span class="number">0</span>  <span class="number">90144</span>.<span class="number">27</span> <span class="number">2128780</span>.<span class="number">95</span>   <span class="number">7979</span>.<span class="number">43</span> <span class="number">3140205</span>.<span class="number">66</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">53</span></span><br><span class="line"><span class="attribute">Average</span>:       ens<span class="number">5</span>f<span class="number">1</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">Average</span>:     atmptf/s  estres/s retrans/s isegerr/s   orsts/s</span><br><span class="line"><span class="attribute">Average</span>:         <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>     <span class="number">88</span>.<span class="number">41</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">Average</span>:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s</span><br><span class="line"><span class="attribute">Average</span>:      ens<span class="number">17</span>f<span class="number">0</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span></span><br><span class="line"><span class="attribute">Average</span>:      ens<span class="number">17</span>f<span class="number">1</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span></span><br><span class="line"><span class="attribute">Average</span>:           lo      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span></span><br><span class="line"><span class="attribute">Average</span>:       ens<span class="number">5</span>f<span class="number">0</span> <span class="number">2020613</span>.<span class="number">85</span> <span class="number">1988556</span>.<span class="number">99</span> <span class="number">2864642</span>.<span class="number">04</span> <span class="number">2866199</span>.<span class="number">64</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">54</span></span><br><span class="line"><span class="attribute">Average</span>:       ens<span class="number">5</span>f<span class="number">1</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">Average</span>:     atmptf/s  estres/s retrans/s isegerr/s   orsts/s</span><br><span class="line"><span class="attribute">Average</span>:         <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span>     <span class="number">41</span>.<span class="number">72</span>      <span class="number">0</span>.<span class="number">00</span>      <span class="number">0</span>.<span class="number">00</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<p>compress lz4 with zero</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">Average:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s</span><br><span class="line">Average:      ens17f0      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">Average:      ens17f1      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">Average:           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">Average:       ens5f0 2866428.55  69583.06 4230439.68   6926.46      0.00      0.00      0.53</span><br><span class="line">Average:       ens5f1      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">Average:     atmptf/s  estres/s retrans/s isegerr/s   orsts/s</span><br><span class="line">Average:         0.00      0.00      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">Average:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s</span><br><span class="line">Average:      ens17f0      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">Average:      ens17f1      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">Average:           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">Average:       ens5f0 150131.37 3217785.25  12902.95 4745508.62      0.00      0.00      0.53</span><br><span class="line">Average:       ens5f1      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">Average:     atmptf/s  estres/s retrans/s isegerr/s   orsts/s</span><br><span class="line">Average:         0.00      0.00     76.34      0.00      0.00</span><br><span class="line"></span><br><span class="line">	Speed: 40000Mb/s</span><br><span class="line">	Duplex: Full</span><br><span class="line">	Port: Direct Attach Copper</span><br><span class="line"></span><br><span class="line">  36.44%  [zfs]                 [k] zio_compress_zeroed_cb</span><br><span class="line">   8.57%  [kernel]              [k] memcpy</span><br><span class="line">   5.22%  [kernel]              [k] copy_user_enhanced_fast_string</span><br><span class="line">   4.08%  [kernel]              [k] memset</span><br><span class="line">   2.26%  [kernel]              [k] native_queued_spin_lock_slowpath</span><br><span class="line">   1.98%  [kernel]              [k] do_csum</span><br><span class="line">   1.42%  [mlx5_core]           [k] mlx5e_skb_from_cqe_mpwrq_linear</span><br><span class="line">   1.26%  [kernel]              [k] __check_object_size</span><br><span class="line">   1.21%  [kernel]              [k] memcpy_toiovec</span><br><span class="line">   0.97%  [kernel]              [k] _raw_spin_lock_irqsave</span><br><span class="line">   0.93%  [kernel]              [k] __domain_mapping</span><br><span class="line">   0.70%  [kernel]              [k] _raw_spin_lock</span><br><span class="line">   0.70%  [kernel]              [k] tcp_gro_receive</span><br><span class="line">   0.68%  [kernel]              [k] dma_pte_clear_level</span><br><span class="line">   0.58%  [kernel]              [k] math_state_restore</span><br><span class="line">   0.58%  [kernel]              [k] __list_del_entry</span><br><span class="line">   0.56%  [kernel]              [k] tcp_sendpage</span><br><span class="line">   0.51%  [kernel]              [k] queue_iova</span><br><span class="line">   0.49%  [kernel]              [k] free_pages_prepare</span><br><span class="line">   0.45%  [kernel]              [k] put_page</span><br><span class="line">   0.43%  [kernel]              [k] skb_copy_datagram_iovec</span><br><span class="line"></span><br><span class="line">Average:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s</span><br><span class="line">Average:      ens17f0      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">Average:      ens17f1      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">Average:           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">Average:       ens5f0 2939028.32 2883303.26 4160534.95 4158478.74      0.00      0.00      0.54</span><br><span class="line">Average:       ens5f1      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">Average:     atmptf/s  estres/s retrans/s isegerr/s   orsts/s</span><br><span class="line">Average:         0.00      0.00    214.56      0.00      0.00</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Absolutely, The ethernet switch or link has the issue, when the 1x OSS work with 4x clients, when add a new client stress(one by one) the tcp retrans has the jitter, just a few seconds.</p>
<h3 id="Backup-and-recovery-lfs-ZFS-OST"><a href="#Backup-and-recovery-lfs-ZFS-OST" class="headerlink" title="Backup and recovery lfs ZFS OST"></a>Backup and recovery lfs ZFS OST</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">OSS $ umount <span class="regexp">/lustre/</span>test_ost0</span><br><span class="line">OSS $ zfs set canmount=on test_ost0/test_ost0 </span><br><span class="line">OSS $ zfs mount -a</span><br><span class="line">OSS $ ls <span class="regexp">/test_ost0/</span>test_ost0</span><br><span class="line">CONFIGS         nodemap  oi.<span class="number">103</span>  oi.<span class="number">110</span>  oi.<span class="number">118</span>  oi.<span class="number">125</span>  oi.<span class="number">18</span>  oi.<span class="number">25</span>  oi.<span class="number">32</span>  oi.<span class="number">4</span>   oi.<span class="number">47</span>  oi.<span class="number">54</span>  oi.<span class="number">61</span>  oi.<span class="number">69</span>  oi.<span class="number">76</span>  oi.<span class="number">83</span>  oi.<span class="number">90</span>  oi.<span class="number">98</span></span><br><span class="line">......</span><br><span class="line">OSS $ tar cvf <span class="regexp">/backup/</span>test_ost0.tar --xattrs-include=<span class="string">&quot;trusted.*&quot;</span> --sparse . </span><br><span class="line">./</span><br><span class="line">./fld</span><br><span class="line">.<span class="regexp">/oi.46/</span></span><br><span class="line">.<span class="regexp">/oi.31/</span></span><br><span class="line">.<span class="regexp">/oi.114/</span></span><br><span class="line">.<span class="regexp">/quota_slave/</span></span><br><span class="line">.<span class="regexp">/quota_slave/</span><span class="number">0</span>x20000-OST0000</span><br><span class="line">......</span><br><span class="line">.<span class="regexp">/oi.92/</span></span><br><span class="line">.<span class="regexp">/oi.76/</span></span><br><span class="line">.<span class="regexp">/oi.98/</span></span><br><span class="line">tar: Exiting with failure status due to previous errors</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">OSS $ cd ~</span><br><span class="line">OSS $ umount <span class="regexp">/test_ost0/</span>test_ost0/</span><br><span class="line">OSS $ mkfs.lfs --reformat --backfstype=zfs --ost  --index=<span class="number">0</span>  --fsname=testfs1 --servicenode=<span class="variable">$&#123;oss_ipaddr&#125;</span>@tcp --mgsnode=<span class="variable">$&#123;mds_ipaddr&#125;</span>@tcp  test_ost0/test_ost0</span><br><span class="line">OSS $ zfs set canmount=on test_ost0/test_ost0 </span><br><span class="line">OSS $ zfs mount -a</span><br><span class="line">OSS $ cd <span class="regexp">/test_ost0/</span>test_ost0</span><br><span class="line">OSS $ tar xvpf <span class="regexp">/backup/</span>test_ost0.tar --xattrs-include=<span class="string">&quot;trusted.*&quot;</span> --sparse</span><br><span class="line">OSS $ rm -rf oi.<span class="number">16</span>* lfsck_* LFSCK <span class="comment">##zfs could not delete oi.16</span></span><br><span class="line">OSS $ rm -f CATALOGS </span><br><span class="line">OSS $ getfattr -n trusted.lma fld </span><br><span class="line"><span class="comment"># file: fld</span></span><br><span class="line">trusted.lma=<span class="number">0</span>sAAAAAAAAAAABAAAAAgAAAAMAAAAAAAAA</span><br><span class="line"></span><br><span class="line">OSS $ rm -f fld</span><br><span class="line">OSS $ umount <span class="regexp">/test_ost0/</span>test_ost0</span><br><span class="line"></span><br><span class="line">writeconf all OST and MDT</span><br><span class="line">patch https:<span class="regexp">//</span>review.whamcloud.com<span class="regexp">/#/</span>c<span class="regexp">/46723/</span></span><br><span class="line"></span><br><span class="line">OSS $ lustre_rmmod</span><br><span class="line">OSS $ mount.lfs test_ost0<span class="regexp">/test_ost0 /</span>lfs/test_ost0</span><br></pre></td></tr></table></figure>

<h3 id="lustre-multiple-ethernet-port-for-diff-LAN"><a href="#lustre-multiple-ethernet-port-for-diff-LAN" class="headerlink" title="lustre multiple ethernet port for diff LAN"></a>lustre multiple ethernet port for diff LAN</h3><p>just modify lnet driver parameters in server and client</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">options lnet networks=tcp0(eth0),tcp1(ens12) <span class="comment">## in server and client</span></span><br><span class="line"></span><br><span class="line">client $ mount.lustre 192.168.0.1@tcp0:/lfs /mnt</span><br><span class="line">client $ mount.lustre 10.0.0.1@tcp1:/lfs /mnt</span><br><span class="line"></span><br><span class="line">[Fri Mar 25 15:55:47 2022] lfs: lfs-OST0000: Connection restored to ed9931e3-3560-4e6c-0914-9e6e65e6bfc0 (at 10.0.0.1@tcp)</span><br><span class="line">[Fri Mar 25 16:00:08 2022] lfs: lfs-OST0000: Connection restored to ed9931e3-3560-4e6c-0914-9e6e65e6bfc0 (at 10.0.0.1@tcp)</span><br><span class="line"></span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### trace the kernel sock </span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>./lnet/include/lnet/lib-types.h<br>        int                             ln_niinit_self;<br>        /* LNetNIInit/LNetNIFini counter <em>/<br>        int                             ln_refcount;<br>        /</em> SHUTDOWN/RUNNING/STOPPING */</p>
<p>$ lctl –net tcp del_peer $ipaddr<br>lnet/peer.c<br> LNetPut<br>  ksocknal_alloc_tx<br>   ksocknal_launch_packet</p>
<p>$ ss -tulpen<br>udp    UNCONN     0      0                                                             [::]:111                                                                       [::]:*                   users:((“rpcbind”,pid=936,fd=9)) ino:16271 sk:ffffa0b3b54a0000 v6only:1 &lt;-&gt;</p>
<p>tcp    LISTEN     0      127                                                              *:988                                                                          <em>:</em>                   ino:389210 sk:ffffa0b43b7e2e80 &lt;-&gt;</p>
<p>tcp    LISTEN     0      128                                                              *:111                                                                          <em>:</em>                   users:((“rpcbind”,pid=936,fd=8)) ino:16270 sk:ffffa0b3b5548000 &lt;-&gt;</p>
<p>$ crash /usr/lib/debug/usr/lib/modules/3.10.0-1160.el7.x86_64/vmlinux</p>
<p>crash&gt; struct sock.__sk_common.skc_num ffffa0b43b7e2e80<br>  __sk_common.skc_num = 988</p>
<p>crash&gt; struct sock.sk_rcvbuf ffffa0b43b7e2e80<br>  sk_rcvbuf = 134217728</p>
<p>crash&gt; struct sock.sk_sndbuf ffffa0b43b7e2e80<br>sk_sndbuf = 134217728<br>net.ipv4.tcp_rmem = 65536    134217728    268435456<br>net.ipv4.tcp_wmem = 65536    134217728    268435456</p>
<p>struct sock.sk_max_ack_backlog ffffa0b43b7e2e80<br>  sk_max_ack_backlog = 127</p>
<p>crash&gt; struct sock.sk_max_ack_backlog ffffa0b3b5548000<br>  sk_max_ack_backlog = 128</p>
<p>crash&gt; struct sock.__sk_common.skc_num ffffa0b3b54a0000<br>  __sk_common.skc_num = 111<br>crash&gt; struct sock.sk_rcvbuf ffffa0b3b54a0000<br>  sk_rcvbuf = 212992<br>crash&gt; struct sock.sk_rcvbuf ffffa0b3b54a0000<br>  sk_rcvbuf = 212992<br>net.core.rmem_default = 212992<br>net.core.rmem_max = 212992<br>net.core.wmem_default = 212992<br>net.core.wmem_max = 212992</p>
<p>#looks like the ss show the struct sock address</p>
<h1 id="proc-net-tcp"><a href="#proc-net-tcp" class="headerlink" title="/proc/net/tcp"></a>/proc/net/tcp</h1><p>netstat -wtpeav</p>
<pre><code></code></pre>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Homer</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2019/12/29/lfs_cmd/">http://yoursite.com/2019/12/29/lfs_cmd/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/lfs/">lfs</a></div><div class="post_share"><div class="social-share" data-image="https://homerl.github.io/img/hard-disk_223cf.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/03/03/gdb/"><img class="prev-cover" src="https://homerl.github.io/img/PROPELLER-2-1-1024x1024.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">GDB tips</div></div></a></div><div class="next-post pull-right"><a href="/2019/11/26/smartctl/"><img class="next-cover" src="https://homerl.github.io/img/hard-disk_223cf.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">smartctl</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2015/11/06/deploy_lfs_and_zfs/" title="Deploy Lfs and OpenZFS"><img class="cover" src="https://homerl.github.io/img/the_last_word_fs.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2015-11-06</div><div class="title">Deploy Lfs and OpenZFS</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></article></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2015 - 2022 By Homer</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk({
      clientID: '0b9d5b98d0a972b33cf7',
      clientSecret: '769efc11b32f6c0d03bcbf3ee800dfc4e2690459',
      repo: 'homerl.github.io',
      owner: 'homerl',
      admin: ['homerl'],
      id: '59c52e8208158dea8f819fbd16f217fb',
      language: 'en',
      perPage: 10,
      distractionFreeMode: false,
      pagerDirection: 'last',
      createIssueManually: false,
      updateCountCallback: commentCount
    })
    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    $.getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js', initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>