<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>openzfs tips | 己不由心，身又岂能由己</title><meta name="keywords" content="scsi,zfs"><meta name="author" content="Ginger"><meta name="copyright" content="Ginger"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="Why ZFS Transaction file system(Data consistency guaranteed) Manages writing of data with copy-on-write method Does not overwrite original data Commits or ignores sequential processing completely No d">
<meta property="og:type" content="article">
<meta property="og:title" content="openzfs tips">
<meta property="og:url" content="http://yoursite.com/2016/12/04/openzfs-tips/index.html">
<meta property="og:site_name" content="己不由心，身又岂能由己">
<meta property="og:description" content="Why ZFS Transaction file system(Data consistency guaranteed) Manages writing of data with copy-on-write method Does not overwrite original data Commits or ignores sequential processing completely No d">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/img/photo_by_spacex.jpg">
<meta property="article:published_time" content="2016-12-04T07:06:13.000Z">
<meta property="article:modified_time" content="2020-09-29T15:22:06.000Z">
<meta property="article:author" content="Ginger">
<meta property="article:tag" content="scsi">
<meta property="article:tag" content="zfs">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/img/photo_by_spacex.jpg"><link rel="shortcut icon" href="/img/stout-shield.png"><link rel="canonical" href="http://yoursite.com/2016/12/04/openzfs-tips/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.2.0',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-09-29 23:22:06'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><meta name="generator" content="Hexo 5.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/244247-guts.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">60</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">66</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">23</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Why-ZFS"><span class="toc-number">1.</span> <span class="toc-text">Why ZFS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#How-many-devices-in-single-raidz-zpool"><span class="toc-number">2.</span> <span class="toc-text">How many devices in single raidz zpool</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NVME-SSD-test-with-openzfs-0-7"><span class="toc-number">3.</span> <span class="toc-text">NVME SSD test with openzfs 0.7</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6x-Intel-P3520"><span class="toc-number">4.</span> <span class="toc-text">[6x Intel P3520]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-old-optimize-with-lfs"><span class="toc-number">5.</span> <span class="toc-text">the old optimize with lfs</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#zfs-txg-timeout"><span class="toc-number">5.1.</span> <span class="toc-text">zfs_txg_timeout</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#L2ARC-and-slog"><span class="toc-number">5.2.</span> <span class="toc-text">L2ARC and slog</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#max-write-speed-to-l2arc"><span class="toc-number"></span> <span class="toc-text">max write speed to l2arc</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#tradeoff-between-write-read-and-durability-of-ssd"><span class="toc-number"></span> <span class="toc-text">tradeoff between write&#x2F;read and durability of ssd (?)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#default-8-1024-1024-8-MB-s"><span class="toc-number"></span> <span class="toc-text">default : 8 * 1024 * 1024  &#x3D; 8 MB&#x2F;s</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#setting-here-500-1024-1024"><span class="toc-number"></span> <span class="toc-text">setting here : 500 * 1024 * 1024</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#number-of-max-device-writes-to-precache"><span class="toc-number"></span> <span class="toc-text">number of max device writes to precache</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#default-2"><span class="toc-number"></span> <span class="toc-text">default : 2</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#default-32768"><span class="toc-number"></span> <span class="toc-text">default : 32768</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#default-1024-1024-1mb"><span class="toc-number"></span> <span class="toc-text">default : 1024*1024 &#x3D; 1mb</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#zfs-vdev-scheduler"><span class="toc-number">0.1.</span> <span class="toc-text">zfs_vdev_scheduler</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zfs-read-chunk-size"><span class="toc-number">0.2.</span> <span class="toc-text">zfs_read_chunk_size</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zfs-max-recordsize"><span class="toc-number">0.3.</span> <span class="toc-text">zfs_max_recordsize</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zfs-vdev-aggregation-limit"><span class="toc-number">0.4.</span> <span class="toc-text">zfs_vdev_aggregation_limit</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDD-prefetch"><span class="toc-number">0.5.</span> <span class="toc-text">HDD prefetch</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Vdev-cache"><span class="toc-number">0.6.</span> <span class="toc-text">Vdev cache</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#txg-timeout"><span class="toc-number">0.7.</span> <span class="toc-text">txg_timeout</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zpool-sync-mode"><span class="toc-number">0.8.</span> <span class="toc-text">zpool sync mode</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Limit-memory-usage"><span class="toc-number">0.9.</span> <span class="toc-text">Limit memory usage</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zfs-multihost-history"><span class="toc-number">0.10.</span> <span class="toc-text">zfs_multihost_history</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zpool-could-not-mount"><span class="toc-number">1.</span> <span class="toc-text">[zpool could not mount]</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Try-zpool-T-parameter"><span class="toc-number">1.1.</span> <span class="toc-text">Try zpool -T parameter</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Add-skip-error-by-some-parameters-in-ZOL-0-8"><span class="toc-number">1.2.</span> <span class="toc-text">Add skip error by some parameters in ZOL 0.8</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zfs-vol"><span class="toc-number">1.3.</span> <span class="toc-text">zfs vol</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-event"><span class="toc-number">2.</span> <span class="toc-text">zfs event</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Compression"><span class="toc-number">3.</span> <span class="toc-text">Compression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Xattr"><span class="toc-number">4.</span> <span class="toc-text">Xattr</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ACL"><span class="toc-number">5.</span> <span class="toc-text">ACL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Rmount"><span class="toc-number">6.</span> <span class="toc-text">Rmount</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Zpool-rename"><span class="toc-number">7.</span> <span class="toc-text">Zpool rename</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Detach-hdd-from-mirror"><span class="toc-number">8.</span> <span class="toc-text">Detach hdd from mirror</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attach-one-hdd-to-mirror"><span class="toc-number">9.</span> <span class="toc-text">Attach one hdd to mirror</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sub-zpool"><span class="toc-number">10.</span> <span class="toc-text">sub zpool</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Quota"><span class="toc-number">11.</span> <span class="toc-text">Quota</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#user-quota"><span class="toc-number">12.</span> <span class="toc-text">user quota</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Add-and-remove-hotspare"><span class="toc-number">13.</span> <span class="toc-text">Add and remove hotspare</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Clear-zfs-label"><span class="toc-number">14.</span> <span class="toc-text">Clear zfs label</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Auto-rebuild-in-0-7-x"><span class="toc-number">15.</span> <span class="toc-text">Auto rebuild in 0.7.x</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Show-status"><span class="toc-number">16.</span> <span class="toc-text">Show status</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Custom-script-show-temperature"><span class="toc-number">17.</span> <span class="toc-text">Custom script ,show temperature</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#performance-tuning"><span class="toc-number">18.</span> <span class="toc-text">performance tuning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#check-your-zpool-could-be-imported"><span class="toc-number">19.</span> <span class="toc-text">check your zpool could be imported</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zpool-status"><span class="toc-number">20.</span> <span class="toc-text">zpool status</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Disable-AVX512-for-scalable-Xeon-silver-and-gold-5xxx"><span class="toc-number">21.</span> <span class="toc-text">Disable AVX512 for scalable Xeon silver and gold 5xxx</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#not-make-sure"><span class="toc-number"></span> <span class="toc-text">not make sure</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Too-bad-about-zfs-dracut"><span class="toc-number">1.</span> <span class="toc-text">Too bad about zfs-dracut</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ZFS-0-7-x-MMP-cause-too-many-issues-the-ZFS-0-6-5-more-stable"><span class="toc-number">2.</span> <span class="toc-text">ZFS 0.7.x MMP cause too many issues, the ZFS 0.6.5 more stable</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Resolved-import-too-slow"><span class="toc-number">3.</span> <span class="toc-text">Resolved import too slow</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Enable-zfs-message-log"><span class="toc-number">4.</span> <span class="toc-text">Enable zfs message log</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Remove-slog"><span class="toc-number">5.</span> <span class="toc-text">Remove slog</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#About-SMR-HDD"><span class="toc-number">6.</span> <span class="toc-text">About SMR HDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#About-silent-error"><span class="toc-number">7.</span> <span class="toc-text">About silent error</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ext4-xfs-hardware-raid"><span class="toc-number">7.1.</span> <span class="toc-text">ext4 xfs hardware raid</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Openzfs-with-slient-error"><span class="toc-number">8.</span> <span class="toc-text">Openzfs with slient error</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#script-for-raidz"><span class="toc-number">9.</span> <span class="toc-text">script for raidz</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Extend-single-device-become-a-mirror"><span class="toc-number">10.</span> <span class="toc-text">Extend single device become a mirror</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Add-special-device"><span class="toc-number">11.</span> <span class="toc-text">Add special device</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zpool-replace"><span class="toc-number">12.</span> <span class="toc-text">zpool replace</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Online-upgrade-SAS-device-firmware"><span class="toc-number">13.</span> <span class="toc-text">Online upgrade SAS device firmware</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Custom-packages"><span class="toc-number">14.</span> <span class="toc-text">Custom packages</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Backup-zpool"><span class="toc-number">15.</span> <span class="toc-text">Backup zpool</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Openzip-direct-IO"><span class="toc-number">16.</span> <span class="toc-text">Openzip direct IO</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Performance-tuning"><span class="toc-number">17.</span> <span class="toc-text">Performance tuning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-bit-of-throught-about-the-read-performance-not-balaning-in-a-single-raidz-zpool"><span class="toc-number">18.</span> <span class="toc-text">A bit of throught about the read performance not balaning in a single raidz zpool</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Thanks-for-Javen-%E2%80%98s-help"><span class="toc-number">19.</span> <span class="toc-text">Thanks for Javen ‘s help</span></a></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(/img/photo_by_spacex.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">己不由心，身又岂能由己</a></span><span id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">openzfs tips</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2016-12-04T07:06:13.000Z" title="Created 2016-12-04 15:06:13">2016-12-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2020-09-29T15:22:06.000Z" title="Updated 2020-09-29 23:22:06">2020-09-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/filesystem/">filesystem</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h3 id="Why-ZFS"><a href="#Why-ZFS" class="headerlink" title="Why ZFS"></a><a target="_blank" rel="noopener" href="https://www.fujitsu.com/global/Images/ZFS%20Overview%20and%20Design%20Guide.pdf">Why ZFS</a></h3><ul>
<li>Transaction file system(Data consistency guaranteed)<ul>
<li>Manages writing of data with copy-on-write method</li>
<li>Does not overwrite original data</li>
<li>Commits or ignores sequential processing completely<ul>
<li>No data inconsistency</li>
</ul>
</li>
<li>Consistency check not required for the file system</li>
<li>Asynchronous writing to disks<ul>
<li>The system writes to the disk (I/O) after the end of sequential processing, so a check of the actual disk capacity (by a command such as df or du) may find that it is different.</li>
</ul>
</li>
</ul>
</li>
<li>End-to-end checksum(Invalid data detected Data self-corrected)<ul>
<li>Stores checksum of data block in its parent block</li>
<li>Restores data from redundant block when error is detected</li>
<li>Also detects and corrects logical inconsistencies (software bugs)</li>
<li>Data cannot be recovered when the data corruption range includes the checksum</li>
<li>The data and checksum are physically separated and read individually. The checksum itself can be recovered from the higher-level block.</li>
<li>ZFS not only detects the read errors caused by a disk fault, but also detects and corrects logical inconsistencies caused by a software bug</li>
<li>When read or written, invalid data is detected by the checksum. When invalid data is detected, if the system is in a redundant configuration (RAID 1 (mirroring), RAID-Z, RAID-Z2, or RAID-Z3), the data is automatically recovered (selfhealing)</li>
</ul>
</li>
</ul>
<h3 id="How-many-devices-in-single-raidz-zpool"><a href="#How-many-devices-in-single-raidz-zpool" class="headerlink" title="How many devices in single raidz zpool"></a>How many devices in single raidz zpool</h3><p>2^n + parity<br>raidz2 2^3+2=10 2^4+2=18 , we have 14 x HDDs in single raidz2 zpool in production<br>raidz3 2^3+3=11 2^4+3=19 , next we will choice 19 x HDDs in single raidz3</p>
<a id="more"></a>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">openzfs 0.8.2 </span><br><span class="line"></span><br><span class="line">10TBxHDD  ashift12    ashift9(TiB)  a12&#x2F;a9   raw(TiB) </span><br><span class="line">8+2         68           71         95.77%   90.957 (9.0957*10)</span><br><span class="line">10+2        81           88         92%      109.148</span><br><span class="line">12+2       102          106         96.22%   127.34</span><br><span class="line">14+2       116          123         94.3%    145.531</span><br><span class="line">16+2       141          141         100%     163.723</span><br><span class="line">8+3         71           71         100%     100.053</span><br><span class="line">11+3        90           97         92.78%   127.34</span><br><span class="line">12+3        97          105         92.38%   136.436</span><br><span class="line">16+3       134          141         95%      172.818</span><br><span class="line">17+3       141          149         94.6%    181.914</span><br><span class="line">18+3       148          156         94.8%    191.01</span><br><span class="line">19+3       155          166         93.3%    200.105</span><br><span class="line">20+3       162          175         92.5%    209.201</span><br><span class="line"></span><br><span class="line">12TBxHDD  ashift12    ashift9(TiB)  a12&#x2F;a9   raw(TB)   raw compare with 9       raw compare with 12</span><br><span class="line">5+3         49	         53         92.4%    96          55.2%                   51%</span><br><span class="line">6+2         61           64         92.4%    96          66.6%                   63.5%</span><br><span class="line">8+2         81           85         95.2%    120         70.8%                   67.5%</span><br><span class="line">16+2       170          170         100%     216         78.7%  (recommand)    78.7%</span><br><span class="line">8+3         85           85         100%     132         64.3%                 64.3%</span><br><span class="line">11+3       108          116         93.1%    168         69%                   64.2%</span><br><span class="line">12+3       116          126         92%      180         70%                   64.4%</span><br><span class="line">15+3       139          157         88.5%    216         72.6%                 64.3%</span><br><span class="line">16+3       161          170         94.7%    228         74.5% (recommand)     70.6%</span><br><span class="line">19+3       182          194         93.8%    264         73.4%                 68.9%</span><br><span class="line">20+3       195          210         91.42%   276         76%                   70.6%</span><br><span class="line">21+3       203          220         92.2%    288         76.3%                 70.4%</span><br><span class="line"></span><br><span class="line">12TB SAS device</span><br><span class="line">5U 84 bay 12TB</span><br><span class="line">(16+2)*4+(6+2)+4 hs &#x3D; 170*4+61 &#x3D; 741TiB&#x2F;1008TB &#x3D; 73.4% (TiB&#x2F;TB) (ashfit 12, raidz2, 4 hot spare)</span><br><span class="line">(16+3)*4+(6+2) &#x3D; 161*4 + 61 &#x3D; 705TiB&#x2F;1008TB &#x3D; 69.9% (ashfit 12, raidz3)</span><br><span class="line">(16+3)*4+(6+2) &#x3D; 170*4 + 61 &#x3D; 741TiB&#x2F;1008TB &#x3D; 73.4% (ashfit 9, raidz3)</span><br><span class="line"></span><br><span class="line">4U 60 bay 12TB</span><br><span class="line">(16+2)*3+6hs&#x3D;170*3&#x2F;720&#x3D;70.8% (ashfit 12,raidz2,6 hot spare)</span><br><span class="line">(16+3)*3+3hs&#x3D;161*3&#x2F;720&#x3D;67%   (ashfit 12, raidz3, 3 hot spare)</span><br><span class="line">(16+3)*3+3hs&#x3D;170*3&#x2F;720&#x3D;70.8% (ashfit 9, raidz3, 3 hot spare)</span><br><span class="line"></span><br><span class="line">4U 90 bay 12TB</span><br><span class="line">(16+2)*4+(12+2)+4 hs&#x3D;(170*4+102)&#x2F;1080 &#x3D; 72.4% (ahsift 12, raidz2, 4 hot spare)</span><br><span class="line">(16+3)*4+(8+2)&#x3D;170*4+81&#x2F;1080 &#x3D; 70.4% (ahsift 12, raidz3)</span><br><span class="line">(16+3)*4+(8+2)&#x3D;170*4+85&#x2F;1080 &#x3D; 70.8% (ahsift 9, raidz3)</span><br><span class="line"></span><br><span class="line">4U 102 bay</span><br><span class="line">(16+2)*5+(6+2)+4hs &#x3D;(170*5+61)&#x2F;1224 &#x3D; 74.4% (ashfit 12, raidz2, 4 hot spare)</span><br><span class="line">(16+3)*5+ 43(4+2)+1hs &#x3D;((161*5)+43)&#x2F;1224 &#x3D; 69.2% (ashfit 12, raidz3, 1 hot spare)</span><br><span class="line">(16+3)*5+ 43(4+2)+1hs &#x3D;((170*5)+43)&#x2F;1224 &#x3D; 72.9% (ashfit 9, raidz3, 1 hot spare)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">openzfs 0.7.9</span><br><span class="line">       9  12  raw  84-jbod</span><br><span class="line">16+3 163 157 228   788</span><br><span class="line">16+3 163 157 228   788</span><br><span class="line">15+3 153 136</span><br><span class="line">14+3 142 128</span><br><span class="line">13+3 134 121</span><br><span class="line">12+3 123 113</span><br><span class="line">11+3 113 105       678</span><br><span class="line">10+3 103 98</span><br><span class="line">9+3  93  91</span><br><span class="line">8+3  83  83</span><br></pre></td></tr></table></figure>

<p>not make sure</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[381558.364210] sd 17:0:132:0: [sg134] CDB: Test Unit Ready 00 00 00 00 00 00</span><br><span class="line">[381558.365100] scsi target17:0:132: handle(0x0097), sas_address(0x5000c500955e5855), phy(23)</span><br><span class="line">[381558.365961] scsi target17:0:132: enclosure_logical_id(0x50050cc11ac01572), slot(45)</span><br><span class="line">[381558.368328] sd 17:0:132:0: task abort: SUCCESS scmd(ffff8803abf18a80)</span><br><span class="line">[381558.369172] scsi target17:0:132: attempting target reset! scmd(ffff8803abf18a80)</span><br><span class="line">[381558.370012] sd 17:0:132:0: [sg134] CDB: Inquiry 12 01 83 00 f0 00</span><br><span class="line">[381558.370845] scsi target17:0:132: handle(0x0097), sas_address(0x5000c500955e5855), phy(23)</span><br><span class="line">[381558.371660] scsi target17:0:132: enclosure_logical_id(0x50050cc11ac01572), slot(45)</span><br><span class="line">[381558.631612] scsi target17:0:132: target reset: SUCCESS scmd(ffff8803abf18a80)</span><br><span class="line">[381568.608006] sd 17:0:132:0: attempting task abort! scmd(ffff8803abf18a80)</span><br><span class="line">[381568.608802] sd 17:0:132:0: [sg134] CDB: Test Unit Ready 00 00 00 00 00 00</span><br><span class="line">[381568.609600] scsi target17:0:132: handle(0x0097), sas_address(0x5000c500955e5855), phy(23)</span><br><span class="line">[381568.610369] scsi target17:0:132: enclosure_logical_id(0x50050cc11ac01572), slot(45)</span><br><span class="line">[381568.612506] sd 17:0:132:0: task abort: SUCCESS scmd(ffff8803abf18a80)</span><br><span class="line">[381568.613253] mpt2sas0: attempting host reset! scmd(ffff8803abf18a80)</span><br><span class="line">[381568.613989] sd 17:0:132:0: [sg134] CDB: Inquiry 12 01 83 00 f0 00</span><br><span class="line">[381568.614761] mpt2sas0: sending diag reset !!</span><br><span class="line">[381569.611617] mpt2sas0: diag reset: SUCCESS</span><br><span class="line">[381569.747649] mpt2sas0: LSISAS2308: FWVersion(20.00.07.00), ChipRevision(0x05), BiosVersion(07.27.01.01)</span><br><span class="line">[381569.748356] mpt2sas0: Protocol=(Initiator,Target), Capabilities=(TLR,EEDP,Snapshot Buffer,Diag Trace Buffer,Task Set Full,NCQ)</span><br><span class="line">[381569.749818] mpt2sas0: sending port <span class="built_in">enable</span> !!</span><br><span class="line">[381573.276922] WARNING: Pool <span class="string">&#x27;ost_81&#x27;</span> has encountered an uncorrectable I/O failure and has been suspended.</span><br><span class="line"></span><br><span class="line">[381573.392645] WARNING: Pool <span class="string">&#x27;ost_85&#x27;</span> has encountered an uncorrectable I/O failure and has been suspended.</span><br><span class="line"></span><br><span class="line">[381573.420579] WARNING: Pool <span class="string">&#x27;ost_83&#x27;</span> has encountered an uncorrectable I/O failure and has been suspended.</span><br><span class="line"></span><br><span class="line">[381573.445523] WARNING: Pool <span class="string">&#x27;ost_87&#x27;</span> has encountered an uncorrectable I/O failure and has been suspended.</span><br><span class="line"></span><br><span class="line">[381576.820367] mpt2sas0: port <span class="built_in">enable</span>: SUCCESS</span><br><span class="line">[381576.821174] mpt2sas0: search <span class="keyword">for</span> end-devices: start</span><br><span class="line">[381576.823066] scsi target17:0:0: handle(0x000e), sas_addr(0x50050cc11938283e), enclosure logical id(0x50050cc11ac018e1), slot(0)</span><br><span class="line">[381576.824595] scsi target17:0:1: handle(0x000f), sas_addr(0x5000cca2735b5bd5), enclosure logical id(0x50050cc11ac018e1), slot(76)</span><br><span class="line">[381576.826169] scsi target17:0:2: handle(0x0010), sas_addr(0x5000cca2735d80cd), enclosure logical id(0x50050cc11ac018e1), slot(79)a</span><br><span class="line">[381576.827819] scsi target17:0:3: handle(0x0011), sas_addr(0x5000cca2732f2521), enclosure logical id(0x50050cc11ac018e1), slot(82)</span><br><span class="line">[381576.829497] scsi target17:0:4: handle(0x0012), sas_addr(0x5000cca27355f089), enclosure logical id(0x50050cc11ac018e1), slot(83)</span><br></pre></td></tr></table></figure>

<h3 id="NVME-SSD-test-with-openzfs-0-7"><a href="#NVME-SSD-test-with-openzfs-0-7" class="headerlink" title="NVME SSD test with openzfs 0.7"></a>NVME SSD test with openzfs 0.7</h3><h3 id="6x-Intel-P3520"><a href="#6x-Intel-P3520" class="headerlink" title="[6x Intel P3520]"></a>[6x Intel P3520]</h3><p>Before modify default parameter. there are only 1.xGB/s read or write. too slow…….</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">parted /dev/nvme0n1 p</span><br><span class="line">Model: Unknown (unknown)</span><br><span class="line">Disk /dev/nvme0n1: 2000GB</span><br><span class="line">Sector size (logical/physical): 512B/512B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags:</span><br><span class="line"></span><br><span class="line">Number  Start   End     Size    File system  Name  Flags</span><br><span class="line"> 1      1049kB  1900GB  1900GB               disk1</span><br><span class="line"></span><br><span class="line">zpool destroy tank01</span><br><span class="line">zpool create tank01 -o ashift=13 -O recordsize=1M /dev/nvme&#123;0..5&#125;n1p1</span><br><span class="line"></span><br><span class="line"><span class="comment">## SAS SSD test in centos 8 with openzfs 0.8.3, the best </span></span><br><span class="line">zpool create -f tank -O canmount=on -O xattr=sa -O acltype=posixacl -o ashift=13 -O recordsize=8K -O primarycache=none -O secondarycache=none raidz2 /dev/sd&#123;c..l&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># logbias=throughput will cause filesystem fragmentation</span></span><br><span class="line"></span><br><span class="line">In the all flash raidz <span class="built_in">test</span>, In the begining the iops could reach 35k iops, about 15 mins, it <span class="string">&#x27;s 18k iops, after 40 mins, it &#x27;</span>s only 4k~5k iops. average about 8~9k iops. that too bad.</span><br><span class="line">And openzfs mirror has the same issues. because the fragmentation go to 12%, <span class="built_in">disable</span> hyper-threading and re-test has no <span class="built_in">help</span>  </span><br><span class="line">mdadm raid10 about 6x performance than openzfs mirror <span class="keyword">in</span> 8K <span class="built_in">test</span>  </span><br><span class="line">the fio will cause a lot of zfs fragment rather <span class="keyword">in</span> benchmark with ext4/xfs  </span><br><span class="line">The differernt are fallocate write file <span class="keyword">in</span> ext4 and xfs.   </span><br><span class="line">ZFS is fadvise64   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#test mds</span></span><br><span class="line">$ zpool create -f tank01 -O canmount=on -O xattr=sa -O acltype=posixacl -o ashift=12 -O recordsize=32k -O secondarycache=none -O logbias=throughput -o multihost=on -O autoreplace=on  /dev/sd&#123;b..d&#125;</span><br><span class="line"></span><br><span class="line">xattr=on stores extended attributes <span class="keyword">in</span> hidden sub directories, <span class="built_in">which</span> can require multiple lookups when accessing a file</span><br><span class="line">The alternative, xattr=sa, stores extended attributes <span class="keyword">in</span> inodes, resulting <span class="keyword">in</span> less IO requests when extended attributes are <span class="keyword">in</span> use</span><br><span class="line"></span><br><span class="line">The logbias property – You can use this property to provide a hint to ZFS about handling synchronous requests <span class="keyword">for</span> a specific dataset. If logbias is <span class="built_in">set</span> to latency, ZFS uses the pool<span class="string">&#x27;s separate log devices, if any, to handle the requests at low latency. If logbias is set to throughput, ZFS does not use the pool&#x27;</span>s separate <span class="built_in">log</span> devices. Instead, ZFS optimizes synchronous operations <span class="keyword">for</span> global pool throughput and efficient use of rebackup.</span><br><span class="line"></span><br><span class="line">Does it mean <span class="keyword">in</span> all flash env, you<span class="string">&#x27;d better set logbias=throughput</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">              capacity     operations     bandwidth</span></span><br><span class="line"><span class="string">pool        alloc   free   read  write   read  write</span></span><br><span class="line"><span class="string">----------  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">tank01       132G  10.2T      0  16.7K      0  5.79G</span></span><br><span class="line"><span class="string">tank01       146G  10.2T      0  12.4K      0  7.22G</span></span><br><span class="line"><span class="string">tank01       161G  10.2T      0  10.9K      0  7.41G</span></span><br><span class="line"><span class="string">tank01       174G  10.1T      0  12.1K      0  7.18G</span></span><br></pre></td></tr></table></figure>

<h3 id="the-old-optimize-with-lfs"><a href="#the-old-optimize-with-lfs" class="headerlink" title="the old optimize with lfs"></a><a target="_blank" rel="noopener" href="http://wiki.lfs.org/images/4/48/ZFS-As-Backend-File-System_Paciucci.pdf">the old optimize with lfs</a></h3><p><code>There are some issue in this config that means the engineer not make zfs struct clearly</code><br>If you are disbale compression function, zio_taskq_batch_pct could must be limit</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/modprobe.d/zfs.conf</span><br><span class="line">options zfs zfs_prefetch_disable=1</span><br><span class="line">options zfs metaslab_debug_unload=1</span><br><span class="line">options zfs zfs_arc_max=$(awk <span class="string">&#x27;$0~/MemTotal/&#123;printf &quot;%.0f \n&quot;, $2*1024*0.6&#125;&#x27;</span> /proc/meminfo) <span class="comment">#60%, 3/5</span></span><br><span class="line">options zfs zfs_arc_meta_limit=$(awk <span class="string">&#x27;$0~/MemTotal/&#123;printf &quot;%.0f \n&quot;, $2*1024*0.75&#125;&#x27;</span> /proc/meminfo) <span class="comment">#MDS only,75%</span></span><br><span class="line">options zfs zfs_dirty_data_max=$(awk <span class="string">&#x27;$0~/MemTotal/&#123;printf &quot;%.0f \n&quot;, $2*1024*0.15&#125;&#x27;</span> /proc/meminfo) <span class="comment">#15%</span></span><br><span class="line">options zfs zfs_vdev_async_write_min_active=5 <span class="comment"># depends your hdd number of your volume ,SSD could be higher</span></span><br><span class="line">options zfs zfs_vdev_async_write_max_active=15</span><br><span class="line">options zfs zfs_vdev_sync_read_min_active=16 <span class="comment"># depends your hdd number of your volume ,SSD could be higher</span></span><br><span class="line">options zfs zfs_vdev_sync_read_max_active=16</span><br><span class="line">options zfs zfs_vdev_async_write_active_min_dirty_percent=20</span><br><span class="line">options zfs zfs_vdev_scheduler=deadline <span class="comment">#noop for SSD</span></span><br></pre></td></tr></table></figure>
<p>zfs_vdev_scheduler not support mq-deadline<br>since ZFS has its own I/O scheduler, using a simple scheduler can result in more consistent performance<br>expected: noop, cfq, bfq, and deadline</p>
<p><a target="_blank" rel="noopener" href="http://lfs.ornl.gov/ecosystem-2016/documents/tutorials/Stearman-LLNL-ZFS.pdf">Tutorial: How to install, tune and Monitor a ZFS based Lustre file system</a></p>
<h4 id="zfs-txg-timeout"><a href="#zfs-txg-timeout" class="headerlink" title="zfs_txg_timeout"></a>zfs_txg_timeout</h4><p>The last zfs_txg_history txg commits are available in /proc/spl/kstat/zfs/POOL_NAME/txgs</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/spl/kstat/zfs/tank/txgs</span><br><span class="line">25 0 0x01 83 9296 11320470741 2382694537989353</span><br><span class="line">txg      birth            state ndirty       nread        nwritten     reads    writes   otime        qtime        wtime        stime</span><br><span class="line">13974499 2382281791630216 C     1949696      0            1433600      0        526      4999917952   44691        34328        382573295</span><br><span class="line">13974500 2382286791548168 C     1998848      0            1453056      0        552      5000003695   46223        44384        386415831</span><br><span class="line">13974501 2382291791551863 C     1458176      0            1245696      0        539      4999926718   45704        33119        386636075</span><br><span class="line">13974502 2382296791478581 C     950272       0            887296       0        492      4999981264   44784        34896        336542457</span><br><span class="line">13974503 2382301791459845 C     1146880      0            1244672      0        546      4999967135   67884        46074        346304560</span><br><span class="line">13974504 2382306791426980 C     4177920      0            2828288      0        600      4999946876   46380        45891        423776127</span><br></pre></td></tr></table></figure>

<h4 id="L2ARC-and-slog"><a href="#L2ARC-and-slog" class="headerlink" title="L2ARC and slog"></a>L2ARC and slog</h4><p>l2arc_write_max<br>If the cache devices can sustain the write workload, increasing the rate of cache device fill when workloads generate new data at a rate higher than l2arc_write_max can increase L2ARC hit rate</p>
<h1 id="max-write-speed-to-l2arc"><a href="#max-write-speed-to-l2arc" class="headerlink" title="max write speed to l2arc"></a>max write speed to l2arc</h1><h1 id="tradeoff-between-write-read-and-durability-of-ssd"><a href="#tradeoff-between-write-read-and-durability-of-ssd" class="headerlink" title="tradeoff between write/read and durability of ssd (?)"></a>tradeoff between write/read and durability of ssd (?)</h1><h1 id="default-8-1024-1024-8-MB-s"><a href="#default-8-1024-1024-8-MB-s" class="headerlink" title="default : 8 * 1024 * 1024  = 8 MB/s"></a>default : 8 * 1024 * 1024  = 8 MB/s</h1><h1 id="setting-here-500-1024-1024"><a href="#setting-here-500-1024-1024" class="headerlink" title="setting here : 500 * 1024 * 1024"></a>setting here : 500 * 1024 * 1024</h1><p>options zfs l2arc_write_max=524288000</p>
<p>l2arc_headroom</p>
<h1 id="number-of-max-device-writes-to-precache"><a href="#number-of-max-device-writes-to-precache" class="headerlink" title="number of max device writes to precache"></a>number of max device writes to precache</h1><h1 id="default-2"><a href="#default-2" class="headerlink" title="default : 2"></a>default : 2</h1><p>options zfs l2arc_headroom=12</p>
<p>zfs_immediate_write_sz</p>
<h1 id="default-32768"><a href="#default-32768" class="headerlink" title="default : 32768"></a>default : 32768</h1><p>options zfs zfs_immediate_write_sz=131072</p>
<p>zil_slog_limit</p>
<h1 id="default-1024-1024-1mb"><a href="#default-1024-1024-1mb" class="headerlink" title="default : 1024*1024 = 1mb"></a>default : 1024*1024 = 1mb</h1><p>options zfs zil_slog_limit=536870912</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ arcstat.py -f <span class="string">&quot;time,read,hit%,hits,miss%,miss,arcsz,c&quot;</span> 1</span><br><span class="line"></span><br><span class="line">$ cat /proc/spl/kstat/zfs/arcstats</span><br><span class="line">$ arcstat.py 2</span><br><span class="line">$ arc_summary.py | grep <span class="string">&quot;L2 ARC Breakdown&quot;</span> -A 2</span><br><span class="line">L2 ARC Breakdown:                               846.25m</span><br><span class="line">        Hit Ratio:                      0.47%   3.98m</span><br><span class="line">        Miss Ratio:                     99.53%  842.27m</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h4 id="zfs-vdev-scheduler"><a href="#zfs-vdev-scheduler" class="headerlink" title="zfs_vdev_scheduler"></a>zfs_vdev_scheduler</h4><p>Set the IO scheduler used by ZFS. Both noop and deadline, which implement simple scheduling algorithms, are good options, as the storage daemon is run by a single Linux user.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> deadline &gt; /sys/module/zfs/parameters/zfs_vdev_scheduler</span><br></pre></td></tr></table></figure>

<h4 id="zfs-read-chunk-size"><a href="#zfs-read-chunk-size" class="headerlink" title="zfs_read_chunk_size"></a>zfs_read_chunk_size</h4><p>Data is read by ZFS in data chunks of a certain size.  it depends your zfs setting</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 1310720 &gt; /sys/module/zfs/parameters/zfs_read_chunk_size</span><br></pre></td></tr></table></figure>

<h4 id="zfs-max-recordsize"><a href="#zfs-max-recordsize" class="headerlink" title="zfs_max_recordsize"></a>zfs_max_recordsize</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 4194304 &gt; /sys/module/zfs/parameters/zfs_max_recordsize</span><br></pre></td></tr></table></figure>

<h4 id="zfs-vdev-aggregation-limit"><a href="#zfs-vdev-aggregation-limit" class="headerlink" title="zfs_vdev_aggregation_limit"></a>zfs_vdev_aggregation_limit</h4><p>ZFS is able to aggregate small IO operations that handle neighboring or overlapping data into larger operations, in order to reduce the number of IOPs</p>
<p>Setting zfs_vdev_aggregation_limit = 0 effectively disables aggregation by ZFS</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 262144 &gt; /sys/module/zfs/parameters/zfs_vdev_aggregation_limit</span><br></pre></td></tr></table></figure>


<h4 id="HDD-prefetch"><a href="#HDD-prefetch" class="headerlink" title="HDD prefetch"></a>HDD prefetch</h4><p>Set this option to 0 to activate data prefetching if you are using spinning disks. Set it to 1 to disable it if you are using flash devices like SSDs.</p>
<p>increase HDD performance a lot</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zfs_prefetch_disable&#x3D;0 #Must enable for HDD</span><br></pre></td></tr></table></figure>

<h4 id="Vdev-cache"><a href="#Vdev-cache" class="headerlink" title="Vdev cache"></a>Vdev cache</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">options zfs zfs_vdev_cache_size=1310720</span><br><span class="line">options zfs zfs_vdev_cache_max=131072</span><br><span class="line">options zfs zfs_vdev_cache_bshift=17</span><br></pre></td></tr></table></figure>

<h4 id="txg-timeout"><a href="#txg-timeout" class="headerlink" title="txg_timeout"></a>txg_timeout</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ cat /sys/module/zfs/parameters/zfs_txg_timeout</span><br><span class="line">5</span><br><span class="line"></span><br><span class="line">every 5s will <span class="keyword">do</span> transaction sync</span><br><span class="line"></span><br><span class="line">$ zdb -lu /dev/sdc1 | grep <span class="string">&quot;txg =&quot;</span> -c</span><br><span class="line">128</span><br><span class="line"></span><br><span class="line">128 x 5s= 640s</span><br><span class="line"></span><br><span class="line">If <span class="keyword">in</span> 640s, power off the server imported the zpool ASAP, reboot single server to check the zpool status.</span><br><span class="line">If out of 640s, restart another server, keep the single import server, and backup your data, hope it not be panic.</span><br><span class="line">Need a zfs fsck tool.</span><br><span class="line"></span><br><span class="line">should I improve the value ??? Dangerous !!! impact performance,  enough memory ?</span><br><span class="line">$ <span class="built_in">echo</span> 10 &gt;/sys/module/zfs/parameters/zfs_txg_timeout</span><br></pre></td></tr></table></figure>

<h4 id="zpool-sync-mode"><a href="#zpool-sync-mode" class="headerlink" title="zpool sync mode"></a>zpool sync mode</h4><p>sync=standard : sync writes are written 2 times (first to LOG, second as normal write every ~5 seconds), async write are written only once (every ~5 seconds)<br>sync=always : sync writes and async writes are written 2 times (first to LOG, second as normal write every ~5 seconds).<br>sync=disabled : sync writes and async writes are written only once (every ~5 seconds)<br>Of course ZFS can flush it write cache between 5 second period but the biggest flush is every ~5 seconds.</p>
<h4 id="Limit-memory-usage"><a href="#Limit-memory-usage" class="headerlink" title="Limit memory usage"></a>Limit memory usage</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> xxx &gt; /sys/module/zfs/parameters/zfs_arc_max</span><br><span class="line">cat xxx &gt; /sys/module/zfs/parameters/zfs_arc_min</span><br><span class="line"></span><br><span class="line">cat /etc/modprobe.d/zfs.conf <span class="comment"># Min 4096MB / Max 8192MB limit</span></span><br><span class="line">options zfs zfs_arc_min=4294967296</span><br><span class="line">options zfs zfs_arc_max=8589934592</span><br><span class="line"></span><br><span class="line">modprobe zfs zfs_arc_min=4294967296</span><br></pre></td></tr></table></figure>

<h4 id="zfs-multihost-history"><a href="#zfs-multihost-history" class="headerlink" title="zfs_multihost_history"></a>zfs_multihost_history</h4><p>The pool multihost multimodifier protection (MMP) subsystem can record historical updates in the /proc/spl/kstat/zfs/POOL_NAME/multihost</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 400 &gt;/sys/module/zfs/parameters/zfs_multihost_history</span><br><span class="line">cat /proc/spl/kstat/zfs/ost_0/multihost</span><br><span class="line">47 0 0x01 109 9592 181591887850 4241294257803975</span><br><span class="line">id         txg        timestamp  error  duration   mmp_delay    vdev_guid                vdev_label vdev_path</span><br><span class="line">88663237   43205944   1556725492      0     259182    409397766 5775269556738108556      0          /dev/disk/by-id/scsi-35000c500a670c847-part1</span><br><span class="line">88663238   43205944   1556725492      0    1651942    406910425 10040726243797242983     1          /dev/disk/by-id/scsi-35000c500a670c6cb-part1</span><br><span class="line">88663239   43205944   1556725492      0     262443    404453220 85871317386613310        0          /dev/disk/by-id/scsi-35000c500a65ada5f-part1</span><br><span class="line">88663240   43205944   1556725492      0   16196129    401993428 14057198631110435259     1          /dev/disk/by-id/scsi-35000c500a6753f53-part1</span><br><span class="line">88663241   43205944   1556725493      0     148851    399688210 15150783868311748605     2          /dev/disk/by-id/scsi-35000c500a670c73b-part1</span><br><span class="line">88663242   43205944   1556725493      0     213149    397151149 14018438879105052775     1          /dev/disk/by-id/scsi-35000c500a670cbc7-part1</span><br><span class="line">88663243   43205944   1556725493      0     916612    394759789 12962037773541489706     2          /dev/disk/by-id/scsi-35000c500a65afba7-part1</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://www.svennd.be/tuning-of-zfs-module/">reference</a></p>
<h3 id="zpool-could-not-mount"><a href="#zpool-could-not-mount" class="headerlink" title="[zpool could not mount]"></a>[zpool could not mount]</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">$ zpool <span class="built_in">set</span> cachefile=none tank</span><br><span class="line">$ zpool import -N -o cachefile=none -d /dev/disk/by-id tank01</span><br><span class="line">$ zfs mount -o ro tank01</span><br><span class="line"></span><br><span class="line">$ zpool import -Nm tank</span><br><span class="line"></span><br><span class="line"><span class="comment"># very dangerours, please backup all data</span></span><br><span class="line">$ zpool import -fFX -R /tmp/tank ost_11</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Security script</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### Just check, not take action</span></span><br><span class="line">pool=<span class="string">&quot;tank&quot;</span></span><br><span class="line"><span class="comment">#find a vdev of pool</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">read</span> line1</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  vdev=<span class="variable">$line1</span></span><br><span class="line">  zdb -lu <span class="variable">$vdev</span> |grep <span class="string">&quot;txg &quot;</span> |cut -d <span class="string">&quot; &quot;</span> -f 3 &gt; /tmp/txg</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&#x27;echo ----&#x27;</span><span class="variable">$line1</span><span class="string">&#x27;-----&#x27;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> <span class="built_in">read</span> LINE;</span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&#x27;echo zdb -e &#x27;</span><span class="variable">$pool</span><span class="string">&#x27; -d -t &#x27;</span><span class="variable">$LINE</span></span><br><span class="line">    <span class="built_in">echo</span> zdb -e <span class="variable">$pool</span> -d -t <span class="variable">$LINE</span></span><br><span class="line">  <span class="keyword">done</span> &lt; /tmp/txg</span><br><span class="line"></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&#x27;echo ----&#x27;</span><span class="variable">$vdev</span><span class="string">&#x27;--finish-----&#x27;</span></span><br><span class="line"><span class="keyword">done</span> &lt; zpool_devs</span><br></pre></td></tr></table></figure>

<h4 id="Try-zpool-T-parameter"><a href="#Try-zpool-T-parameter" class="headerlink" title="Try zpool -T parameter"></a><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/issues/2831">Try zpool -T parameter</a></h4><p>You’ll need to make this one line change and rebuild the module. After which you’ll be able to use the -T option. This effectively disables the logic which prevents you from using uberblocks which are older than the label.</p>
<p>import from txg, like import snapshot timestamp</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">diff --git a/module/zfs/vdev_label.c b/module/zfs/vdev_label.c</span><br><span class="line">index 1c2f00f..509e812 100644</span><br><span class="line">--- a/module/zfs/vdev_label.c</span><br><span class="line">+++ b/module/zfs/vdev_label.c</span><br><span class="line">@@ -471,7 +471,7 @@ retry:</span><br><span class="line">                        <span class="keyword">if</span> ((error || label_txg == 0) &amp;&amp; !config) &#123;</span><br><span class="line">                                config = label;</span><br><span class="line">                                <span class="built_in">break</span>;</span><br><span class="line">-                       &#125; <span class="keyword">else</span> <span class="keyword">if</span> (label_txg &lt;= txg &amp;&amp; label_txg &gt; best_txg) &#123;</span><br><span class="line">+                       &#125; <span class="keyword">else</span> <span class="keyword">if</span> (label_txg &gt; best_txg) &#123;</span><br><span class="line">                                best_txg = label_txg;</span><br><span class="line">                                nvlist_free(config);</span><br><span class="line">                                config = fnvlist_dup(label);</span><br></pre></td></tr></table></figure>

<h4 id="Add-skip-error-by-some-parameters-in-ZOL-0-8"><a href="#Add-skip-error-by-some-parameters-in-ZOL-0-8" class="headerlink" title="Add skip error by some parameters in ZOL 0.8"></a>Add skip error by some parameters in ZOL 0.8</h4><p>spa_load_verify_data<br>At the risk of data integrity, to speed extreme import of large pool<br>If this parameter is set to 0, the traversal skips non-metadata blocks. It can be toggled once the import has started to stop or start the traversal of non-metadata blocks.</p>
<p>spa_load_verify_metadata<br>At the risk of data integrity, to speed extreme import of large pool<br>If this parameter is set to 0, the traversal is not performed. It can be toggled once the import has started to stop or start the traversal</p>
<h4 id="zfs-vol"><a href="#zfs-vol" class="headerlink" title="zfs vol"></a>zfs vol</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">zfs create -V 40T -o volblocksize=128k tank/zvol</span><br><span class="line">ls -l /dev/zvol/tank/zvol</span><br><span class="line">``</span><br><span class="line"></span><br><span class="line"><span class="comment">### mount zfs</span></span><br><span class="line">```bash</span><br><span class="line">pool=tank</span><br><span class="line">zfs <span class="built_in">set</span> canmount=on <span class="variable">$pool</span></span><br><span class="line">zfs <span class="built_in">set</span> mountpoint=/mnt <span class="variable">$pool</span></span><br><span class="line">zfs mount <span class="variable">$pool</span></span><br><span class="line">rm -f /mnt/xxx/xxx/xxx</span><br><span class="line">zfs umount <span class="variable">$pool</span></span><br><span class="line">zfs <span class="built_in">set</span> canmount=off</span><br></pre></td></tr></table></figure>

<h3 id="zfs-event"><a href="#zfs-event" class="headerlink" title="zfs event"></a>zfs event</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ zpool events -c <span class="comment"># clean events</span></span><br><span class="line">$ zpool import tank</span><br><span class="line">$ zpool events -v</span><br><span class="line">$ zpool events -v &gt; zpool_import_tank</span><br></pre></td></tr></table></figure>

<h3 id="Compression"><a href="#Compression" class="headerlink" title="Compression"></a>Compression</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> compression=lz4 tank</span><br><span class="line">zfs <span class="built_in">set</span> compression=gzip-9 tank</span><br></pre></td></tr></table></figure>
<h3 id="Xattr"><a href="#Xattr" class="headerlink" title="Xattr"></a>Xattr</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> xattr=sa tank</span><br></pre></td></tr></table></figure>

<h3 id="ACL"><a href="#ACL" class="headerlink" title="ACL"></a>ACL</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> acltype=posixacl tank/gentoo/home</span><br><span class="line">zfs get acltype tank/gentoo/home</span><br></pre></td></tr></table></figure>

<h3 id="Rmount"><a href="#Rmount" class="headerlink" title="Rmount"></a>Rmount</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> mountpoint=/var/lib/mysql tank/mysql</span><br><span class="line">zfs umount tank/mysql</span><br><span class="line">zfs mount tank/mysql</span><br></pre></td></tr></table></figure>

<h3 id="Zpool-rename"><a href="#Zpool-rename" class="headerlink" title="Zpool rename"></a>Zpool rename</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zpool <span class="built_in">export</span> app</span><br><span class="line">zpool import app apps</span><br></pre></td></tr></table></figure>

<h3 id="Detach-hdd-from-mirror"><a href="#Detach-hdd-from-mirror" class="headerlink" title="Detach hdd from mirror"></a>Detach hdd from mirror</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">zpool detach tank sdz <span class="comment">#detach zda from mirror-17</span></span><br><span class="line">only sdal <span class="keyword">in</span> mirror-17, and mirror-17 is missing</span><br><span class="line"></span><br><span class="line">          mirror-16  ONLINE       0     0     0</span><br><span class="line">            sdai     ONLINE       0     0     0</span><br><span class="line">            sdaj     ONLINE       0     0     0</span><br><span class="line">          sdal       ONLINE       0     0     0</span><br><span class="line">          mirror-18  ONLINE       0     0     0</span><br><span class="line">            sdam     ONLINE       0     0     0</span><br><span class="line">            sdan     ONLINE       0     0     0</span><br></pre></td></tr></table></figure>

<h3 id="Attach-one-hdd-to-mirror"><a href="#Attach-one-hdd-to-mirror" class="headerlink" title="Attach one hdd to mirror"></a>Attach one hdd to mirror</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zpool attach tank sdal sdz</span><br><span class="line">          mirror-17  ONLINE       0     0     0</span><br><span class="line">            sdal     ONLINE       0     0     0</span><br><span class="line">            sdz      ONLINE       0     0     0  (resilvering)</span><br></pre></td></tr></table></figure>

<h3 id="sub-zpool"><a href="#sub-zpool" class="headerlink" title="sub zpool"></a>sub zpool</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zfs create tank/<span class="built_in">test</span></span><br></pre></td></tr></table></figure>

<h3 id="Quota"><a href="#Quota" class="headerlink" title="Quota"></a>Quota</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> quota=1024G tank/<span class="built_in">test</span></span><br><span class="line"><span class="comment"># disable</span></span><br><span class="line">zfs <span class="built_in">set</span> quota=none tank</span><br></pre></td></tr></table></figure>

<h3 id="user-quota"><a href="#user-quota" class="headerlink" title="user quota"></a>user quota</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> userquota@nagios=2G tank</span><br><span class="line">dd <span class="keyword">if</span>=/dev/zero of=/tank01/<span class="built_in">test</span>/nagios bs=1M</span><br><span class="line">dd: writing /tank/<span class="built_in">test</span>/nagios:Disk quota exceeded</span><br><span class="line"></span><br><span class="line"><span class="comment"># zfs get userquota@nagios</span></span><br><span class="line">NAME            PROPERTY          VALUE             SOURCE</span><br><span class="line">tank01          userquota@nagios  2G                <span class="built_in">local</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># group</span></span><br><span class="line">zfs <span class="built_in">set</span> groupquota@staff=10G tank/staff/admins</span><br></pre></td></tr></table></figure>

<h3 id="Add-and-remove-hotspare"><a href="#Add-and-remove-hotspare" class="headerlink" title="Add and remove hotspare"></a>Add and remove hotspare</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zpool add tank spare c1t4d0 c1t5d0</span><br><span class="line">zpool remove tank c1t5d0</span><br></pre></td></tr></table></figure>

<h3 id="Clear-zfs-label"><a href="#Clear-zfs-label" class="headerlink" title="Clear zfs label"></a><a target="_blank" rel="noopener" href="https://icesquare.com/wordpress/freebsdhow-to-remove-zfs-meta-data/">Clear zfs label</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ zpool labelclear &#x2F;dev&#x2F;sdxxx</span><br><span class="line">or</span><br><span class="line">$ dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;dev&#x2F;sdXX bs&#x3D;512 count&#x3D;10</span><br><span class="line">$ dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;dev&#x2F;sdXX bs&#x3D;512 seek&#x3D;$(( $(blockdev --getsz &#x2F;dev&#x2F;sdXX) - 4096 ))</span><br></pre></td></tr></table></figure>

<h3 id="Auto-rebuild-in-0-7-x"><a href="#Auto-rebuild-in-0-7-x" class="headerlink" title="Auto rebuild in 0.7.x"></a>Auto rebuild in 0.7.x</h3><p><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/issues/2449">Autoreplace not working</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zpool <span class="built_in">set</span> autoreplace=on tank</span><br></pre></td></tr></table></figure>

<h3 id="Show-status"><a href="#Show-status" class="headerlink" title="Show status"></a>Show status</h3><p>Mapping /dev/shm/zil_cache to loop0p1</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line">$ dd <span class="keyword">if</span>=/dev/zero of=/dev/shm/zil_cache bs=1M count=4096</span><br><span class="line">$ losetup -v -f /dev/shm/zil_cache</span><br><span class="line">$ losetup -a</span><br><span class="line">/dev/loop0: [0018]:55156303 (/dev/shm/zil_cache)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add zil by mirror</span></span><br><span class="line">$ zpool add tank <span class="built_in">log</span> mirror /dev/loop0p1 /dev/loop0p2, because it <span class="string">&#x27;s test file from /dev/shm</span></span><br><span class="line"><span class="string">$ zpool add tank log /dev/loop0p1</span></span><br><span class="line"><span class="string">$ zpool iostat -v 2</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">pool                                            alloc   free   read  write   read  write</span></span><br><span class="line"><span class="string">----------------------------------------------  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">tank                                            24.4G  1.30T      0  8.74K      0   575M</span></span><br><span class="line"><span class="string">  scsi-3600605b005811bf01db36f957f619f26-part1  24.4G  1.30T      0  5.50K      0   448M</span></span><br><span class="line"><span class="string">logs                                                -      -      -      -      -      -</span></span><br><span class="line"><span class="string">  loop0p1                                        102M  3.87G      0  3.23K      0   127M</span></span><br><span class="line"><span class="string">----------------------------------------------  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ zpool iostat -r 1</span></span><br><span class="line"><span class="string">tank_7         sync_read    sync_write    async_read    async_write      scrub</span></span><br><span class="line"><span class="string">req_size      ind    agg    ind    agg    ind    agg    ind    agg    ind    agg</span></span><br><span class="line"><span class="string">----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">512             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">1K              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">2K              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">4K              0      0      8      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">8K              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">16K             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">32K             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">64K             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">128K           63      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">256K            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">512K            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">1M              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">2M              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">4M              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">8M              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">16M             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">--------------------------------------------------------------------------------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ zpool iostat -L -v -q 2</span></span><br><span class="line"><span class="string">              capacity     operations     bandwidth    syncq_read    syncq_write   asyncq_read  asyncq_write   scrubq_read   trimq_write</span></span><br><span class="line"><span class="string">pool        alloc   free   read  write   read  write   pend  activ   pend  activ   pend  activ   pend  activ   pend  activ   pend  activ</span></span><br><span class="line"><span class="string">----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">tank         161G  90.8T      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">  raidz2     161G  90.8T      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sda         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdb         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdc         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdd         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sde         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdf         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdg         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdi         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdk         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdl         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ zpool iostat -w 2</span></span><br><span class="line"><span class="string">tank         total_wait     disk_wait    syncq_wait    asyncq_wait</span></span><br><span class="line"><span class="string">latency      read  write   read  write   read  write   read  write  scrub   trim</span></span><br><span class="line"><span class="string">----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">1ns             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">3ns             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">7ns             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">15ns            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">31ns            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">63ns            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">127ns           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">255ns           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">511ns           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">1us             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">2us             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">4us             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">8us             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">16us            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">32us            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">65us            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">131us           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">262us           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">524us           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">1ms             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">2ms             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">4ms             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">8ms             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">16ms            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">33ms            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">67ms            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">134ms           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">268ms           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">536ms           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">1s              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">2s              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">4s              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">8s              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">17s             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">34s             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">68s             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">137s            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">--------------------------------------------------------------------------------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ cat /proc/spl/kstat/zfs/ost_86/dmu_tx_assign</span></span><br><span class="line"><span class="string">75 1 0x01 41 1968 259828173343194 262983153792140</span></span><br><span class="line"><span class="string">name                            type data</span></span><br><span class="line"><span class="string">1 ns                            4    0</span></span><br><span class="line"><span class="string">2 ns                            4    0</span></span><br><span class="line"><span class="string">4 ns                            4    0</span></span><br><span class="line"><span class="string">8 ns                            4    0</span></span><br><span class="line"><span class="string">16 ns                           4    0</span></span><br><span class="line"><span class="string">32 ns                           4    0</span></span><br><span class="line"><span class="string">64 ns                           4    0</span></span><br><span class="line"><span class="string">128 ns                          4    40</span></span><br><span class="line"><span class="string">256 ns                          4    40</span></span><br><span class="line"><span class="string">512 ns                          4    6</span></span><br><span class="line"><span class="string">1024 ns                         4    1</span></span><br><span class="line"><span class="string">2048 ns                         4    0</span></span><br><span class="line"><span class="string">4096 ns                         4    0</span></span><br><span class="line"><span class="string">8192 ns                         4    0</span></span><br><span class="line"><span class="string">16384 ns                        4    0</span></span><br><span class="line"><span class="string">32768 ns                        4    0</span></span><br><span class="line"><span class="string">65536 ns                        4    5</span></span><br><span class="line"><span class="string">131072 ns                       4    105</span></span><br><span class="line"><span class="string">262144 ns                       4    331</span></span><br><span class="line"><span class="string">524288 ns                       4    419</span></span><br><span class="line"><span class="string">1048576 ns                      4    325</span></span><br><span class="line"><span class="string">2097152 ns                      4    338</span></span><br><span class="line"><span class="string">4194304 ns                      4    188</span></span><br><span class="line"><span class="string">8388608 ns                      4    65</span></span><br><span class="line"><span class="string">16777216 ns                     4    59</span></span><br><span class="line"><span class="string">33554432 ns                     4    47</span></span><br><span class="line"><span class="string">67108864 ns                     4    34</span></span><br><span class="line"><span class="string">134217728 ns                    4    60</span></span><br><span class="line"><span class="string">268435456 ns                    4    55</span></span><br><span class="line"><span class="string">536870912 ns                    4    65</span></span><br><span class="line"><span class="string">1073741824 ns                   4    94</span></span><br><span class="line"><span class="string">2147483648 ns                   4    56</span></span><br><span class="line"><span class="string">4294967296 ns                   4    271</span></span><br><span class="line"><span class="string">8589934592 ns                   4    3454</span></span><br><span class="line"><span class="string">17179869184 ns                  4    4687</span></span><br><span class="line"><span class="string">34359738368 ns                  4    1970</span></span><br><span class="line"><span class="string">68719476736 ns                  4    642</span></span><br><span class="line"><span class="string">137438953472 ns                 4    380</span></span><br><span class="line"><span class="string">274877906944 ns                 4    440</span></span><br><span class="line"><span class="string">549755813888 ns                 4    299</span></span><br><span class="line"><span class="string">1099511627776 ns                4    36</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ zpool iostat -v -y -l 2</span></span><br><span class="line"><span class="string">tank                           146T  13.8T  1.46K    345  49.0M   319K   24ms    2ms   24ms    1ms  741ns  627ns  610ns    1ms      -</span></span><br><span class="line"><span class="string">  raidz3                       146T  13.8T  1.46K    345  49.0M   319K   24ms    2ms   24ms    1ms  741ns  627ns  610ns    1ms      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a66cec97        -      -     88     18  2.76M  15.6K    8ms  840us    8ms  442us  664ns  474ns  758ns  407us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670caef        -      -     93     18  2.93M  15.1K   68ms   11ms   68ms   10ms  919ns    1us  758ns  281us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a65a02fb        -      -     73     18  2.40M  14.8K   16ms  514us   16ms  335us  714ns  758ns  568ns  239us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670bbc7        -      -     79     17  2.74M  13.8K   10ms  556us   10ms  334us  679ns  474ns  568ns  244us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a6754b03        -      -     70     20  2.36M  15.6K   26ms  679us   26ms  360us  766ns  474ns  758ns  329us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a675ae43        -      -     88     19  2.89M  15.1K   60ms    5ms   60ms    5ms  802ns  985ns  758ns  366us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670c787        -      -     82     20  2.61M  16.8K   15ms  833us   15ms  468us  713ns  379ns  379ns  394us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a6754d7b        -      -     97     25  3.05M  43.9K    8ms   14ms    8ms    6ms  710ns  379ns  379ns    9ms      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a67540d7        -      -     74     19  2.50M  18.3K   23ms  796us   23ms  466us  727ns  474ns  379ns  371us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a675455b        -      -     83     19  2.79M  17.3K   32ms  687us   32ms  393us  760ns  474ns  758ns  315us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670ca8f        -      -     71     19  2.38M  17.0K    9ms  771us    9ms  398us  644ns  474ns      -  361us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a65b55bb        -      -     89     18  2.84M  16.0K    8ms  909us    8ms  567us  779ns  682ns      -  329us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a65a7f57        -      -     82     20  2.60M  17.8K   32ms  527us   32ms  318us  744ns  695ns      -  289us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a65b2683        -      -     92     17  2.99M  15.3K   57ms  687us   57ms  443us  863ns    1us  758ns  242us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a675449f        -      -     74     16  2.55M  16.0K   11ms  625us   11ms  425us  633ns  474ns  379ns  257us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670c30f        -      -     82     16  2.82M  15.6K    9ms  808us    9ms  539us  680ns  379ns  758ns  363us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670af17        -      -     77     19  2.65M  17.5K    8ms  876us    8ms  458us  695ns  474ns  758ns  380us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670c2ab        -      -     95     18  3.10M  17.8K   18ms  786us   18ms  498us  770ns  758ns  379ns  333us      -</span></span><br><span class="line"><span class="string">----------------------------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br></pre></td></tr></table></figure>

<h3 id="Custom-script-show-temperature"><a href="#Custom-script-show-temperature" class="headerlink" title="Custom script ,show temperature"></a>Custom script ,show temperature</h3><p>/etc/zfs/zpool.d</p>
<p>$  ZPOOL_SCRIPTS_AS_ROOT=1 zpool status -c temp<br>  pool: tank<br> state: ONLINE<br>  scan: scrub repaired 0B in 0 days 06:46:11 with 0 errors on Tue Oct 15 06:46:14 2019<br>config:</p>
<pre><code>    NAME        STATE     READ WRITE CKSUM  temp
    tank        ONLINE       0     0     0
      raidz2-0  ONLINE       0     0     0
        sdc     ONLINE       0     0     0    27
        sdd     ONLINE       0     0     0    28
        sde     ONLINE       0     0     0    27
        sdf     ONLINE       0     0     0    29
        sdg     ONLINE       0     0     0    27
        sdh     ONLINE       0     0     0    28
    logs
      mirror-1  ONLINE       0     0     0
        sda3    ONLINE       0     0     0    24
        sdb3    ONLINE       0     0     0    25
    cache
      sda4      ONLINE       0     0     0    24
      sdb4      ONLINE       0     0     0    25</code></pre>
<p>errors: No known data errors</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### [Tuning for Scrubs and Resilvers](http:&#x2F;&#x2F;broken.net&#x2F;uncategorized&#x2F;zfs-performance-tuning-for-scrubs-and-resilvers&#x2F;)</span><br><span class="line">Prioritize resilvering by setting the delay to zero</span><br><span class="line">Idle window in clock ticks</span><br><span class="line">* set zfs:zfs_resilver_delay &#x3D; 0</span><br><span class="line"></span><br><span class="line">Prioritize scrubs by setting the delay to zero</span><br><span class="line">Number of ticks to delay scrub</span><br><span class="line">* set zfs:zfs_scrub_delay &#x3D; 0</span><br><span class="line"></span><br><span class="line">Maximum number of scrub I&#x2F;O per top-level vdev, by default 32. Increases zfs scrub speed. (at what cost, no idea)</span><br><span class="line">set maximum number of inflight IOs to a reasonable value - this number will vary for your environment</span><br><span class="line">* set zfs:zfs_top_maxinflight &#x3D; 512</span><br><span class="line"></span><br><span class="line">zfs_scan_min_time_ms:Min millisecs to scrub per txg (int)</span><br><span class="line">resilver for five seconds per TXG</span><br><span class="line">* set zfs:zfs_resilver_min_time_ms &#x3D; 5000</span><br><span class="line"></span><br><span class="line">The resilvers or scrub speed increase from 50MB&#x2F;s to 800MB&#x2F;s, increase a lot</span><br><span class="line"></span><br><span class="line">### zdb</span><br><span class="line">&#96;&#96;&#96;bash</span><br><span class="line">zdb -MM -P test_0</span><br><span class="line">        vdev          0         metaslabs  116          fragmentation 88%</span><br><span class="line">                          9: 31097798 ***********************</span><br><span class="line">                         10: 33383938 ************************</span><br><span class="line">                         11: 56520516 ****************************************</span><br><span class="line">                         12: 53907990 ***************************************</span><br><span class="line">                         13: 11130025 ********</span><br><span class="line">                         14: 4519568 ****</span><br><span class="line">                         15: 2751108 **</span><br><span class="line">                         16: 179324 *</span><br><span class="line">                         17:   2534 *</span><br><span class="line">                         18:    138 *</span><br><span class="line">                         19:     16 *</span><br><span class="line">                         20:      3 *</span><br><span class="line">                         21:      2 *</span><br><span class="line">        pool test_0     fragmentation    88%</span><br><span class="line">                          9: 31097798 ***********************</span><br><span class="line">                         10: 33383938 ************************</span><br><span class="line">                         11: 56520516 ****************************************</span><br><span class="line">                         12: 53907990 ***************************************</span><br><span class="line">                         13: 11130025 ********</span><br><span class="line">                         14: 4519568 ****</span><br><span class="line">                         15: 2751108 **</span><br><span class="line">                         16: 179324 *</span><br><span class="line">                         17:   2534 *</span><br><span class="line">                         18:    138 *</span><br><span class="line">                         19:     16 *</span><br><span class="line">                         20:      3 *</span><br><span class="line">                         21:      2 *</span><br><span class="line"></span><br><span class="line">$ zdb -b test_0</span><br><span class="line"></span><br><span class="line">Traversing all blocks to verify nothing leaked ...</span><br><span class="line"></span><br><span class="line">loading space map for vdev 0 of 1, metaslab 115 of 116 ...</span><br><span class="line">39.0G completed (1569MB&#x2F;s) estimated time remaining: 0hr 30min 43sec</span><br><span class="line">2.80T completed ( 553MB&#x2F;s) estimated time remaining: 0hr 00min 00sec</span><br><span class="line">        No leaks (block sum matches space maps exactly)</span><br><span class="line"></span><br><span class="line">        bp count:      1154215521</span><br><span class="line">        ganged count:           0</span><br><span class="line">        bp logical:    3028205925888      avg:   2623</span><br><span class="line">        bp physical:   2695917303296      avg:   2335     compression:   1.12</span><br><span class="line">        bp allocated:  3076338462208      avg:   2665     compression:   0.98</span><br><span class="line">        bp deduped:             0    ref&gt;1:      0   deduplication:   1.00</span><br><span class="line">        SPA allocated: 3076338462208     used: 77.18%</span><br><span class="line"></span><br><span class="line">        additional, non-pointer bps of type 0:         40</span><br><span class="line">        Dittoed blocks on same vdev: 590073873</span><br></pre></td></tr></table></figure>

<p>ZDB Get file info</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line">dd <span class="keyword">if</span>=/dev/zero of=/tank/<span class="built_in">test</span>/1 bs=1M count=1</span><br><span class="line">zdb -vv -bbbb -O tank/<span class="built_in">test</span> test_1</span><br><span class="line">zdb -ddddd tank/<span class="built_in">test</span></span><br><span class="line">......</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L0 0:86bef7d8400:600 4000L/200P F=1 B=11629333/11629333</span><br><span class="line">            4000 L0 0:867fb11c400:1800 4000L/e00P F=1 B=11629333/11629333</span><br><span class="line"></span><br><span class="line">                segment [0000000000000000, 0000000000008000) size   32K</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  <span class="built_in">type</span></span><br><span class="line">       256    2   128K   128K  1.00M     512     1M  100.00  ZFS plain file</span><br><span class="line">                                               168   bonus  System attributes</span><br><span class="line">        dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED</span><br><span class="line">        dnode maxblkid: 7</span><br><span class="line">        path    /1</span><br><span class="line">        uid     0</span><br><span class="line">        gid     0</span><br><span class="line">        atime   Tue Dec 25 22:25:42 2018</span><br><span class="line">        mtime   Tue Dec 25 22:25:42 2018</span><br><span class="line">        ctime   Tue Dec 25 22:25:42 2018</span><br><span class="line">        crtime  Tue Dec 25 22:25:42 2018</span><br><span class="line">        gen     11630673</span><br><span class="line">        mode    100644</span><br><span class="line">        size    1048576</span><br><span class="line">        parent  34</span><br><span class="line">        links   1</span><br><span class="line">        pflags  40800000004</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L1  0:86d5203a600:c00 20000L/400P F=8 B=11630673/11630673</span><br><span class="line">               0  L0 0:86e648c9400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           20000  L0 0:86e648f9400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           40000  L0 0:86e64929400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           60000  L0 0:86e64959400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           80000  L0 0:86e64989400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           a0000  L0 0:86e649b9400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           c0000  L0 0:86e649e9400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           e0000  L0 0:86e64a19400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line"></span><br><span class="line">                segment [0000000000000000, 0000000000100000) size    1M</span><br><span class="line"></span><br><span class="line">    Dnode slots:</span><br><span class="line">        Total used:             8</span><br><span class="line">        Max used:             256</span><br><span class="line">        Percent empty:  96.875000</span><br><span class="line"></span><br><span class="line">$ ls -l /tank/backup/test100/test.c</span><br><span class="line">-rw-r--r-- 1 root root 739 Mar  4 08:50 /tank/backup/test100/test.c</span><br><span class="line">$ zdb -vv -O tank/backup test100/test.c</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  <span class="built_in">type</span></span><br><span class="line">   5526595    1   128K     1K     9K     512     1K  100.00  ZFS plain file</span><br><span class="line">                                               176   bonus  System attributes</span><br><span class="line">	dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED</span><br><span class="line">	dnode maxblkid: 0</span><br><span class="line">	uid     0</span><br><span class="line">	gid     0</span><br><span class="line">	atime	Wed Mar  4 08:50:52 2020</span><br><span class="line">	mtime	Wed Mar  4 08:50:50 2020</span><br><span class="line">	ctime	Wed Mar  4 08:50:50 2020</span><br><span class="line">	crtime	Wed Mar  4 08:50:50 2020</span><br><span class="line">	gen	1668705</span><br><span class="line">	mode	100644</span><br><span class="line">	size	739</span><br><span class="line">	parent	43</span><br><span class="line">	links	1</span><br><span class="line">	pflags	840800000004</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L0 0:328e967a2000:3000 400L/400P F=1 B=1668705/1668705</span><br><span class="line"></span><br><span class="line">		segment [0000000000000000, 0000000000000400) size    1K</span><br><span class="line"></span><br><span class="line">zdb -vv -bbbb -O tank/backup test100/test.c</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  <span class="built_in">type</span></span><br><span class="line">   5526595    1   128K     1K     9K     512     1K  100.00  ZFS plain file</span><br><span class="line">                                               176   bonus  System attributes</span><br><span class="line">	dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED</span><br><span class="line">	dnode maxblkid: 0</span><br><span class="line">	uid     0</span><br><span class="line">	gid     0</span><br><span class="line">	atime	Wed Mar  4 08:50:52 2020</span><br><span class="line">	mtime	Wed Mar  4 08:50:50 2020</span><br><span class="line">	ctime	Wed Mar  4 08:50:50 2020</span><br><span class="line">	crtime	Wed Mar  4 08:50:50 2020</span><br><span class="line">	gen	1668705</span><br><span class="line">	mode	100644</span><br><span class="line">	size	739</span><br><span class="line">	parent	43</span><br><span class="line">	links	1</span><br><span class="line">	pflags	840800000004</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L0 DVA[0]=&lt;0:328e967a2000:3000&gt; [L0 ZFS plain file] fletcher4 uncompressed unencrypted LE contiguous unique single size=400L/400P birth=1668705L/1668705P fill=1 cksum=36e170f019:231ac0e9e639:c8f24370cf5ff:340704ee1157fd0</span><br><span class="line"></span><br><span class="line">		segment [0000000000000000, 0000000000000400) size    1K</span><br><span class="line"></span><br><span class="line"><span class="comment">## Got the data block</span></span><br><span class="line">$ zdb -R tank/backup 0:328e967a2000:3000</span><br><span class="line">Found vdev <span class="built_in">type</span>: raidz</span><br><span class="line"></span><br><span class="line">0:328e967a2000:3000</span><br><span class="line">          0 1 2 3 4 5 6 7   8 9 a b c d e f  0123456789abcdef</span><br><span class="line">000000:  23696e636c756465  203c737464696f2e  <span class="comment">#include &lt;stdio.</span></span><br><span class="line">000010:  683e0a23696e636c  756465203c737973  h&gt;.<span class="comment">#include &lt;sys</span></span><br><span class="line">000020:  2f73686d2e683e0a  23696e636c756465  /shm.h&gt;.<span class="comment">#include</span></span><br><span class="line">000030:  203c7379732f7374  61742e683e0a2369   &lt;sys/stat.h&gt;.<span class="comment">#i</span></span><br><span class="line">000040:  6e636c756465203c  7379732f6d6d616e  nclude &lt;sys/mman</span><br><span class="line">000050:  2e683e0a23696e63  6c756465203c6663  .h&gt;.<span class="comment">#include &lt;fc</span></span><br><span class="line">000060:  6e746c2e683e0a23  696e636c75646520  ntl.h&gt;.<span class="comment">#include</span></span><br><span class="line">000070:  3c7374646c69622e  683e0a23696e636c  &lt;stdlib.h&gt;.<span class="comment">#incl</span></span><br><span class="line">000080:  756465203c737472  696e672e683e0a23  ude &lt;string.h&gt;.<span class="comment">#</span></span><br><span class="line">000090:  696e636c75646520  3c756e697374642e  include &lt;unistd.</span><br><span class="line">0000a0:  683e0a0a696e7420  6d61696e28696e74  h&gt;..int main(int</span><br><span class="line">0000b0:  20617267632c2063  686172202a2a6172   argc, char **ar</span><br><span class="line">......</span><br><span class="line">002fd0:  0000000000000000  0000000000000000  ................</span><br><span class="line">002fe0:  0000000000000000  0000000000000000  ................</span><br><span class="line">002ff0:  0000000000000000  0000000000000000  ................</span><br></pre></td></tr></table></figure>
<p>0:86e648c9400:30000 20000L/20000P F=1 B=11630673/11630673<br>vdev:offset:size</p>
<p>vdev=0<br>offset=86e648c9400 (offset in logic disk ?)</p>
<p>20000L/20000P means size,  compressed size(2000L),the no compression file size(2000P)<br>0x2000=131072</p>
<p>B=11630673/11630673 means write operate transaction group</p>
<h3 id="performance-tuning"><a href="#performance-tuning" class="headerlink" title="performance tuning"></a>performance tuning</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># number of z_wr_iss processes tunable</span></span><br><span class="line">zio_taskq_batch_pct</span><br><span class="line">(The bad setting will casue the performance issue, Many thanks <span class="keyword">for</span> Javen <span class="string">&#x27;s help)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">core=1</span></span><br><span class="line"><span class="string">[[ $(grep MHz /proc/cpuinfo -c) -gt 8 ]] &amp;&amp; maxcpu=8 &amp;&amp; for i in $(pgrep &#x27;</span>z_wr_iss$<span class="string">&#x27;)</span></span><br><span class="line"><span class="string">do</span></span><br><span class="line"><span class="string">   [[ $core -gt $maxcpu ]] &amp;&amp; core=0</span></span><br><span class="line"><span class="string">   taskset -p -c $core $i</span></span><br><span class="line"><span class="string">   ((core++))</span></span><br><span class="line"><span class="string">done</span></span><br></pre></td></tr></table></figure>

<h3 id="check-your-zpool-could-be-imported"><a href="#check-your-zpool-could-be-imported" class="headerlink" title="check your zpool could be imported"></a>check your zpool could be imported</h3><p>running it in another server(not import the pool), you can see can be import or can not be import<br>make sure the MMP was worked</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">$ zpool import</span><br><span class="line"></span><br><span class="line"><span class="comment">## export the zpool or you have antoher server connect this zpool</span></span><br><span class="line">$ zdb -ec <span class="variable">$poolname</span></span><br><span class="line">Traversing all blocks to verify metadata checksums and verify nothing leaked ...</span><br><span class="line"></span><br><span class="line">loading space map <span class="keyword">for</span> vdev 0 of 1, metaslab 1 of 139 ...</span><br><span class="line"></span><br><span class="line">$ zdb -ec -AAA <span class="variable">$poolname</span></span><br><span class="line"><span class="comment"># if there is &quot;assertion failure&quot;...</span></span><br><span class="line"><span class="comment"># -AAA could not abort</span></span><br><span class="line">Configuration <span class="keyword">for</span> import:</span><br><span class="line">        vdev_children: 1</span><br><span class="line">        version: 5000</span><br><span class="line">        pool_guid: 1548875728334022114</span><br><span class="line">        name: <span class="string">&#x27;mdt_0&#x27;</span></span><br><span class="line">        state: 0</span><br><span class="line">        hostid: 4281985155</span><br><span class="line">        hostname: <span class="string">&#x27;cngb-mds-m20-1&#x27;</span></span><br><span class="line">        vdev_tree:</span><br><span class="line">            <span class="built_in">type</span>: <span class="string">&#x27;root&#x27;</span></span><br><span class="line">            id: 0</span><br><span class="line">            guid: 1548875728334022114</span><br><span class="line">            children[0]:</span><br><span class="line">                <span class="built_in">type</span>: <span class="string">&#x27;raidz&#x27;</span></span><br><span class="line">                id: 0</span><br><span class="line">                guid: 1680148767042285697</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ zdb -e -AAA <span class="variable">$poolname</span> &gt; <span class="variable">$poolnam</span></span><br><span class="line">Configuration <span class="keyword">for</span> import:</span><br><span class="line">        vdev_children: 1</span><br><span class="line">        version: 5000</span><br><span class="line">        pool_guid: 1548875728334022114</span><br><span class="line">        name: <span class="string">&#x27;mdt_0&#x27;</span></span><br><span class="line">        state: 0</span><br><span class="line">        hostid: 4281985155</span><br><span class="line">        hostname: <span class="string">&#x27;cngb-mds-m20-1&#x27;</span></span><br><span class="line">        vdev_tree:</span><br><span class="line">            <span class="built_in">type</span>: <span class="string">&#x27;root&#x27;</span></span><br><span class="line">            id: 0</span><br><span class="line">            guid: 1548875728334022114</span><br><span class="line">            children[0]:</span><br><span class="line">                <span class="built_in">type</span>: <span class="string">&#x27;raidz&#x27;</span></span><br><span class="line">                id: 0</span><br><span class="line">                guid: 1680148767042285697</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ zdb -em -AAA <span class="variable">$poolname</span> <span class="comment"># to check metalab</span></span><br><span class="line">Metaslabs:</span><br><span class="line">        vdev          0</span><br><span class="line">        metaslabs   139   offset                spacemap          free</span><br><span class="line">        ---------------   -------------------   ---------------   -------------</span><br><span class="line">        metaslab      0   offset            0   spacemap     39   free    27.6G</span><br><span class="line">        metaslab      1   offset    800000000   spacemap     55   free    31.1G</span><br><span class="line">        metaslab      2   offset   1000000000   spacemap     58   free    30.3G</span><br><span class="line">        metaslab      3   offset   1800000000   spacemap     42   free    29.4G</span><br><span class="line">        metaslab      4   offset   2000000000   spacemap     45   free    28.3G</span><br><span class="line"></span><br><span class="line"><span class="comment">#metaslab check</span></span><br></pre></td></tr></table></figure>

<h3 id="zpool-status"><a href="#zpool-status" class="headerlink" title="zpool status"></a>zpool status</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ zpool status -x -v</span><br><span class="line">$ zpool status -v</span><br><span class="line"></span><br><span class="line">errors: Permanent errors have been detected <span class="keyword">in</span> the following files:</span><br><span class="line"></span><br><span class="line">        tank/mdt_0:/oi.1/0x1:0x1fcc1:0x0</span><br><span class="line">        tank/mdt_0:/oi.1/0x1:0x1fcc5:0x0</span><br><span class="line">        tank/mdt_0:/oi.1/0x1:0x1fccc:0x0</span><br><span class="line">        tank/mdt_0:/oi.1/0x1:0x1fcc2:0x0</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Disable-AVX512-for-scalable-Xeon-silver-and-gold-5xxx"><a href="#Disable-AVX512-for-scalable-Xeon-silver-and-gold-5xxx" class="headerlink" title="Disable AVX512 for scalable Xeon silver and gold 5xxx"></a>Disable AVX512 for scalable Xeon silver and gold 5xxx</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Gold 5115</span></span><br><span class="line">$ cat /proc/spl/kstat/zfs/fletcher_4_bench</span><br><span class="line">0 0 0x01 -1 0 43875755056 2388098231282525</span><br><span class="line">implementation   native         byteswap</span><br><span class="line">scalar           5097645469     4106654089</span><br><span class="line">superscalar      6876819423     5086086480</span><br><span class="line">superscalar4     5926587517     4946863265</span><br><span class="line">sse2             11659567916    6572225291</span><br><span class="line">ssse3            11660661610    10379355285</span><br><span class="line">avx2             17865482202    16120833783</span><br><span class="line">avx512f          24802818495    8841728748</span><br><span class="line">fastest          avx512f        avx2</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> ssse3 &gt; /sys/module/zcommon/parameters/zfs_fletcher_4_impl</span><br><span class="line"><span class="built_in">echo</span> ssse3 &gt; /sys/module/zfs/parameters/zfs_vdev_raidz_impl</span><br></pre></td></tr></table></figure>
<p>Because I ‘m wrong about when avx2 or avx512 called, The cpu will reduce the frequency and it will impact the others (no avx system call) at the same time</p>
<p><code>In some fio mix IO case, I can &#39;t beleive the avx2 lower than 4% than the ssse3, the CPU was 1x Xeon E2136 </code></p>
<p><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/wiki/ZFS-on-Linux-Module-Parameters#zfs_vdev_raidz_impl">zfs_vdev_raidz_impl</a><br><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/wiki/ZFS-on-Linux-Module-Parameters#zfs_fletcher_4_impl">zfs_fletcher_4_impl</a></p>
<p>or you could disable AVX by kernel parameter</p>
<h1 id="not-make-sure"><a href="#not-make-sure" class="headerlink" title="not make sure"></a>not make sure</h1><h3 id="Too-bad-about-zfs-dracut"><a href="#Too-bad-about-zfs-dracut" class="headerlink" title="Too bad about zfs-dracut"></a>Too bad about zfs-dracut</h3><p>In zfs old version 0.6.x, there is no multihost parameter.<br>When you upgrade to 0.7.x, it ‘s not enable by default !!!  but you can see the mmp history is worked !!!<br>So if you zfs-dracut , maybe you will be brain-split, lsinitrd show hostid == 0 !!!<br>My version is 0.7.9-1</p>
<p>##not make sure<br>lsinitrd</p>
<p>Thanks Javen Wu save me again…</p>
<p>You could unistall the package or remove it , zfs dracut is too dangerous for HA arch</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">rpm -ql zfs-dracut-0.7.9-1.el7.x86_64</span><br><span class="line">/usr/lib/dracut/modules.d/02zfsexpandknowledge</span><br><span class="line">/usr/lib/dracut/modules.d/02zfsexpandknowledge/module-setup.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/export-zfs.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/module-setup.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/mount-zfs.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/parse-zfs.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/zfs-generator.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/zfs-lib.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/zfs-needshutdown.sh</span><br><span class="line">/usr/share/doc/zfs-dracut-0.7.9</span><br><span class="line">/usr/share/doc/zfs-dracut-0.7.9/README.dracut.markdown</span><br></pre></td></tr></table></figure>

<h3 id="ZFS-0-7-x-MMP-cause-too-many-issues-the-ZFS-0-6-5-more-stable"><a href="#ZFS-0-7-x-MMP-cause-too-many-issues-the-ZFS-0-6-5-more-stable" class="headerlink" title="ZFS 0.7.x MMP cause too many issues, the ZFS 0.6.5 more stable"></a>ZFS 0.7.x MMP cause too many issues, the ZFS 0.6.5 more stable</h3><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/issues/7834">Performance impact</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/issues/7731">Zpool import slow</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/issues/7709">Disk fail cause zpool suspend</a><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/pull/8495">2</a></li>
</ul>
</li>
</ul>
<h3 id="Resolved-import-too-slow"><a href="#Resolved-import-too-slow" class="headerlink" title="Resolved import too slow"></a>Resolved import too slow</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ cat /sys/module/zfs/parameters/zfs_multihost_import_intervals</span><br><span class="line">10</span><br><span class="line">$ <span class="built_in">echo</span> 1 &gt; /sys/module/zfs/parameters/zfs_multihost_import_intervals</span><br><span class="line">$ zpool import -a</span><br><span class="line">$ <span class="built_in">echo</span> 10 &gt; /sys/module/zfs/parameters/zfs_multihost_import_intervals</span><br><span class="line"></span><br><span class="line"> * Don<span class="string">&#x27;t allow fail_intervals larger than import_intervals</span></span><br><span class="line"><span class="string"># after import ,switch the parameters</span></span><br><span class="line"><span class="string">(sleep 300; echo 100 &gt;  /sys/module/zfs/parameters/zfs_multihost_fail_intervals;echo 200 &gt; /sys/module/zfs/parameters/zfs_multihost_import_intervals ;echo 6000 &gt; /sys/module/zfs/parameters/zfs_multihost_interval) &amp;</span></span><br></pre></td></tr></table></figure>

<h3 id="Enable-zfs-message-log"><a href="#Enable-zfs-message-log" class="headerlink" title="Enable zfs message log"></a>Enable zfs message log</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> 1 &gt;/sys/module/zfs/parameters/zfs_dbgmsg_enable</span><br><span class="line">$ cat /proc/spl/kstat/zfs/dbgmsg</span><br></pre></td></tr></table></figure>

<h3 id="Remove-slog"><a href="#Remove-slog" class="headerlink" title="Remove slog"></a>Remove slog</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## slog device has been destroy, skip/bypass import slog</span></span><br><span class="line">$ zpool import -m -a -f</span><br><span class="line"></span><br><span class="line">$ zpool status -x</span><br><span class="line">  pool: tank</span><br><span class="line"> state: DEGRADED</span><br><span class="line">status: One or more devices could not be used because the label is missing or</span><br><span class="line">        invalid.  Sufficient replicas exist <span class="keyword">for</span> the pool to <span class="built_in">continue</span></span><br><span class="line">        functioning <span class="keyword">in</span> a degraded state.</span><br><span class="line">action: Replace the device using <span class="string">&#x27;zpool replace&#x27;</span>.</span><br><span class="line">   see: http://zfsonlinux.org/msg/ZFS-8000-4J</span><br><span class="line">  scan: scrub repaired 0B <span class="keyword">in</span> 0 days 06:46:11 with 0 errors on Mon Oct 14 18:46:14 2019</span><br><span class="line">config:</span><br><span class="line"></span><br><span class="line">        NAME                      STATE     READ WRITE CKSUM</span><br><span class="line">        tank                      DEGRADED     0     0     0</span><br><span class="line">          raidz2-0                ONLINE       0     0     0</span><br><span class="line">            sdc                   ONLINE       0     0     0</span><br><span class="line">            sdd                   ONLINE       0     0     0</span><br><span class="line">            sde                   ONLINE       0     0     0</span><br><span class="line">            sdf                   ONLINE       0     0     0</span><br><span class="line">            sdg                   ONLINE       0     0     0</span><br><span class="line">            sdh                   ONLINE       0     0     0</span><br><span class="line">        logs</span><br><span class="line">          mirror-1                UNAVAIL      0     0     0  insufficient replicas</span><br><span class="line">            13378777246457412929  UNAVAIL      0     0     0  was /dev/sda3</span><br><span class="line">            1725913437883392852   UNAVAIL      0     0     0  was /dev/sdb</span><br><span class="line"></span><br><span class="line">$ zpool remove tank mirror-1</span><br><span class="line">that ok</span><br></pre></td></tr></table></figure>

<h3 id="About-SMR-HDD"><a href="#About-SMR-HDD" class="headerlink" title="About SMR HDD"></a><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/issues/4877">About SMR HDD</a></h3><p>Another thing you might try is to leave the block size set at 1M but increase the zfs_vdev_aggregation_limit to 16M. This way as long as your doing 1M aligned IO you should never write partial blocks and leave holes. ZFS will aggregate these 1M blocks in to larger 16M IOs to the disk.</p>
<p><a target="_blank" rel="noopener" href="http://broken.net/uncategorized/zfs-performance-tuning-for-scrubs-and-resilvers/">zfs-performance-tuning-for-scrubs-and-resilvers</a><br><a target="_blank" rel="noopener" href="https://forum.proxmox.com/threads/zfs-zil-log-txg_sync-causing-high-io-every-5-seconds-any-solution.24376/">zfs-zil-log-txg_sync-causing-high-io-every-5-seconds</a><br><a target="_blank" rel="noopener" href="https://www.svennd.be/tuning-of-zfs-module/">tuning zfs module</a><br><a target="_blank" rel="noopener" href="http://lfs.ornl.gov/ecosystem-2016/documents/tutorials/Stearman-LLNL-ZFS.pdf">Stearman-LLNL-ZFS</a><br><a target="_blank" rel="noopener" href="http://fibrevillage.com/storage/171-zfs-on-linux-performance-tuning">171-zfs-on-linux-performance</a><br><a target="_blank" rel="noopener" href="http://list.zfsonlinux.org/pipermail/zfs-discuss/2018-March/030666.html">Is the number of z_wr_iss processes tunable?</a><br><a target="_blank" rel="noopener" href="https://www.beegfs.io/wiki/StorageServerTuning#hn_59ca4f8bbb_16">Tips and Recommendations for Storage Server Tuning</a></p>
<h3 id="About-silent-error"><a href="#About-silent-error" class="headerlink" title="About silent error"></a><a target="_blank" rel="noopener" href="http://www.lieberbiber.de/2018/07/17/improving-data-safety-on-all-systems-using-zfs-and-btrfs/">About silent error</a></h3><p>Enterprise drives can store checksums in a separate space by using slightly larger on-disk blocks of 520 or 4104 bytes in size. The additional eight bytes can be accessed by the operating system using a feature called T10 Protection Information (T10-PI). On consumer drives the file system has to store the checksums with the rest of the data, slightly reducing the available space.</p>
<p>Even if you have just one drive, you now get to know immediately when the drive is acting up before something bad happens. The operating system signals read errors to the applications instead of passing corrupt data. The backup software doesn’t overwrite the good backup. You don’t end up with corrupted data on the second system when transferring data using that flaky USB drive.</p>
<p>Sun Microsystems invented ZFS as a modern file system with lots of features, among them checksumming for all data. It has its own storage layer, its own RAID level implementations and can perform all checks while the file system is mounted. It was open-sourced as part of OpenSolaris, but cannot be assimilated into the Linux kernel due to licensing constraints. Ubuntu is the only Linux distribution which ships a ready-made ZFSonLinux kernel module, with all other distribution things are more complicated. I recommend using ZFS if you can, but even I only use it in my Ubuntu-based NAS. It is also rather complicated to set up and was not designed for removable devices, making it less attractive for many use cases.</p>
<h4 id="ext4-xfs-hardware-raid"><a href="#ext4-xfs-hardware-raid" class="headerlink" title="ext4 xfs hardware raid"></a>ext4 xfs hardware raid</h4><p>Both ext4 and XFS were supposed to get checksums for both metadata and data blocks. xfsprogs have enabled metadata checksums by default for all new XFS file systems as of version 3.2.3 (released in May 2015). e2fsprogs didn’t make metadata_csums the default before version 1.44.0 (released in March 2018).</p>
<p>Over the last twenty years I’ve only seen one hardware RAID controller which could be forced into checking parities at every read. Software implementations like Linux md-raid and LVM can also not be forced to do so. All you can do is to schedule regular parity scans of the full array, <code>but if the data gets silently corrupted between two scans</code> you’ve been working with it for a while. Congratulations.</p>
<h3 id="Openzfs-with-slient-error"><a href="#Openzfs-with-slient-error" class="headerlink" title="Openzfs with slient error"></a><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-10472">Openzfs with slient error</a></h3><p>Under the “end to end” not means the data full link path. just avoid slient error</p>
<p>The T10-PI checks (end therefore end-to-end) only apply to ldiskfs, not ZFS, correct?<br>Correct, ZFS does not implement T10-PI support. There is the old design for integrated end-to-end checksums from the client, which would use a similar, but more sophisticated, checksum method as the current T10 mechanism.</p>
<h3 id="script-for-raidz"><a href="#script-for-raidz" class="headerlink" title="script for raidz"></a>script for raidz</h3><p>by sas port</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ index=4; lsscsi -tiv | grep <span class="string">&quot;host0\/port-0:1&quot;</span> -B 1 | grep -vEi <span class="string">&#x27;dir|process|enclo|\-\-&#x27;</span>  | awk -v idx=<span class="variable">$&#123;index&#125;</span>  <span class="string">&#x27;&#123;if(NR%19==1 || NR==1) &#123; print &quot; &quot;;printf &quot;zpool create -f ost_&quot;idx&quot; raidz3 -O canmount=on -O xattr=sa -O acltype=posixacl -o ashift=9 -o multihost=on &quot;$(NF-1)&quot; &quot;;idx++&#125; else &#123;printf $(NF-1)&quot; &quot;&#125; &#125;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="Extend-single-device-become-a-mirror"><a href="#Extend-single-device-become-a-mirror" class="headerlink" title="Extend single device become a mirror"></a>Extend single device become a mirror</h3><p>scsi-35001173d028c8c08 not in the mirror, it ‘s dangerous</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">$ zpool status</span><br><span class="line">  pool: mdt_0</span><br><span class="line"> state: ONLINE</span><br><span class="line">  scan: scrub repaired 0B <span class="keyword">in</span> 1h35m with 0 errors on Tue Oct  1 01:35:20 2019</span><br><span class="line">config:</span><br><span class="line"></span><br><span class="line">	NAME                        STATE     READ WRITE CKSUM</span><br><span class="line">	mdt_0                       ONLINE       0     0     0</span><br><span class="line">	  mirror-0                  ONLINE       0     0     0</span><br><span class="line">	    sdb                     ONLINE       0     0     0</span><br><span class="line">	    sdc                     ONLINE       0     0     0</span><br><span class="line">	  scsi-35001173d028c8c08    ONLINE       0     0     0</span><br><span class="line">	  mirror-2                  ONLINE       0     0     0</span><br><span class="line">	    scsi-35001173d028c8740  ONLINE       0     0     0</span><br><span class="line">	    scsi-35001173d028cb334  ONLINE       0     0     0</span><br><span class="line"></span><br><span class="line">errors: No known data errors</span><br><span class="line">$ zpool attach -f mdt_0 scsi-35001173d028c8c08 /dev/sdh</span><br><span class="line">$ zpool status -x</span><br><span class="line">  pool: mdt_0</span><br><span class="line"> state: ONLINE</span><br><span class="line">status: One or more devices is currently being resilvered.  The pool will</span><br><span class="line">	<span class="built_in">continue</span> to <span class="keyword">function</span>, possibly <span class="keyword">in</span> a degraded state.</span><br><span class="line">action: Wait <span class="keyword">for</span> the resilver to complete.</span><br><span class="line">  scan: resilver <span class="keyword">in</span> progress since Sun Jan 19 12:46:57 2020</span><br><span class="line">	176M scanned out of 465G at 29.3M/s, 4h30m to go</span><br><span class="line">	58.8M resilvered, 0.04% <span class="keyword">done</span></span><br><span class="line">config:</span><br><span class="line"></span><br><span class="line">	NAME                        STATE     READ WRITE CKSUM</span><br><span class="line">	mdt_0                       ONLINE       0     0     0</span><br><span class="line">	  mirror-0                  ONLINE       0     0     0</span><br><span class="line">	    sdb                     ONLINE       0     0     0</span><br><span class="line">	    sdc                     ONLINE       0     0     0</span><br><span class="line">	  mirror-1                  ONLINE       0     0     0</span><br><span class="line">	    scsi-35001173d028c8c08  ONLINE       0     0     0</span><br><span class="line">	    sdh                     ONLINE       0     0     0  (resilvering)</span><br><span class="line">	  mirror-2                  ONLINE       0     0     0</span><br><span class="line">	    scsi-35001173d028c8740  ONLINE       0     0     0</span><br><span class="line">	    scsi-35001173d028cb334  ONLINE       0     0     0</span><br><span class="line"></span><br><span class="line">errors: No known data errors</span><br></pre></td></tr></table></figure>

<h3 id="Add-special-device"><a href="#Add-special-device" class="headerlink" title="Add special device"></a>Add special device</h3><p>special<br>A device dedicated solely for allocating various kinds of internal metadata, and optionally small file blocks. The redundancy of this device should match the redundancy of the other normal devices in the pool. If more than one special device is specified, then allocations are load-balanced between those devices.<br>For more information on special allocations, see the Sx Special Allocation Class section.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ zpool add <span class="variable">$POOLNAME</span> special mirror <span class="variable">$TMPDIR</span>/zpool&#123;4,5&#125;.dat</span><br></pre></td></tr></table></figure>

<h3 id="zpool-replace"><a href="#zpool-replace" class="headerlink" title="zpool replace"></a>zpool replace</h3><p>Plug out the wrong device, and re-plugin the device, you can ‘t replace it , just online it  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ zpool replace -f ost_82 6957378459605677274 sden</span><br><span class="line">invalid vdev specification</span><br><span class="line">the following errors must be manually repaired:</span><br><span class="line">/dev/sden1 is part of active pool <span class="string">&#x27;ost_82&#x27;</span></span><br><span class="line"></span><br><span class="line">$ zpool online ost_82 6957378459605677274</span><br></pre></td></tr></table></figure>

<h3 id="Online-upgrade-SAS-device-firmware"><a href="#Online-upgrade-SAS-device-firmware" class="headerlink" title="Online upgrade SAS device firmware"></a>Online upgrade SAS device firmware</h3><p>unzip Dell XXX.EXE firmware, you could got TT53.fwh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">$ zpool offline tank scsi-35000c500c3f5c9f2</span><br><span class="line">$ lsscsi -gis | grep 35000c500c3f5c9f2</span><br><span class="line">[6:0:76:0]   disk    SEAGATE  ST10000NM0256    TT53  /dev/sdbu  35000c500c3f5c9f2  /dev/sg76  9.79TB</span><br><span class="line"></span><br><span class="line">$ shmlic list drives -enc = xxxxx -verbose</span><br><span class="line">5000c50003c9cd9d /dev/sg76                 SEAGATE                            ST10000NM0256            XXXXXXX     TT55      8.91TB          50050cc1628e1043 (EN-8435A-E6EBD)  TH0YF87JSGT0087401M5A00          26   00 / 26   6G   2018    NO   5000c500dd9da3b8, 0000000000000000</span><br><span class="line"></span><br><span class="line">$ shmcli update drive -d = 5000c50003c9cd9d -file = ./TT54.fwh</span><br><span class="line"></span><br><span class="line">update drive - Executing <span class="built_in">command</span>..</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	(SEAGATE / ST10000NM0256 / ZA27NCHH) - File firmware version = TT54</span><br><span class="line">	(SEAGATE / ST10000NM0256 / ZA27NCHH) - Current drive firmware version = TT53</span><br><span class="line">	(SEAGATE / ST10000NM0256 / ZA27NCHH) - Sending firmware file to device.</span><br><span class="line">	(SEAGATE / ST10000NM0256 / ZA27NCHH) - Firmware successfully sent to drive.</span><br><span class="line">	(SEAGATE / ST10000NM0256 / ZA27NCHH) - After update drive firmware version = TT54</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Drive Update Statistics:</span><br><span class="line">	Total Drives Attempted					 : 1</span><br><span class="line">	Successful Drive update count				 : 1</span><br><span class="line">	FW File transfer failures				 : 0</span><br><span class="line">	FW Update failures despite successful FW file transfer   : 0</span><br><span class="line">	Unsupported Drive count (Non-Dell Drives)		 : 0</span><br><span class="line">	Drives Incompatible with Firmware File			 : 0</span><br><span class="line">	Drives skipped because drive is same version		 : 0</span><br><span class="line">	Drives skipped because drive is newer version		 : 0</span><br><span class="line"></span><br><span class="line">$ ls -l | grep -v ata</span><br><span class="line">total 0</span><br><span class="line">lrwxrwxrwx 1 root root 0 Apr 25 14:22 host6 -&gt; ../../devices/pci0000:b2/0000:b2:00.0/0000:b3:00.0/host6/scsi_host/host6</span><br><span class="line">$ <span class="built_in">echo</span> 1 &gt; /sys/block/sdbu/device/delete</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;0 76 0&quot;</span> &gt;  /sys/class/scsi_host/host6/scan</span><br><span class="line">$ lsscsi -gis | grep 35000c500c3f5c9f2</span><br><span class="line">[6:0:76:0]   disk    SEAGATE  ST10000NM0256    TT54  /dev/sdbu  35000c500c3f5c9f2  /dev/sg76  9.79TB</span><br><span class="line">$ zpool online tank  scsi-35000c500c3f5c9f2</span><br></pre></td></tr></table></figure>

<h3 id="Custom-packages"><a href="#Custom-packages" class="headerlink" title="Custom packages"></a><a target="_blank" rel="noopener" href="https://openzfs.github.io/openzfs-docs/Developer%20Rebackup/Custom%20Packages.html">Custom packages</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install epel-release gcc make autoconf automake libtool rpm-build dkms libtirpc-devel libblkid-devel libuuid-devel libudev-devel openssl-devel zlib-devel libaio-devel libattr-devel elfutils-libelf-devel kernel-devel-$(uname -r) python python2-devel python-setuptools python-cffi libffi-devel</span><br><span class="line"></span><br><span class="line">$ sudo dnf install gcc make autoconf automake libtool rpm-build kernel-rpm-macros dkms libtirpc-devel libblkid-devel libuuid-devel libudev-devel openssl-devel zlib-devel libaio-devel libattr-devel elfutils-libelf-devel kernel-devel-$(uname -r) python3 python3-devel python3-setuptools python3-cffi libffi-devel</span><br><span class="line"></span><br><span class="line"><span class="comment">### DKMS</span></span><br><span class="line">$ <span class="built_in">cd</span> zfs</span><br><span class="line">$ ./configure</span><br><span class="line">$ make -j1 rpm-utils rpm-dkms</span><br><span class="line">$ sudo yum localinstall *.$(uname -p).rpm *.noarch.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment">### kmod</span></span><br><span class="line">$ <span class="built_in">cd</span> zfs</span><br><span class="line">$ ./configure</span><br><span class="line">$ make -j1 rpm-utils rpm-kmod</span><br><span class="line">$ sudo yum localinstall *.$(uname -p).rpm</span><br><span class="line"></span><br><span class="line"><span class="comment">### kABI-tracking kmod</span></span><br><span class="line">$ <span class="built_in">cd</span> zfs</span><br><span class="line">$ ./configure --with-spec=redhat</span><br><span class="line">$ make -j1 rpm-utils rpm-kmod</span><br><span class="line">$ sudo yum localinstall *.$(uname -p).rpm</span><br></pre></td></tr></table></figure>

<h3 id="Backup-zpool"><a href="#Backup-zpool" class="headerlink" title="Backup zpool"></a>Backup zpool</h3><p><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/issues/8458">send recv slow issue</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">-L   Generate  a  stream <span class="built_in">which</span> may contain blocks larger than 128KB.  This flag has no effect <span class="keyword">if</span> the large_blocks pool feature is disabled, or <span class="keyword">if</span> the recordsize property of this filesystem has never been <span class="built_in">set</span> above 128KB.  The receiving system must have the large_blocks pool feature enabled as well.  See zpool-features(5) <span class="keyword">for</span> details on ZFS feature flags and the large_blocks feature.  </span><br><span class="line"></span><br><span class="line">A $ umount /lfs/test_mgt /lfs/test_mdt</span><br><span class="line">A $ zfs snapshot test0_mdt/test_mgt@20200529-1</span><br><span class="line">A $ zfs snapshot test0_mdt/test_mdt@20200529-1</span><br><span class="line">A $ zfs send -Rp test0_mdt/test_mdt@20200529-1 &gt; <span class="string">&quot;/nfshare/test0_mdt_test_mdt@20200529-1&quot;</span></span><br><span class="line">A $ zfs send -Rp test0_mdt/test_mgt@20200529-1 &gt; <span class="string">&quot;/nfshare/test0_mdt_test_mgt@20200529-1&quot;</span></span><br><span class="line"></span><br><span class="line">B $ zfs create test0_backup/test_mgt</span><br><span class="line">B $ zfs create test0_backup/test_mdt</span><br><span class="line">B $ zfs receive -F test0_backup/test_mgt &lt; test0_mdt_test_mgt@20200529-1</span><br><span class="line">B $ dd <span class="keyword">if</span>=test0_mdt_test_mdt\@20200529-1 bs=1M | zfs recv -F test0_backup/test_mdt</span><br><span class="line">577+1 records <span class="keyword">in</span></span><br><span class="line">577+1 records out</span><br><span class="line">605602396 bytes (606 MB) copied, 2340.37 s, 259 kB/s</span><br><span class="line"></span><br><span class="line">B $ mount.lfs test0_backup/test_mgt /lfs/test_mgt</span><br><span class="line">mount.lfs: test0_backup/test_mgt is configured <span class="keyword">for</span> failover but zpool does not have multihost enabled</span><br><span class="line">mount.lfs: test0_backup/test_mgt is configured <span class="keyword">for</span> failover but zpool does not have multihost enabled</span><br><span class="line">mount.lfs: test0_backup/test_mgt is configured <span class="keyword">for</span> failover but zpool does not have multihost enabled</span><br><span class="line">B $ mount.lfs test0_backup/test_mdt /lfs/test_mdt</span><br><span class="line">mount.lfs: test0_backup/test_mdt is configured <span class="keyword">for</span> failover but zpool does not have multihost enabled</span><br><span class="line">mount.lfs: test0_backup/test_mdt is configured <span class="keyword">for</span> failover but zpool does not have multihost enabled</span><br><span class="line">mount.lfs: test0_backup/test_mdt is configured <span class="keyword">for</span> failover but zpool does not have multihost enabled</span><br><span class="line"></span><br><span class="line">test0_backup/test_mgt      lfs    287G  2.8M  287G   1% /lfs/test_mgt</span><br><span class="line">test0_backup/test_mdt      lfs    287G  196M  287G   1% /lfs/test_mdt</span><br><span class="line"></span><br><span class="line"><span class="comment"># the server B create a snapshot, if not you could create it</span></span><br><span class="line">B $ zfs  list -t snapshot</span><br><span class="line">NAME                               USED  AVAIL  REFER  MOUNTPOINT</span><br><span class="line">test0_backup/test_mdt@20200529-1   488K      -   174M  -</span><br></pre></td></tr></table></figure>

<p>write new data from the client</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">B $ zfs snapshot test0_backup&#x2F;test_mdt@20200529-2</span><br><span class="line">B $ zfs  list -t snapshot</span><br><span class="line">NAME                               USED  AVAIL  REFER  MOUNTPOINT</span><br><span class="line">test0_backup&#x2F;test_mdt@20200529-1   488K      -   174M  -</span><br><span class="line">test0_backup&#x2F;test_mdt@20200529-2     0B      -   305M  -</span><br><span class="line"></span><br><span class="line">B $ zfs send -p -i test0_backup&#x2F;test_mdt@20200529-1 test0_backup&#x2F;test_mdt@20200529-2 &gt; &#x2F;nfshare&#x2F;test0_mdt_test_mdt\@20200529-from-1-to-2</span><br><span class="line"></span><br><span class="line"># import to server A</span><br><span class="line">A $ zfs receive -dF test0_mdt&#x2F;test_mdt &lt; test0_mdt_test_mdt\@20200529-from-1-to-2</span><br><span class="line">A $ zfs list -t snapshot</span><br><span class="line">NAME                            USED  AVAIL  REFER  MOUNTPOINT</span><br><span class="line">test0_mdt&#x2F;test_mdt@20200529-1   498K      -   174M  -</span><br><span class="line">test0_mdt&#x2F;test_mdt@20200529-2     0B      -   305M  -</span><br><span class="line">test0_mdt&#x2F;test_mgt@20200529-1    62K      -  2.72M  -</span><br></pre></td></tr></table></figure>
<p>In the client, you could see the modified files</p>
<h3 id="Openzip-direct-IO"><a href="#Openzip-direct-IO" class="headerlink" title="Openzip direct IO"></a><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/pull/10018">Openzip direct IO</a></h3><p>directio &lt;off,on,strict,legacy&gt;<br>off - Do not accept O_DIRECT flag<br>on - Accept O_DIRECT flag and possibly bypass ARC<br>(will bypass base on directio_write/read_align SEE BELOW)<br>strict - Accept O_DIRECT flag and always bypass ARC<br>(may fail based on directio_write/read_align SEE BELOW)<br>legacy - Accept O_DIRECT flag and always use ARC</p>
<p>directio=standard | always | disabled<br>where standard means: if you request DIRECTIO, we’ll do it directly if we think it’s a good idea (e.g. writes are recordsize-aligned), and otherwise we’ll do the i/o non-directly (we won’t fail it for poor alignment). This is the default.<br>always means act like DIRECTIO was always requested (may be actually direct or indirect depending on i/o alignment, won’t fail for poor alignment).<br>disabled means act like DIRECTIO was never requested (which is the current behavior).</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">This means that DIRECTIO writes will be spread out among the vdevs using the old round-robin algorithm. This could potentially result <span class="keyword">in</span> poor performance due to allocating from the slowest / most fragmented vdev, and we could potentially make the vdevs even more imbalanced (at least <span class="keyword">in</span> terms of performance/fragmentation). @grwilson <span class="keyword">do</span> you have any thoughts on this? How big the impact could be, and potential ways to mitigate? Could we make this use the throttle?</span><br></pre></td></tr></table></figure>


<h3 id="Performance-tuning"><a href="#Performance-tuning" class="headerlink" title="Performance tuning"></a><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/issues/8381">Performance tuning</a></h3><p>zio_dva_throttle_enabled - Throttle block allocations in the ZIO pipeline<br>zfs_vdev_queue_depth_pct - Queue depth percentage for each top-level vdev<br>increase the default value for high throuhput system  or disable zio_dva_throttle_enabled</p>
<p>zfs_vdev_sync_read_max_active<br>zfetch_max_distance<br>zfs_vdev_max_active<br>zfs_commit_timeout_pct<br>controls the amount of time that a log (ZIL) write block (lwb) remains “open” when it isn’t “full” and it has a thread waiting to commit to stable storage. The timeout is scaled based on a percentage of the last lwb latency to avoid significantly impacting the latency of each individual intent log transaction (itx).</p>
<p>zfs_prefetch_disable<br>primarycache</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yes I tried setting the primarycache to just metadata and disabling prefetching. Neither had any effect. Even <span class="keyword">if</span> we are not caching the <span class="keyword">in</span> ARC, the data is still being copied into a ZFS buffer. This is <span class="built_in">where</span> the copy penalty comes from</span><br><span class="line"> there is no reason to <span class="built_in">test</span> with Direct IO <span class="keyword">in</span> 0.8.2. Up to this point, with all 0.8.x releases, Direct IO support merely allows <span class="keyword">for</span> the O_DIRECT flag to be accepted. However, <span class="keyword">while</span> accepting the flag it just sends the reads/writes down the normal ZFS paths. This means a data copy is still made <span class="keyword">in</span> the ZFS software stack. </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>In general, smaller max_active’s will lead to lower latency of synchronous operations.  Larger max_active’s may lead to higher overall throughput, depending on underlying storage.<br>The ratio of the queues’ max_actives determines the balance of performance between reads, writes,  and  scrubs.<br>E.g., increasing zfs_vdev_scrub_max_active will cause the scrub or resilver to complete more quickly, but reads and writes to have higher latency and lower throughput.</p>
<p>Async Writes<br>The number of concurrent operations issued for the async write I/O class follows a piece-wise  linear  function defined by a few adjustable points.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">       |              o---------| &lt;-- zfs_vdev_async_write_max_active</span><br><span class="line">  ^    |             &#x2F;^         |</span><br><span class="line">  |    |            &#x2F; |         |</span><br><span class="line">active |           &#x2F;  |         |</span><br><span class="line"> I&#x2F;O   |          &#x2F;   |         |</span><br><span class="line">count  |         &#x2F;    |         |</span><br><span class="line">       |        &#x2F;     |         |</span><br><span class="line">       |-------o      |         | &lt;-- zfs_vdev_async_write_min_active</span><br><span class="line">      0|_______^______|_________|</span><br><span class="line">       0%      |      |       100% of zfs_dirty_data_max</span><br><span class="line">               |      |</span><br><span class="line">               |      ‘-- zfs_vdev_async_write_active_max_dirty_percent</span><br><span class="line">               ‘--------- zfs_vdev_async_write_active_min_dirty_percent</span><br></pre></td></tr></table></figure>
<p>Until  the  amount  of  dirty  data exceeds a minimum percentage of the dirty data allowed in the pool, the I/O scheduler will limit the number of concurrent operations to the minimum. As that threshold is crossed, the num-ber  of  concurrent  operations issued increases linearly to the maximum at the specified maximum percentage of the dirty data allowed in the pool.</p>
<p>Ideally, the amount of dirty data on a busy pool  will  stay  in  the  sloped  part  of  the  function  between zfs_vdev_async_write_active_min_dirty_percent  and zfs_vdev_async_write_active_max_dirty_percent. If it exceeds the maximum percentage, this indicates that the rate of incoming data is greater than the rate that the backend storage can handle. In this case, we must further throttle incoming writes, as described in the next section.</p>
<p>The minimum time for a transaction to take is calculated as:<br>           min_time = zfs_delay_scale * (dirty - min) / (max - dirty)<br>           min_time is then capped at 100 milliseconds.</p>
<p>The  delay has two degrees of freedom that can be adjusted via tunables.  The percentage of dirty data at which we  start  to  delay  is  defined  by  zfs_delay_min_dirty_percent.  This  should  typically  be  at  or  above zfs_vdev_async_write_active_max_dirty_percent  so  that  we only start to delay after writing at full speed has failed to keep up with the incoming write rate.<br>The scale of the curve is defined by  zfs_delay_scale.  Roughly speaking, this variable determines the amount of delay at the midpoint of the curve.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ cat zfs_vdev_async_write_active_min_dirty_percent zfs_vdev_async_write_active_max_dirty_percent zfs_delay_min_dirty_percent zfs_delay_scale</span><br><span class="line">30</span><br><span class="line">60</span><br><span class="line">60</span><br><span class="line">500000</span><br><span class="line"></span><br><span class="line">$ cat zfs_dirty_data_max</span><br><span class="line">4294967296</span><br></pre></td></tr></table></figure>

<p>note  that since the delay is added to the outstanding time remaining on the most recent transaction, the delay is effectively the inverse of IOPS.  Here the midpoint of 500us translates to 2000 IOPS. The shape of the curve was chosen such that small changes in the amount of accumulated dirty data in the first 3/4 of the curve yield relatively small differences in the amount of delay.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">delay</span><br><span class="line">100ms +-------------------------------------------------------------++</span><br><span class="line">      +                                                              +</span><br><span class="line">      |                                                              |</span><br><span class="line">      +                                                             *+</span><br><span class="line"> 10ms +                                                             *+</span><br><span class="line">      +                                                           ** +</span><br><span class="line">      |                                              (midpoint)  **  |</span><br><span class="line">      +                                                  |     **    +</span><br><span class="line">  1ms +                                                  v ****      +</span><br><span class="line">      +             zfs_delay_scale ----------&gt;        *****         +</span><br><span class="line">      |                                             ****             |</span><br><span class="line">      +                                          ****                +</span><br><span class="line">100us +                                        **                    +</span><br><span class="line">      +                                       *                      +</span><br><span class="line">      |                                      *                       |</span><br><span class="line">      +                                     *                        +</span><br><span class="line"> 10us +                                     *                        +</span><br><span class="line">      +                                                              +</span><br><span class="line">      |                                                              |</span><br><span class="line">      +                                                              +</span><br><span class="line">      +--------------------------------------------------------------+</span><br><span class="line">      0%                    &lt;- zfs_dirty_data_max -&gt;               100%</span><br></pre></td></tr></table></figure>

<p>Note here that only as the amount of dirty data approaches its limit does the delay start to increase  rapidly. The  goal  of  a  properly  tuned  system should be to keep the amount of dirty data out of that range by first ensuring that the appropriate limits are set for the I/O scheduler to reach optimal throughput on  the  backend storage, and then by changing the value of zfs_delay_scale to increase the steepness of the curve.</p>
<p>set spl_taskq_thread_dynamic to 0<br>This will disable all of the dynamic task queues and statistically allocate the threads.</p>
<p>Test case:<br>10 x SAS/SATA mix HDD in single raidz2 zpool, 32 x rsync copy data from another dir in this zpool </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">              capacity     operations     bandwidth</span><br><span class="line">pool        alloc   free   <span class="built_in">read</span>  write   <span class="built_in">read</span>  write</span><br><span class="line">----------  -----  -----  -----  -----  -----  -----</span><br><span class="line">tank        47.4T  41.7T  1.06K    398  24.5M  17.9M</span><br><span class="line">tank        47.4T  41.7T  9.01K  7.96K   193M   158M</span><br><span class="line">tank        47.4T  41.7T  9.77K  8.08K   182M   162M</span><br><span class="line">tank        47.4T  41.7T  7.88K  6.32K   178M   195M</span><br><span class="line">tank        47.4T  41.7T  7.36K  6.70K   175M   152M</span><br><span class="line">tank        47.4T  41.7T  8.05K  6.81K   155M   140M</span><br><span class="line">tank        47.4T  41.7T  9.16K  9.10K   166M   182M</span><br><span class="line">tank        47.4T  41.7T  5.97K  4.30K   187M   402M</span><br><span class="line">tank        47.4T  41.7T  8.24K    739   210M   365M</span><br><span class="line">tank        47.4T  41.7T  10.8K  14.0K   180M   254M</span><br><span class="line">tank        47.4T  41.7T  9.93K  7.43K   205M   163M</span><br><span class="line">tank        47.4T  41.7T  8.97K  7.35K   181M   150M</span><br><span class="line">tank        47.4T  41.7T  9.05K  7.52K   169M   159M</span><br><span class="line">tank        47.4T  41.7T  9.34K  8.06K   160M   160M</span><br><span class="line">tank        47.4T  41.7T  8.98K  8.34K   161M   168M</span><br><span class="line">tank        47.4T  41.7T  8.85K  8.29K   182M   177M</span><br><span class="line">tank        47.4T  41.7T  9.03K  9.27K   188M   188M</span><br><span class="line">tank        47.4T  41.7T  8.35K  9.36K   209M   206M</span><br><span class="line">tank        47.4T  41.7T  7.71K  9.22K   165M   229M</span><br><span class="line">tank        47.4T  41.7T  3.89K  7.69K  93.9M   158M</span><br><span class="line">tank        47.4T  41.7T  3.48K  6.78K  56.1M   127M</span><br><span class="line">tank        47.4T  41.7T  6.49K  15.5K   105M   288M</span><br><span class="line">tank        47.4T  41.7T  10.0K  8.16K   162M   202M</span><br><span class="line">tank        47.4T  41.7T  10.2K  8.77K   171M   175M</span><br><span class="line">tank        47.4T  41.7T  10.2K  7.86K   168M   169M</span><br><span class="line">tank        47.4T  41.7T  9.84K  7.74K   163M   155M</span><br><span class="line">tank        47.4T  41.7T  7.48K  6.02K   177M   208M</span><br><span class="line">tank        47.4T  41.7T  9.54K  6.80K   169M   140M</span><br><span class="line">tank        47.4T  41.7T  10.0K  8.44K   171M   190M</span><br><span class="line">tank        47.4T  41.7T  9.56K  7.94K   168M   167M</span><br><span class="line">tank        47.4T  41.7T  9.52K  7.69K   168M   155M</span><br><span class="line">tank        47.4T  41.7T  8.13K  10.5K   139M   225M</span><br><span class="line">tank        47.4T  41.7T  5.04K  11.0K  80.8M   215M</span><br><span class="line">tank        47.4T  41.7T  3.24K  8.40K  58.7M   149M</span><br><span class="line">tank        47.4T  41.7T  6.61K  11.6K   128M   244M</span><br><span class="line">tank        47.4T  41.7T  8.39K  6.01K   157M   169M</span><br><span class="line">tank        47.4T  41.7T  9.09K  7.57K   149M   160M</span><br><span class="line">tank        47.4T  41.7T  7.62K  6.03K   151M   123M</span><br><span class="line">tank        47.4T  41.7T  9.06K  9.47K   159M   209M</span><br><span class="line">tank        47.4T  41.7T  10.3K  8.52K   186M   198M</span><br><span class="line">tank        47.4T  41.7T  10.1K  8.27K   182M   177M</span><br><span class="line">tank        47.4T  41.7T  8.11K  7.67K   165M   187M</span><br></pre></td></tr></table></figure>
<p>Read and write are slow, all stack show the read is slow, looks like the zfs parameter was wrong (not default).</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># after modify the zfs parameter, increase the read and write throughput </span></span><br><span class="line">$ cat zfs_vdev_async_read_max_active zfs_vdev_async_read_min_active zfs_vdev_async_write_active_max_dirty_percent zfs_vdev_async_write_active_min_dirty_percent zfs_vdev_async_write_max_active zfs_vdev_async_write_min_active</span><br><span class="line">20</span><br><span class="line">2</span><br><span class="line">120</span><br><span class="line">60</span><br><span class="line">10</span><br><span class="line">1</span><br><span class="line">$ <span class="built_in">echo</span> 8589934592 &gt; zfs_dirty_data_max</span><br><span class="line">tank        47.7T  41.4T  2.84K  1.12K   525M  1.02G</span><br><span class="line">tank        47.7T  41.4T  6.32K     23  1023M  23.1M</span><br><span class="line">tank        47.7T  41.4T  6.20K    140   920M  8.98M</span><br><span class="line">tank        47.7T  41.3T  2.76K  1.64K   242M  1.44G</span><br><span class="line">tank        47.7T  41.3T  5.71K    939   471M   785M</span><br><span class="line">tank        47.7T  41.3T  5.03K    470   777M   297M</span><br><span class="line">tank        47.7T  41.3T  3.51K  1.14K   445M  1.05G</span><br><span class="line">tank        47.7T  41.3T  6.19K     88   991M  1.42M</span><br><span class="line">tank        47.8T  41.3T    959  1.70K   193M  1.60G</span><br><span class="line">tank        47.8T  41.3T  5.72K    228   930M  58.1M</span><br><span class="line">tank        47.8T  41.3T  2.15K  1.33K   301M  1.12G</span><br><span class="line">tank        47.8T  41.3T  5.90K    316   895M   237M</span><br><span class="line">tank        47.8T  41.3T  3.48K  1.04K   439M   834M</span><br><span class="line">tank        47.8T  41.3T  3.83K    927   606M   860M</span><br><span class="line">tank        47.8T  41.3T  5.15K    397   822M   214M</span><br><span class="line">tank        47.8T  41.3T  3.02K  1.17K   446M  1.05G</span><br><span class="line">tank        47.8T  41.3T  5.99K    243   940M  71.8M</span><br><span class="line">tank        47.8T  41.3T  2.57K  1.32K   312M  1.21G</span><br><span class="line">tank        47.8T  41.3T  4.48K    765   746M   522M</span><br><span class="line">tank        47.8T  41.3T  4.90K    781   642M   551M</span><br><span class="line">tank        47.8T  41.3T  2.94K  1.17K   487M  1.04G</span><br><span class="line">tank        47.8T  41.3T  6.25K    242   889M  51.1M</span><br><span class="line">              capacity     operations     bandwidth</span><br><span class="line">pool        alloc   free   <span class="built_in">read</span>  write   <span class="built_in">read</span>  write</span><br><span class="line">----------  -----  -----  -----  -----  -----  -----</span><br><span class="line">tank        47.8T  41.3T  2.96K  1.20K   373M  1.01G</span><br><span class="line">tank        47.8T  41.3T  5.10K    761   694M   656M</span><br><span class="line">tank        47.8T  41.3T  6.78K    610   571M   415M</span><br><span class="line">tank        47.8T  41.3T  4.46K    787   649M   663M</span><br><span class="line">tank        47.8T  41.3T  5.47K    500   670M   410M</span><br><span class="line">tank        47.8T  41.3T  3.48K  1.00K   500M   783M</span><br><span class="line">tank        47.8T  41.3T  3.82K    856   493M   660M</span><br><span class="line">tank        47.8T  41.3T  3.71K  1.08K   515M   997M</span><br><span class="line">tank        47.8T  41.3T  5.93K    247   826M  73.9M</span><br><span class="line">tank        47.8T  41.3T  3.34K  1.21K   465M  1.05G</span><br><span class="line">tank        47.8T  41.3T  5.26K    608   748M   422M</span><br><span class="line">tank        47.8T  41.3T  5.70K    834   509M   652M</span><br><span class="line">tank        47.8T  41.3T  3.62K    875   622M   814M</span><br><span class="line">tank        47.8T  41.3T  6.32K    439   758M   258M</span><br><span class="line">tank        47.8T  41.3T  2.89K  1.32K   409M  1.05G</span><br><span class="line">tank        47.8T  41.3T  5.86K    609   792M   455M</span><br><span class="line">tank        47.8T  41.3T  4.62K    834   565M   619M</span><br><span class="line">tank        47.8T  41.3T  3.34K  1.17K   480M  1.05G</span><br><span class="line"></span><br><span class="line"><span class="comment">#disable single SATA driver write cache /dev/sdl cause the big fluctuation</span></span><br><span class="line">              capacity     operations     bandwidth</span><br><span class="line">pool        alloc   free   <span class="built_in">read</span>  write   <span class="built_in">read</span>  write</span><br><span class="line">----------  -----  -----  -----  -----  -----  -----</span><br><span class="line">tank        47.4T  41.7T  19.5K    503   430M   485M</span><br><span class="line">tank        47.4T  41.7T  22.1K    571   440M   572M</span><br><span class="line">tank        47.4T  41.7T  22.5K    534   424M   533M</span><br><span class="line">tank        47.4T  41.7T  6.02K  1.18K   250M  1.18G</span><br><span class="line">tank        47.4T  41.7T  10.2K     64   777M  64.2M</span><br><span class="line">tank        47.4T  41.7T  15.2K    468   336M   117M</span><br><span class="line">tank        47.4T  41.7T  3.86K  1.63K   229M  1.49G</span><br><span class="line">tank        47.4T  41.7T    576  1.85K  56.5M  1.84G</span><br><span class="line">tank        47.4T  41.7T  6.56K    334   537M   324M</span><br><span class="line">tank        47.4T  41.7T  9.41K      6   997M  6.50M</span><br><span class="line">tank        47.4T  41.7T  8.39K      7   976M  6.88M</span><br><span class="line">tank        47.4T  41.7T  2.08K     73  79.4M  68.6M</span><br><span class="line">tank        47.4T  41.7T     12     81  63.9K  76.8M</span><br><span class="line">tank        47.4T  41.7T     16    105  65.9K  68.3M</span><br><span class="line">tank        47.4T  41.7T  2.35K  1.74K   196M  1.36G</span><br><span class="line">tank        47.4T  41.7T  2.49K  1.48K   229M  1.46G</span><br><span class="line">tank        47.4T  41.7T  7.59K     16   834M  16.0M</span><br><span class="line">tank        47.4T  41.7T  8.46K      7  1009M  7.50M</span><br><span class="line">tank        47.4T  41.7T  8.80K      6   985M  6.49M</span><br><span class="line">tank        47.4T  41.7T  5.65K    168   119M   166M</span><br><span class="line">tank        47.4T  41.7T  12.2K    862   386M   429M</span><br><span class="line">tank        47.4T  41.7T  1.19K  1.91K   109M  1.75G</span><br><span class="line">tank        47.4T  41.7T  3.19K  1.43K   219M  1.42G</span><br><span class="line">tank        47.4T  41.7T  8.27K     20  1.01G  16.1M</span><br><span class="line">tank        47.4T  41.7T  6.76K     23   833M  18.6M</span><br><span class="line">tank        47.4T  41.7T  8.90K     17   841M  17.5M</span><br><span class="line">tank        47.4T  41.7T  8.28K    144   141M   136M</span><br><span class="line">tank        47.4T  41.7T  5.96K  1.30K   283M   749M</span><br><span class="line">tank        47.4T  41.7T    522  1.84K  65.3M  1.83G</span><br><span class="line">tank        47.4T  41.7T  6.07K    804   575M   802M</span><br><span class="line">tank        47.4T  41.7T  7.62K     23   865M  18.6M</span><br><span class="line">tank        47.4T  41.7T  8.69K     19   983M  15.1M</span><br><span class="line">tank        47.4T  41.7T  8.97K     41   456M  40.1M</span><br><span class="line">tank        47.4T  41.7T  8.46K    145   145M   146M</span><br><span class="line">tank        47.4T  41.7T  3.44K  1.75K   151M  1.35G</span><br><span class="line">tank        47.4T  41.7T  1.82K  1.70K   170M  1.64G</span><br><span class="line">tank        47.4T  41.7T  8.81K    357   799M   345M</span><br><span class="line">tank        47.4T  41.7T  7.30K      6  1009M  6.50M</span><br><span class="line">tank        47.4T  41.7T  7.28K     15   918M  15.4M</span><br><span class="line">tank        47.4T  41.7T      0     76      0  75.3M</span><br><span class="line">tank        47.4T  41.7T      0     74      0  74.4M</span><br><span class="line">tank        47.4T  41.6T  2.04K    715   134M   479M</span><br><span class="line">tank        47.4T  41.6T  2.53K  2.00K   213M  1.62G</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           0.40    0.00    2.68   33.36    0.00   63.56</span><br><span class="line"></span><br><span class="line">Device            r/s     w/s     rMB/s     wMB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util</span><br><span class="line">sdk              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00</span><br><span class="line">sdj              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00</span><br><span class="line">sda             17.50   35.50      0.77      0.64     0.00     0.00   0.00   0.00   10.00    0.89   0.19    44.80    18.42   0.55   2.90</span><br><span class="line">sdb             22.50   35.00      0.75      0.46     0.00     0.00   0.00   0.00    9.24    0.44   0.20    34.22    13.60   0.50   2.90</span><br><span class="line">sdc             11.00   39.50      0.29      5.06     0.00     0.00   0.00   0.00    7.82    1.59   0.13    26.55   131.14   0.57   2.90</span><br><span class="line">sdi             13.00   37.00      0.29      2.22     0.00     0.00   0.00   0.00   13.00    1.62   0.19    22.62    61.51   0.54   2.70</span><br><span class="line">sdd              9.00   35.00      0.09      3.42     0.00     0.00   0.00   0.00   15.17    1.96   0.19     9.78    99.94   0.60   2.65</span><br><span class="line">sde             20.50   27.50      0.82      0.80     0.00     0.00   0.00   0.00   11.39    0.44   0.22    40.78    29.67   0.54   2.60</span><br><span class="line">sdf             11.50   31.00      0.25      2.91     0.00     0.00   0.00   0.00    8.61    2.16   0.15    22.61    96.13   0.60   2.55</span><br><span class="line">sdl             15.50   91.00      0.48     60.62     0.00     0.00   0.00   0.00   16.84    9.32   1.05    32.00   682.18   1.14  12.15</span><br><span class="line">sdh             21.50   31.00      0.76      1.36     0.00     0.00   0.00   0.00   13.35    0.66   0.29    36.09    45.03   0.55   2.90</span><br><span class="line">sdg             25.00   30.50      0.89      2.49     0.00     0.00   0.00   0.00    9.46    1.02   0.24    36.40    83.48   0.54   3.00</span><br><span class="line">sdm              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00</span><br><span class="line">dm-0             0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           2.78    0.00   22.16   34.62    0.00   40.43</span><br><span class="line"></span><br><span class="line">Device            r/s     w/s     rMB/s     wMB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util</span><br><span class="line">sdk              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00</span><br><span class="line">sdj              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00</span><br><span class="line">sda            103.00  175.50     15.76    166.59     1.00     0.00   0.96   0.00   93.70   13.54  11.90   156.68   972.02   0.97  26.95</span><br><span class="line">sdb            133.00  200.00     11.86    191.28     0.00     0.00   0.00   0.00   40.64   11.66   7.58    91.31   979.35   0.97  32.35</span><br><span class="line">sdc             69.00  193.00     10.81    184.69     0.00     0.00   0.00   0.00   36.62   12.47   4.81   160.41   979.92   1.04  27.35</span><br><span class="line">sdi            126.50  182.00     12.86    173.52     0.00     0.00   0.00   0.00   34.08   11.80   6.33   104.08   976.29   0.94  29.10</span><br><span class="line">sdd             46.50  212.50      9.22    203.81     0.50     0.00   1.06   0.00   55.43   12.06   5.02   203.01   982.12   1.08  28.05</span><br><span class="line">sde            108.50  200.00     13.44    191.41     0.00     0.00   0.00   0.00   88.51   12.70  11.99   126.86   980.04   0.96  29.65</span><br><span class="line">sdf             60.00  183.00     16.95    174.79     0.00     0.00   0.00   0.00  190.78   14.18  13.93   289.20   978.05   1.05  25.40</span><br><span class="line">sdl            318.00   81.00     15.86     78.46     0.00     0.00   0.00   0.00   14.41   28.66   6.72    51.07   991.93   0.64  25.50</span><br><span class="line">sdh             84.00  195.50     15.35    186.86     0.50     0.00   0.59   0.00  101.49   15.28  11.38   187.10   978.74   1.03  28.75</span><br><span class="line">sdg            101.00  188.00     16.51    178.72     0.00     0.00   0.00   0.00   94.24   12.07  11.64   167.35   973.45   0.97  28.15</span><br><span class="line">sdm              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00</span><br><span class="line">dm-0             0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           2.75    0.00    8.39   33.43    0.00   55.44</span><br><span class="line"></span><br><span class="line">Device            r/s     w/s     rMB/s     wMB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util</span><br><span class="line">sdk              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00</span><br><span class="line">sdj              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00</span><br><span class="line">sda             45.50  175.50      9.07    175.20     0.00     0.00   0.00   0.00  270.60    5.63  13.19   204.22  1022.27   1.17  25.75</span><br><span class="line">sdb            213.50  151.00     18.24    150.69     0.00     0.00   0.00   0.00  103.23    5.52  22.70    87.48  1021.88   0.79  28.95</span><br><span class="line">sdc            166.00  153.00     20.72    152.69     0.00     0.00   0.00   0.00  147.93    5.14  25.19   127.82  1021.93   0.89  28.30</span><br><span class="line">sdi             73.00  167.00     17.27    166.70     0.00     0.00   0.00   0.00  337.95    6.20  25.59   242.25  1022.13   1.10  26.50</span><br><span class="line">sdd            252.00  135.50     22.37    135.19     0.00     0.00   0.00   0.00  126.91    4.75  32.45    90.91  1021.67   0.77  30.00</span><br><span class="line">sde            237.50  150.50     17.07    150.20     0.50     0.00   0.21   0.00   90.04    5.19  21.98    73.58  1021.93   0.75  29.20</span><br><span class="line">sdf            166.00  165.00     14.48    164.69     0.00     0.00   0.00   0.00   70.75    5.56  12.51    89.31  1022.10   0.87  28.85</span><br><span class="line">sdl            406.50   71.00     12.26     64.25     0.00     0.00   0.00   0.00    7.77   17.15   4.17    30.88   926.68   0.61  29.05</span><br><span class="line">sdh            184.50  154.50     15.33    154.20     0.00     0.00   0.00   0.00  110.93    4.73  21.05    85.07  1021.99   0.84  28.35</span><br><span class="line">sdg            213.50  161.50     14.12    161.20     0.00     0.00   0.00   0.00   38.15    5.54   8.87    67.71  1022.12   0.82  30.80</span><br><span class="line">sdm              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00</span><br><span class="line">dm-0             0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">          19.37    0.00   14.28   22.80    0.00   43.55</span><br><span class="line"></span><br><span class="line">Device            r/s     w/s     rMB/s     wMB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util</span><br><span class="line">sdk              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00</span><br><span class="line">sdj              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00</span><br><span class="line">sda            421.50    0.00    107.24      0.00     0.00     0.00   0.00   0.00   39.27    0.00  16.34   260.53     0.00   0.60  25.10</span><br><span class="line">sdb            951.50    0.00    100.31      0.00     2.00     0.00   0.21   0.00   16.06    0.00  14.81   107.96     0.00   0.40  37.95</span><br><span class="line">sdc            880.00    0.00    100.63      0.00     0.50     0.00   0.06   0.00   17.46    0.00  14.93   117.09     0.00   0.41  35.90</span><br><span class="line">sdi            685.00    0.00    102.45      0.00     1.50     0.00   0.22   0.00   22.36    0.00  14.98   153.15     0.00   0.48  32.85</span><br><span class="line">sdd            858.50    0.00     99.32      0.00     0.00     0.00   0.00   0.00   17.24    0.00  14.39   118.46     0.00   0.42  35.75</span><br><span class="line">sde           1141.50    0.00     97.46      0.00     1.00     0.00   0.09   0.00   13.10    0.00  14.39    87.43     0.00   0.36  41.45</span><br><span class="line">sdf            894.50    0.00     98.18      0.00     0.50     0.00   0.06   0.00   17.11    0.00  14.86   112.39     0.00   0.39  34.70</span><br><span class="line">sdl            457.50   13.00    102.66     13.01     1.50     0.00   0.33   0.00   34.66  142.23  17.48   229.78  1024.77   0.55  26.10</span><br><span class="line">sdh           1070.50    0.00     98.14      0.00     0.50     0.00   0.05   0.00   14.03    0.00  14.50    93.88     0.00   0.38  40.25</span><br><span class="line">sdg           1014.50    0.00     98.00      0.00     0.50     0.00   0.05   0.00   14.45    0.00  14.16    98.92     0.00   0.40  40.40</span><br><span class="line">sdm              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00</span><br><span class="line">dm-0             0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1000 iops</span></span><br><span class="line">$ awk <span class="string">&#x27;BEGIN&#123;print 1/1000&#125;&#x27;</span></span><br><span class="line">0.001</span><br><span class="line">$ <span class="built_in">echo</span> 1000000 &gt; zfs_delay_scale</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h3 id="A-bit-of-throught-about-the-read-performance-not-balaning-in-a-single-raidz-zpool"><a href="#A-bit-of-throught-about-the-read-performance-not-balaning-in-a-single-raidz-zpool" class="headerlink" title="A bit of throught about the read performance not balaning in a single raidz zpool"></a>A bit of throught about the read performance not balaning in a single raidz zpool</h3><ul>
<li>In a stripe, each device store the data in diff location(cylinder,track,sector)<ul>
<li>After you remove file in zfs, the fragment and free space in diff location</li>
</ul>
</li>
<li>In the raidz zpool, Noone could guarantee all read and write the same cylinder,track,sector at the same time</li>
<li>zfs read data not contains the parity data, when it trigger the scrub, it will read all HDDs</li>
<li>zfs stripe is dynamic, some tiny stripes not across the all HDD. some of small IO will cause the IO not balancing</li>
</ul>
<h3 id="Thanks-for-Javen-‘s-help"><a href="#Thanks-for-Javen-‘s-help" class="headerlink" title="Thanks for Javen ‘s help"></a>Thanks for Javen ‘s help</h3></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Ginger</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2016/12/04/openzfs-tips/">http://yoursite.com/2016/12/04/openzfs-tips/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/scsi/">scsi</a><a class="post-meta__tags" href="/tags/zfs/">zfs</a></div><div class="post_share"><div class="social-share" data-image="/img/photo_by_spacex.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2016/12/18/sparse/"><img class="prev-cover" src="/img/photo_by_spacex.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">sparse file data segment</div></div></a></div><div class="next-post pull-right"><a href="/2016/09/04/kcptun/"><img class="next-cover" src="/img/photo_by_spacex.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">proxychains and kcptun</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2017/09/26/openzfs-issues/" title="openzfs issues"><img class="cover" src="/img/photo_by_spacex.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2017-09-26</div><div class="title">openzfs issues</div></div></a></div><div><a href="/2018/09/27/hba/" title="SAS HBA"><img class="cover" src="/img/photo_by_spacex.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-09-27</div><div class="title">SAS HBA</div></div></a></div><div><a href="/2019/04/02/lxc_with_zfs/" title="linux container with zfs"><img class="cover" src="/img/photo_by_spacex.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-04-02</div><div class="title">linux container with zfs</div></div></a></div><div><a href="/2015/12/04/openzfs-replace/" title="Replace HDD for openzfs"><img class="cover" src="/img/photo_by_spacex.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2015-12-04</div><div class="title">Replace HDD for openzfs</div></div></a></div><div><a href="/2015/11/20/recovery-zfs-partition-table/" title="Recover openzfs partition table"><img class="cover" src="/img/photo_by_spacex.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2015-11-20</div><div class="title">Recover openzfs partition table</div></div></a></div><div><a href="/2015/11/06/deploy_lfs_and_zfs/" title="Deploy Lfs and OpenZFS"><img class="cover" src="/img/photo_by_spacex.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2015-11-06</div><div class="title">Deploy Lfs and OpenZFS</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></article></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2015 - 2020 By Ginger</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><div class="js-pjax"><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk({
      clientID: '0b9d5b98d0a972b33cf7',
      clientSecret: '769efc11b32f6c0d03bcbf3ee800dfc4e2690459',
      repo: 'homerl.github.io',
      owner: 'homerl',
      admin: ['homerl'],
      id: '24be2119e9efee55b2b9043e4470f77b',
      language: 'en',
      perPage: 10,
      distractionFreeMode: false,
      pagerDirection: 'last',
      createIssueManually: false,
      updateCountCallback: commentCount
    })
    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    $.getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js', initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>