<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>The block device | 己不由心，身又岂能由己</title><meta name="keywords" content="scsi,block"><meta name="author" content="Ginger"><meta name="copyright" content="Ginger"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="HardwareWhy use 4K device 4-KB native (4Kn) HDDs  * The 4Kn HDD directly maps 4-KB logical sectors (or blocks) to the 4-KB physical sectors.  512-byte emulation (512e) HDDs  * The 512e HDD transparent">
<meta property="og:type" content="article">
<meta property="og:title" content="The block device">
<meta property="og:url" content="http://yoursite.com/2016/08/10/block_device/index.html">
<meta property="og:site_name" content="己不由心，身又岂能由己">
<meta property="og:description" content="HardwareWhy use 4K device 4-KB native (4Kn) HDDs  * The 4Kn HDD directly maps 4-KB logical sectors (or blocks) to the 4-KB physical sectors.  512-byte emulation (512e) HDDs  * The 512e HDD transparent">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/img/photo_by_spacex.jpg">
<meta property="article:published_time" content="2016-08-09T16:04:26.000Z">
<meta property="article:modified_time" content="2020-10-09T12:23:06.482Z">
<meta property="article:author" content="Ginger">
<meta property="article:tag" content="scsi">
<meta property="article:tag" content="block">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/img/photo_by_spacex.jpg"><link rel="shortcut icon" href="/img/stout-shield.png"><link rel="canonical" href="http://yoursite.com/2016/08/10/block_device/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.2.0',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-10-09 20:23:06'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><meta name="generator" content="Hexo 5.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/244247-guts.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">58</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">57</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">9</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hardware"><span class="toc-number">1.</span> <span class="toc-text">Hardware</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Why-use-4K-device"><span class="toc-number">1.1.</span> <span class="toc-text">Why use 4K device</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mpt3sas-mpt2sas-driver-parameter"><span class="toc-number">1.2.</span> <span class="toc-text">mpt3sas&#x2F;mpt2sas driver parameter</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mpt3sas-driver-install"><span class="toc-number">1.3.</span> <span class="toc-text">mpt3sas driver install</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Driver-Build-Instructions"><span class="toc-number">1.3.1.</span> <span class="toc-text">Driver Build Instructions</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Qlogic-driver-setting"><span class="toc-number">1.4.</span> <span class="toc-text">Qlogic driver setting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SAN-controller-setting-MD3460"><span class="toc-number">1.5.</span> <span class="toc-text">SAN controller setting MD3460</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rebuild-feature"><span class="toc-number">2.</span> <span class="toc-text">rebuild feature</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Linux-setting"><span class="toc-number">3.</span> <span class="toc-text">Linux setting</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#block-driver"><span class="toc-number">3.1.</span> <span class="toc-text">block driver</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#block-driver-2"><span class="toc-number">3.2.</span> <span class="toc-text">block driver 2</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#parted-alignment"><span class="toc-number">3.2.1.</span> <span class="toc-text">parted alignment</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hot-plugin-the-scsi-device"><span class="toc-number">3.2.2.</span> <span class="toc-text">Hot plugin the scsi device</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Add-the-scsi-device"><span class="toc-number">3.2.3.</span> <span class="toc-text">Add the scsi device</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Refresh-the-scsi-device"><span class="toc-number">3.2.4.</span> <span class="toc-text">Refresh the scsi device</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Remove-the-scsi-device"><span class="toc-number">3.2.5.</span> <span class="toc-text">Remove the scsi device</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#IO-schdule"><span class="toc-number">3.3.</span> <span class="toc-text">IO schdule</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#deadline-parameters"><span class="toc-number">3.4.</span> <span class="toc-text">deadline parameters</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scsi-driver-error-handaling-EH"><span class="toc-number">4.</span> <span class="toc-text">scsi_driver error handaling (EH)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#iostat"><span class="toc-number">5.</span> <span class="toc-text">iostat</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Re-import-LVM"><span class="toc-number">6.</span> <span class="toc-text">Re-import LVM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Advance-reformat-4Kn-SCSI-devs-to-512-Bytes-sector"><span class="toc-number">7.</span> <span class="toc-text">Advance reformat 4Kn SCSI devs to 512 Bytes sector</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#intel-nvme-switch-to-AF"><span class="toc-number">7.1.</span> <span class="toc-text">intel nvme switch to AF</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SATA-only-support-512-and-4096-Bytes"><span class="toc-number">7.2.</span> <span class="toc-text">SATA only support 512 and 4096 Bytes</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#long-data-sector"><span class="toc-number">7.3.</span> <span class="toc-text">long-data-sector</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#T10-DIF-PI"><span class="toc-number">7.3.1.</span> <span class="toc-text">T10 DIF&#x2F;PI</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#About-DIF-520-Bytes"><span class="toc-number">7.3.2.</span> <span class="toc-text">About DIF 520 Bytes</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#T10-DIX-and-PI"><span class="toc-number">7.3.3.</span> <span class="toc-text">T10 DIX and PI</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Security-erasing"><span class="toc-number">8.</span> <span class="toc-text">Security erasing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Enable-the-blk-mq-and-scsi-mq"><span class="toc-number">9.</span> <span class="toc-text">Enable the blk_mq and scsi_mq</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lsscsi"><span class="toc-number">10.</span> <span class="toc-text">lsscsi</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#udevinfo"><span class="toc-number">11.</span> <span class="toc-text">udevinfo</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Flush-cache-data"><span class="toc-number">12.</span> <span class="toc-text">Flush cache data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dev-info"><span class="toc-number">13.</span> <span class="toc-text">dev info</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#mapper-test-device"><span class="toc-number">13.1.</span> <span class="toc-text">mapper test device</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Add-remove-key"><span class="toc-number">13.2.</span> <span class="toc-text">Add&#x2F;remove key</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Back-restore-header"><span class="toc-number">13.3.</span> <span class="toc-text">Back&#x2F;restore header</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#is-luks-partition"><span class="toc-number">13.4.</span> <span class="toc-text">is luks partition</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Clean-luks-partition"><span class="toc-number">13.5.</span> <span class="toc-text">Clean luks partition</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Why-Trim"><span class="toc-number">14.</span> <span class="toc-text">Why Trim</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Openzfs-on-linux"><span class="toc-number">14.1.</span> <span class="toc-text">Openzfs on linux</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Trim-patch-for-ZOL"><span class="toc-number">14.2.</span> <span class="toc-text">Trim patch for ZOL</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Mdadm"><span class="toc-number">14.3.</span> <span class="toc-text">Mdadm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LSI-Check-Interoperability-and-Compatibility"><span class="toc-number">14.4.</span> <span class="toc-text">LSI Check Interoperability and Compatibility</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HP-Raid-support"><span class="toc-number">14.5.</span> <span class="toc-text">HP Raid support</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Re-assign-bad-block"><span class="toc-number">15.</span> <span class="toc-text">Re-assign bad block</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#linux-block-integrity"><span class="toc-number">16.</span> <span class="toc-text">linux block integrity</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#test"><span class="toc-number">16.1.</span> <span class="toc-text">test</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Seagate-HDD-smart-issue"><span class="toc-number">17.</span> <span class="toc-text">Seagate HDD smart issue</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Seagate-SATA"><span class="toc-number">17.1.</span> <span class="toc-text">Seagate SATA</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#seagate-SAS-device"><span class="toc-number">17.2.</span> <span class="toc-text">seagate SAS device</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#plugin-out-the-device"><span class="toc-number">17.3.</span> <span class="toc-text">plugin out the device</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SMR-device"><span class="toc-number">18.</span> <span class="toc-text">SMR device</span></a></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(/img/photo_by_spacex.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">己不由心，身又岂能由己</a></span><span id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">The block device</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2016-08-09T16:04:26.000Z" title="Created 2016-08-10 00:04:26">2016-08-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2020-10-09T12:23:06.482Z" title="Updated 2020-10-09 20:23:06">2020-10-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Storage/">Storage</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h3 id="Hardware"><a href="#Hardware" class="headerlink" title="Hardware"></a>Hardware</h3><h4 id="Why-use-4K-device"><a href="#Why-use-4K-device" class="headerlink" title="Why use 4K device"></a>Why use 4K device</h4><ul>
<li>4-KB native (4Kn) HDDs<pre><code>  * The 4Kn HDD directly maps 4-KB logical sectors (or blocks) to the 4-KB physical sectors.</code></pre>
</li>
<li>512-byte emulation (512e) HDDs<pre><code>  * The 512e HDD transparently translates 512-byte logical block I/O requests into 4-KB physical sector operations. Each physical sector contains eight logical blocks.</code></pre>
</li>
</ul>
<a id="more"></a>

<p><a target="_blank" rel="noopener" href="https://www.thomas-krenn.com/en/wiki/Advanced_Sector_Format_of_Block_Devices">512e Read Operations</a><br>Read operations are very simple compared to write operations:<br>The host would like to read a 512-byte block.<br>The controller loads the complete 4KB sector containing the requested 512-byte block.<br>The controller extracts the data from the 512-byte block and delivers it in the corresponding format to the host.</p>
<p>512e Write operations use the “read-modify-write” method:<br>The host would like to write a 512-byte block.<br>The controller selects a suitable 4KB sector and loads it completely. (read)<br>The controller modifies 512 bytes in the 4KB sector. (modify)<br>The controller writes the modified 4KB sector to the hard drive. (write)</p>
<p>Compatibility<br>In order to keep performance constant, it is necessary that partitions on 512e hard drives be properly aligned (see also Partition Alignment) so that a 4KB sector contains exactly eight 512-byte blocks. Newer operating systems already correctly align the partitions (Windows &gt; Vista SP1; Linux &gt; 2.6.31 (fdisk &gt; 1.2.3))[1]. With the correct alignment, write operations can be optimally cached by the hard drive controller, which optimizes performance.</p>
<p>In <a target="_blank" rel="noopener" href="https://lwn.net/Articles/322777">This</a>, more details and add new one,the preamble</p>
<ul>
<li>Synchronization/Data Address Mark (Sync/DAM)<pre><code>  * The Sync/DAM field indicates the beginning of the sector and identifies the sector’s number, location, and status.</code></pre>
</li>
<li>User data<pre><code>  * The User data field contains actual stored data.</code></pre>
</li>
<li>Error correcting code (ECC)<pre><code>  * The ECC field contains error-correcting code that is used to recover user data that mightbe damaged during the read or write operation.</code></pre>
</li>
<li>Gap<pre><code>  * The Gap field is used to separate sectors from each other.</code></pre>
</li>
<li>Physical sector<pre><code>  * Physical sector is the minimum amount of data that the HDD can read from or write to the physical media in a single I/O operation. For Advanced Format HDDs, the physical sector size is 4 KB.</code></pre>
</li>
<li>Logical sector<pre><code>  * Logical sector is the addressable logical block, which is the minimum amount of data that the HDD can address. This amount is also the minimum amount of data that the host system can deliver to or request from the HDD in a single I/O operation. Advanced Format HDDs support 512-bytes and 4-KB logical sector sizes.</code></pre>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">------------------------------------------<br>|P|D|  Data Bytes(512/4096Bytes  | ECC |G|<br>------------------------------------------<br>P= Preamble<br>D= Data sync mark<br>ECC= Error Correcting Code<br>G= Inter Sector Gap<br></code></pre></td></tr></table></figure>
<p>I think the OS kernel could not recognise ECC/G/P/D and no need to know them, maybe driver will know them. it ‘s the hardware design<br>In hardware that means there is ECC to check hardware read/write signals.</p>
<p>long-data-sector (512,520,528,4096,4112,4160,4224) will show size in logic sector</p>
<h4 id="mpt3sas-mpt2sas-driver-parameter"><a href="#mpt3sas-mpt2sas-driver-parameter" class="headerlink" title="mpt3sas/mpt2sas driver parameter"></a>mpt3sas/mpt2sas driver parameter</h4><p>parm: command_retry_count: Device discovery TUR command retry count: (default=144) (int)<br>retry count default was 144, it ‘s too large</p>
<p>parm: max_queue_depth: max controller queue depth (int) </p>
<p>The Linux “scatter/gather” table size needs to be large enough to allow IO_SIZE IO, if possible. For most drivers, this typically requires a “sg_tablesize” value of 256 or greater for 4MB IO. Different vendors have different defaults for this parameter, and may require a modprobe.conf entry to increase the value. The QLogic driver defaults to a value of 1024. For Emulex, a modeprobe.conf entry needs to be added to increase the value to 256 or greater, such as:<br>options lpfc lpfc_sg_seg_cnt=256</p>
<p>#The LSI SAS driver, “mpt2sas”, uses the parameter named “max_sgl_entries” to control this value.<br>#Its maximum value in RHEL 6.x currently only 128<br>#options mpt2sas max_sgl_entries=256<br>#Try to upgrade to upgrade mpt3sas 27.00.01.00<br>LSI 9300-8e could not modify from mpt3sas 27.00.01.00, I have not LSI 9400-8e to test</p>
<p>mpt3sas 60 x SATA HDD JBOD<br>each hdd has a fio process<br>R: 5.3GB/s<br>W: 5.3GB/s<br>RW: 3GB/s,3GB/s<br>randread: 11k<br>randwrite: 91k<br>randrw: 7.7k/7.7k</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ lspci | grep 2308<br>01:00.0 Serial Attached SCSI controller: LSI Logic / Symbios Logic SAS2308 PCI-Express Fusion-MPT SAS-2 (rev 05)<br>$ cat /sys/devices/pci0000:00/0000:00:02.1/0000:01:00.0/host1/scsi_host/host1/sg_tablesize<br>128<br><br><span class="hljs-built_in">echo</span> 256 &gt; /sys/devices/pci0000:00/0000:00:02.1/0000:01:00.0/host1/scsi_host/host1/sg_tablesize<br><span class="hljs-built_in">echo</span>: write error: Input/output error<br></code></pre></td></tr></table></figure>

<h4 id="mpt3sas-driver-install"><a href="#mpt3sas-driver-install" class="headerlink" title="mpt3sas driver install"></a>mpt3sas driver install</h4><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment">#compile to local</span><br>$ make -j4 CONFIG_DEBUG_INFO=<span class="hljs-number">1</span> -C <span class="hljs-regexp">/lib/m</span>odules<span class="hljs-regexp">/$(uname -r)/</span>build M=<span class="hljs-regexp">/lsi-9300-8e/m</span>pt3sas<br><br><span class="hljs-comment">#compile to local kernel </span><br>$ make -j4 CONFIG_DEBUG_INFO=<span class="hljs-number">1</span> -C <span class="hljs-regexp">/lib/m</span>odules<span class="hljs-regexp">/$(uname -r)/</span>build M=<span class="hljs-regexp">/lsi-9300-8e/m</span>pt3sas modules_install<br></code></pre></td></tr></table></figure>
<h5 id="Driver-Build-Instructions"><a href="#Driver-Build-Instructions" class="headerlink" title="Driver Build Instructions"></a><a target="_blank" rel="noopener" href="//docs.broadcom.com/docs-and-downloads/host-bus-adapters/host-bus-adapters-common-files/README_FOR_LinuxMPT_SAS2_RHEL5_SLES10_P9-zip.pdf">Driver Build Instructions</a></h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><code class="hljs bash">The following examples show how to configure and build the LSI Fusion-MPT driver(s) as<br>kernel modules. In this example the driver version is (01.255.06.00-1). Here is the procedure<br>to build the drivers out of kernel tree:<br> Extract the packaging from Linux system:<br> <span class="hljs-comment"># tar -zxvf mpt2sas-release.tar.gz</span><br> <span class="hljs-comment"># tar -zxvf mpt2sas-01.255.06.00.tar.gz</span><br> <span class="hljs-comment"># cd mpt2sas</span><br> <span class="hljs-comment"># ./compile</span><br> <span class="hljs-comment"># ./load</span><br>Alternatively, here is the procedure to build driver <span class="hljs-keyword">in</span> kernel tree<br>1. From the /usr/src/linux directory, ensure a clean kernel <span class="hljs-built_in">source</span> tree by executing the<br>following <span class="hljs-built_in">command</span>:<br> <span class="hljs-comment"># make mrproper</span><br>2. From the /usr/src/linux directory, run the normal kernel configuration routine:<br> <span class="hljs-comment"># make oldconfig</span><br> or:<br> <span class="hljs-comment"># make config</span><br> or:<br> <span class="hljs-comment"># make menuconfig</span><br> or:<br> <span class="hljs-comment"># make xconfig</span><br>3. Here are the directions <span class="hljs-keyword">for</span> finding the entry <span class="hljs-keyword">in</span> menuconfig ncurses display<br> Device Drivers ---&gt;<br> SCSI device support ---&gt;<br> SCSI low-level drivers ---&gt;<br> &lt;M&gt; LSI MPT Fusion SAS 2.0 Device Driver<br> (128) LSI MPT Fusion Max number of SG Entries (16 - 128) (NEW)<br> [*] LSI MPT Fusion logging facility<br> On the sub menu, select the <span class="hljs-string">&quot;LSI MPT Fusion SAS 2.0 Device Driver&quot;</span> line,<br> and <span class="hljs-keyword">then</span> enter <span class="hljs-string">&quot;m&quot;</span> to configure <span class="hljs-keyword">for</span> building this support as a module.<br> (Alternatively, you can enter <span class="hljs-string">&quot;y&quot;</span> to have this support built<br> into the kernel.)<br> NOTES:<br> o CONFIG_SCSI_MPT2SAS_MAX_SGE: This option allows you to specify the<br> maximum number of scatter-gather entries per I/O. The driver default<br> is 128, <span class="hljs-built_in">which</span> matches MAX_HW_SEGMENTS. However, it may decreased<br> down to 16. Decreasing this parameter will reduce memory requirements<br> on a per controller instance.<br> o CONFIG_SCSI_MPT2SAS_LOGGING: This turns on a logging facility.<br>4. Save the kernel configuration changes. Follow any post configuration instructions, and <span class="hljs-keyword">do</span><br>everything needed on your platform to rebuild the kernel. This typically includes:<br><br> <span class="hljs-comment"># make dep</span><br> and:<br> <span class="hljs-comment"># make bzImage # varies on non-Intel platforms</span><br>5. Rebuild the kernel modules:<br> <span class="hljs-comment"># make modules</span><br>6. Optionally, (and potentially dangerous!), <span class="hljs-keyword">do</span> everything needed on your platform to install a<br>newly built kernel. (possibly temporarily, <span class="hljs-keyword">for</span> sanity testing)<br> Be careful with this step, and be sure you know what you<span class="hljs-string">&#x27;re doing!</span><br><span class="hljs-string"> It is easy to wipe out a good/stable kernel from this point forward</span><br><span class="hljs-string"> in the procedure!</span><br><span class="hljs-string">7. (Re)Install newly compiled kernel modules:</span><br><span class="hljs-string"> # make modules_install</span><br><span class="hljs-string"> The output from the last step should look something like this:</span><br><span class="hljs-string"> Installing modules under /lib/modules/2.6.30/block</span><br><span class="hljs-string"> Installing modules under /lib/modules/2.6.30/net</span><br><span class="hljs-string"> Installing modules under /lib/modules/2.6.30/ipv4</span><br><span class="hljs-string"> Installing modules under /lib/modules/2.6.30/scsi</span><br><span class="hljs-string"> Installing modules under /lib/modules/2.6.30/fs</span><br><span class="hljs-string"> Installing modules under /lib/modules/2.6.30/fs</span><br><span class="hljs-string"> Installing modules under /lib/modules/2.6.30/cdrom</span><br><span class="hljs-string"> Installing modules under /lib/modules/2.6.30/video</span><br><span class="hljs-string"> Installing modules under /lib/modules/2.6.30/net</span><br><span class="hljs-string"> Installing modules under /lib/modules/2.6.30/misc</span><br><span class="hljs-string">8. Update your /boot sector with the new System.map and bzImage, re-create your ramdisk</span><br><span class="hljs-string">image (refer to your vendor literature), and update your boot manager--i.e., lilo.conf,</span><br><span class="hljs-string">grub.conf. If you are using lilo, you must run lilo -v prior to reboot.</span><br><span class="hljs-string">9. Shut down the system:</span><br><span class="hljs-string"></span><br><span class="hljs-string"> Example:</span><br><span class="hljs-string"> # shutdown -r now</span><br><span class="hljs-string"> and then reboot with the newly built Linux kernel</span><br></code></pre></td></tr></table></figure>

<h4 id="Qlogic-driver-setting"><a href="#Qlogic-driver-setting" class="headerlink" title="Qlogic driver setting"></a>Qlogic driver setting</h4><p>Adpater reset time e.g. Qlogic reset time<br>echo options qla2xxx ql2xextended_error_logging=1 qlport_down_retry=10 ql2xloginretrycount=10 &gt;&gt;  /etc/modprobe.d/qlogic.conf<br>Multipath check_timeout  (reduce to 10 seconds from default 60 seconds)</p>
<p>Set the max_segments for HBA driver<br>| HBA       | Module Parameter|<br>|———–|:—————:|<br>| LSI       |  max_sgl_entries|<br>| Emulex    |  lpfc_sg_seg_cnt|<br>| ib_srp    |  cmd_sg_entries |<br>| Brocade   |  bfa_io_max_sge |</p>
<p>Set max_hw_sectors_kb for HBA driver<br>| HBA       | Module Parameter|<br>|———–|:—————:|<br>| LSI       |  max_sectors    |<br>| ib_srp    |  max_sect       |<br>| Brocade   |  max_xfer_size  |</p>
<p>mpt3sas<br>max_sectors:max sectors, range 64 to 32767  default=32767 (ushort)</p>
<h4 id="SAN-controller-setting-MD3460"><a href="#SAN-controller-setting-MD3460" class="headerlink" title="SAN controller setting MD3460"></a>SAN controller setting MD3460</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">RAID 6               8+2<br>segment              128K<br>cache block size     32K<br>cache flush          90%<br>Write cache mirror   enabled<br>Read cache           disabled<br></code></pre></td></tr></table></figure>

<h3 id="rebuild-feature"><a href="#rebuild-feature" class="headerlink" title="rebuild feature"></a>rebuild feature</h3><p>In May 2012, with 3.5-inch hard drive capacities reaching 4TB, the T10 working group approved including Rebuild Assist in the SCSI SBC-3 specification. In August 2013, the SATA-IO committee adopted TPR-045 (Rebuild Assist) as part of the SATA 3.2 specification.”) so with a Rebuild Assist supported RAID controller: can copy good data from a failing drive to a new drive and only need to rebuild the ‘bad’ portion, supposably resulting in quicker rebuild times</p>
<p>Dell PERC not recommand enable it, because it will cause data loss.</p>
<h3 id="Linux-setting"><a href="#Linux-setting" class="headerlink" title="Linux setting"></a>Linux setting</h3><h4 id="block-driver"><a href="#block-driver" class="headerlink" title="block driver"></a>block driver</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ $ cat /sys/block/sdX/device/state<br>running<br><br>$ cat /sys/block/sdz/device/queue_depth<br>254<br><br>$ cat /sys/block/sda/queue/nomerges <br><span class="hljs-comment">#set nomerges to 0 for HDDs or to 1 for SSDs</span><br><br>$ cat /sys/block/sdd/queue/add_random <br><span class="hljs-comment">#The default value is 1. Set add_random=0 for SSDs because random entropy pool does not optimize SSD performance</span><br><br><span class="hljs-comment"># check scsi state</span><br>$ cat /sys/block/sdX/device/state<br>running<br><br>$ <span class="hljs-built_in">echo</span> 30 &gt; /sys/block/sdX/device/timeout<br><span class="hljs-comment"># I don &#x27;t think set the value too long</span><br><br>$ cat /etc/udev/rules.d/50-udev.rules<br>ACTION==<span class="hljs-string">&quot;add&quot;</span>, SUBSYSTEM==<span class="hljs-string">&quot;scsi&quot;</span> , SYSFS&#123;<span class="hljs-built_in">type</span>&#125;==<span class="hljs-string">&quot;0|7|14&quot;</span>, RUN+=<span class="hljs-string">&quot;/bin/sh -c &#x27;echo 30 &gt; /sys/block/%k/device/timeout&#x27;&quot;</span> <br><br><span class="hljs-comment">### another exapmle</span><br>KERNEL==<span class="hljs-string">&quot;sdc5&quot;</span>, OWNER=<span class="hljs-string">&quot;student&quot;</span>, GROUP=<span class="hljs-string">&quot;student&quot;</span>, MODE=<span class="hljs-string">&quot;0600&quot;</span><br>ACTION==<span class="hljs-string">&quot;add&quot;</span>, KERNEL==<span class="hljs-string">&quot;sd*&quot;</span>, SYSFS==<span class="hljs-string">&quot;4317210A2880EF89&quot;</span>, SYMLINK+=<span class="hljs-string">&quot;fedora%n&quot;</span><br>ACTION==<span class="hljs-string">&quot;add&quot;</span>, KERNEL==<span class="hljs-string">&quot;sdb[1-9]&quot;</span>, RUN=<span class="hljs-string">&quot;/usr/bin/wall  SCSI DEVICE ADDED&quot;</span><br>ACTION==<span class="hljs-string">&quot;remove&quot;</span>, KERNEL==<span class="hljs-string">&quot;sdb[1-9]&quot;</span>, RUN=<span class="hljs-string">&quot;/usr/bin/wall SCSI DEVICE REMOVED&quot;</span><br>ACTION==<span class="hljs-string">&quot;add&quot;</span>, KERNEL==<span class="hljs-string">&quot;sdb[1-9]&quot;</span>, SYMLINK=<span class="hljs-string">&quot;scsi%n&quot;</span><br>ACTION==<span class="hljs-string">&quot;remove&quot;</span>, KERNEL==<span class="hljs-string">&quot;sdb[1-9]&quot;</span>, SYMLINK=<span class="hljs-string">&quot;scsi%n&quot;</span><br>ACTION==<span class="hljs-string">&quot;add&quot;</span>, KERNEL==<span class="hljs-string">&quot;sd*[!0-9]&quot;</span>, SYSFS&#123;vendor&#125;==<span class="hljs-string">&quot;WDC WD32&quot;</span>, RUN+=<span class="hljs-string">&quot;/bin/sh -c &#x27;echo 128 &gt; /sys/block/%k/queue/max_sectors_kb&#x27;&quot;</span><br>ACTION==<span class="hljs-string">&quot;add&quot;</span>, KERNEL==<span class="hljs-string">&quot;sd*[!0-9]&quot;</span>, SYSFS&#123;vendor&#125;==<span class="hljs-string">&quot;WDC WD32&quot;</span>, RUN+=<span class="hljs-string">&quot;/usr/bin/wall /sys/block/%k/queue/max_sectors_kb set to 128&quot;</span><br></code></pre></td></tr></table></figure>
<p>KERNELS - match against the kernel name for the device, or the kernel name for any of the parent devices<br>SUBSYSTEMS - match against the subsystem of the device, or the subsystem of any of the parent devices<br>DRIVERS - match against the name of the driver backing the device, or the name of the driver backing any of the parent devices<br>ATTRS - match a sysfs attribute of the device, or a sysfs attribute of any of the parent devices</p>
<p>udevadm info -a -p $(udevadm info -q path -n /dev/sda)</p>
<p>About <a target="_blank" rel="noopener" href="https://library.netapp.com/ecmdocs/ECMP1196980/html/GUID-A055B184-0876-4376-9C75-35FE8C9BE832.html#:~:text=Queue%20depth%20is%20the%20number,depth%20equates%20to%20better%20performance.">queue_depth</a><br>Note: To estimate the queue depth needed to achieve a certain I/O per second throughput, use this formula.<br>Needed queue depth = (Number of I/O per second) x (Response time)<br>For example, if you need 40,000 I/O per second with a response time of 3 milliseconds, the needed queue depth = 40,000 x (.003) = 120.</p>
<h4 id="block-driver-2"><a href="#block-driver-2" class="headerlink" title="block driver 2"></a><a target="_blank" rel="noopener" href="https://library.netapp.com/ecmdocs/ECMP12404601/html/GUID-436F7286-AD26-4A8D-A2D1-2BC8B5CFC023.html">block driver 2</a></h4><p><a target="_blank" rel="noopener" href="https://access.redhat.com/solutions/43861">redhat doc</a></p>
<ul>
<li>max_hw_sectors_kb (RO) - This parameter sets the maximum number of kilobytes that the hardware allows for request.<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># ubuntu 18.04 server, sas 10TB 512e HDD</span><br>$ cat <span class="hljs-regexp">/sys/</span>block<span class="hljs-regexp">/sda/</span>queue/max_hw_sectors_kb <br><span class="hljs-number">16383</span><br></code></pre></td></tr></table></figure>

</li>
</ul>
<p>max_sectors_kb = avgrq-sz(iostat)</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://access.redhat.com/solutions/2150101">max_sectors_kb</a> (RW) - This parameter sets the maximum number of kilobytes that the block layer allows for a file system request. The value of this parameter must be less than or equal to the maximum size allowed by the hardware. The kernel also places an upper bound on this value with the BLK_DEF_MAX_SECTORS macro. This value varies from distribution to distribution, for example, it is 1024 on RHEL 6.3, 2048 on SLES 11 SP2.</p>
<pre><code>  * This specifies the maximum I/O size that the host will issue to the target storage. linxu 4.1x default value was 1280, Typically it is 512KiB (512 Bytes,1024 sectors, 4KN, 1024 sectors means 4096KiB)</code></pre>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ cat /sys/block/sda/queue/max_sectors_kb <br>1280<br></code></pre></td></tr></table></figure>
</li>
<li><p>max_segments (RO) - This parameter enables low level driver to set an upper limit on the number of hardware data segments in a request. In the HBA drivers, this is also known as sg_tablesize.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ cat /sys/block/sda/queue/max_segments<br>128<br></code></pre></td></tr></table></figure>
</li>
<li><p>max_segment_size (RO) - This parameter enables low level driver to set an upper limit on the size of each data segment in an I/O request in bytes. If clustering is enabled on the low level driver it is set to 65536 or it is set to system PAGE_SIZE by default, which is typically 4K. The maximum I/O size is determined by the following:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ cat /sys/block/sda/queue/max_segment_size <br>65536<br></code></pre></td></tr></table></figure>
<p>MAX_IO_SIZE_KB = MIN(max_sectors_kb, (max_segment_size * max_segments)/1024)<br>1280 KB       = MIN(1280, (65536*128)/1024) = MIN(1280, 8192)</p>
</li>
</ul>
<p>In this command, PAGE_SIZE is architecture independent. It is 4096 for x86_64.</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs angelscript">#<span class="hljs-keyword">or</span> you could <span class="hljs-keyword">set</span> it <span class="hljs-keyword">from</span> <span class="hljs-number">1280</span> to <span class="hljs-number">8192</span><br>$ echo <span class="hljs-number">8192</span> &gt; max_sectors_kb<br></code></pre></td></tr></table></figure>

<ul>
<li><p>physical_block_size</p>
<ul>
<li>Smallest internal unit on which the device can operate<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 4KN HDD, there is no 512n in the large capacity HDD</span><br>$ cat /sys/block/sdj/queue/physical_block_size<br>4096<br></code></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>logical_block_size</p>
<ul>
<li>Used externally to address a location on the device<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 4KN</span><br>$ cat /sys/block/sdj/queue/logical_block_size<br>4096<br><br><span class="hljs-comment"># 512e</span><br>$ cat /sys/block/sdi/queue/logical_block_size<br>512<br></code></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>alignment_offset</p>
<ul>
<li>The number of bytes that the beginning of the Linux block device (partition/MD/LVM device) is offset from the underlying physical alignment<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ cat /sys/block/sdj/alignment_offset<br>0<br></code></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>minimum_io_size</p>
<ul>
<li>The device’s preferred minimum unit for random I/O</li>
<li>The minimum_io_size and optimal_io_size values for /dev/sdX devices are retrieved by the kernel by inquiring the storage vendor-provided I/O Limits within the device VPD pages.<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ cat /sys/block/sdj/queue/minimum_io_size<br>4096<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="http://fibrevillage.com/storage/563-storage-i-o-alignment-and-size">optimal_io_size</a></p>
<ul>
<li>The device’s preferred unit for streaming I/O<ul>
<li>Only one layer in the I/O stack should adjust for a non-zero alignment_offset; once a layer adjusts accordingly, it will export a device with an alignment_offset of zero. </li>
<li>A striped Device Mapper (DM) device created with LVM must export a minimum_io_size and optimal_io_size relative to the stripe count (number of disks) and user-provided chunk size.<br>Note<br>Red Hat Enterprise Linux 7 cannot distinguish between devices that don’t provide I/O hints and those that do so with alignment_offset= 0 and optimal _io_size= 0 . Such a device might be a single SAS 4K device; as such, at worst 1MB of space is lost at the start of the disk.</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ cat /sys/block/sdj/queue/optimal_io_size<br>0<br></code></pre></td></tr></table></figure>
<p>If wanting to change the optimal_io_size on a dm(multipath) device add parameter max_sectors_kb to the /etc/multipath.conf file for the specific storage array (and change the underlying sd devices using an udev rule).</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># I don &#x27;t know why only zero in [1] and [2] location</span><br><br>$ sg_inq -p 0xb0 /dev/sdacn<br>VPD INQUIRY: Block limits page (SBC)<br>  Optimal transfer length granularity: 1 blocks   &lt;&lt; [1] block-size x this field = minimal_io_size, block-size defined <span class="hljs-keyword">in</span> READCAP <span class="hljs-built_in">command</span> below.<br>  Maximum transfer length: 8192 blocks<br>  Optimal transfer length: 8192 blocks            &lt;&lt; [2] block-size x this field = optimal_io_size<br>  Maximum prefetch, xdread, xdwrite transfer length: 0 blocks<br><br>$ sg_readcap -16 /dev/sdacn<br>Read Capacity results:<br>   Protection: prot_en=0, p_type=0, p_i_exponent=0<br>   Thin provisioning: tpe=0, tprz=0<br>   Last logical block address=4194303 (0x3fffff), Number of logical blocks=4194304<br>   Logical block length=512 bytes                &lt;&lt; [3] this is block size (length), multiplier <span class="hljs-keyword">for</span> above fields.<br>   Logical blocks per physical block exponent=0<br>   Lowest aligned logical block address=0<br>Hence:<br>   Device size: 2147483648 bytes, 2048.0 MiB, 2.15 GB<br><br>$ grep -v <span class="hljs-string">&quot;zz&quot;</span> /sys/block/sdacn/queue/*io_size<br>/sys/block/sdacn/queue/minimum_io_size:512       &lt;&lt; [1] 1 block     x [3] 512-bytes/block =     512 bytes.<br>/sys/block/sdacn/queue/optimal_io_size:4194304   &lt;&lt; [2] 8192 blocks x [3] 512-bytes/block = 4194034 bytes.<br></code></pre></td></tr></table></figure>
<p>This does hint to the fact that it is possible to see different optimal_io_size for the same LUN across different access paths. This can happen (from storage controller configuration issues, manual changes (echo) or a trigger of udev rules), which could cause an exceptionally high optimal_io_size for the dm(multipath) device or paths which will not function. If this is seen, and the sg_readcap and sg_inq is lining up with what is in /sys, it is likely due to a storage controller configuration issue, and the system’s SAN vendor should be contacted for analysis. Below is an example of what this would look like, noting that 4278190080 is the LCM (least common multiple) of 4177920 and 16777216</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ multipath -ll disk1<br>disk1 (wwid_entry_omitted) dm-108 3PARdata,VV<br>size=80G features=<span class="hljs-string">&#x27;1 queue_if_no_path&#x27;</span> hwhandler=<span class="hljs-string">&#x27;1 alua&#x27;</span> wp=rw<br>`-+- policy=<span class="hljs-string">&#x27;round-robin 0&#x27;</span> prio=1 status=active<br>  |- 0:0:0:152 sdcg 69:64   active ready running<br>  |- 1:0:0:152 sdjc 8:352   active ready running<br>  |- 0:0:1:152 sdfr 130:208 active ready running<br>  `- 1:0:1:152 sdmn 69:496  active ready running<br><br>$ <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> dm-108 sdcg sdjc sdfr sdmn ; <span class="hljs-keyword">do</span> cat /sys/block/<span class="hljs-variable">$i</span>/queue/optimal_io_size ; <span class="hljs-keyword">done</span><br>4278190080     &lt;&lt; LCM of 16777216 and 4177920<br>16777216       &lt;&lt; Optimal transfer length: 32768 blocks x Logical block length=512 bytes<br>16777216       &lt;&lt; Optimal transfer length: 32768 blocks x Logical block length=512 bytes<br>4177920        &lt;&lt; Optimal transfer length: 8160 blocks x Logical block length=512 bytes<br>4177920        &lt;&lt; Optimal transfer length: 8160 blocks x Logical block length=512 bytes<br></code></pre></td></tr></table></figure>

<h5 id="parted-alignment"><a href="#parted-alignment" class="headerlink" title="parted alignment"></a>parted alignment</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ parted /dev/sdX <span class="hljs-string">&#x27;unit s print&#x27;</span><br><br><span class="hljs-comment">#Is the partition align ?</span><br>$ parted /dev/sdX align-check optimal 1<br>1 aligned<br><br><span class="hljs-comment">#auto align</span><br>$ parted -a optimal /dev/sdX mkpart primary 0% xxTB<br><span class="hljs-comment"># cylinder,none,minimal,optimal</span><br></code></pre></td></tr></table></figure>
<p>GUID Partition Table Scheme<br><img src="/img/GUID_Partition_Table_Scheme.png"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">-------------<br>LBA0 prrotective MBR<br>-------------<br>LBA1 Primary GPT header<br>-------------<br>LBA2 Entry1|Entry2|Entry3|Entry4<br>-------------<br>LBA3 Entries 5-128<br>------------<br>LBA34  Partirion1,2..<br>LBA-34 remaining Partitions<br>------------<br>LBA-33  Entry1|Entry2|entry3|Entry4<br>------------<br>LBA-2   Entries 5-128<br>------------<br>LBA-1   Secondary GPT Header<br>----<br></code></pre></td></tr></table></figure>

<h5 id="Hot-plugin-the-scsi-device"><a href="#Hot-plugin-the-scsi-device" class="headerlink" title="Hot plugin the scsi device"></a>Hot plugin the scsi device</h5><p>What is h c t l<br>          h == hostadapter id (first one being 0)<br>          c == SCSI channel on hostadapter (first one being 0)<br>          t == ID (target)<br>          l == LUN (first one being 0)<br>Generic SCSI devices can also be accessed via the bsg driver in Linux. By default, the bsg driver’s device node names  are  of  the  form ‘/dev/bsg/H:C:T:L’.  So,  for example, the SCSI device shown by this utility on a line starting with the tuple ‘6:0:1:2’ could be accessed via the bsg driver with the ‘/dev/bsg/6:0:1:2’ device node name.</p>
<h5 id="Add-the-scsi-device"><a href="#Add-the-scsi-device" class="headerlink" title="Add the scsi device"></a>Add the scsi device</h5><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs angelscript">echo <span class="hljs-string">&quot;- - -&quot;</span> &gt; /sys/<span class="hljs-keyword">class</span>/<span class="hljs-symbol">scsi_host</span>/<span class="hljs-symbol">host</span>&lt;<span class="hljs-symbol">h</span>&gt;/<span class="hljs-symbol">scan</span><br><span class="hljs-symbol">echo</span> &quot;<span class="hljs-symbol">c</span> <span class="hljs-symbol">t</span> <span class="hljs-symbol">l</span>&quot; &gt;  /<span class="hljs-symbol">sys</span>/<span class="hljs-symbol">class</span>/<span class="hljs-symbol">scsi_host</span>/<span class="hljs-symbol">host</span>&lt;<span class="hljs-symbol">h</span>&gt;/<span class="hljs-symbol">scan</span><br></code></pre></td></tr></table></figure>

<h5 id="Refresh-the-scsi-device"><a href="#Refresh-the-scsi-device" class="headerlink" title="Refresh the scsi device"></a>Refresh the scsi device</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> 1 &gt; /sys/block/sdau/device/rescan<br><span class="hljs-built_in">echo</span> 1 &gt; /sys/class/scsi_device/h:c:t:l/device/rescan<br></code></pre></td></tr></table></figure>

<h5 id="Remove-the-scsi-device"><a href="#Remove-the-scsi-device" class="headerlink" title="Remove the scsi device"></a>Remove the scsi device</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> 1 &gt; /sys/class/scsi_device/h:c:t:l/device/delete<br><span class="hljs-built_in">echo</span> 1 &gt; /sys/block/&lt;dev&gt;/device/delete<br></code></pre></td></tr></table></figure>

<h4 id="IO-schdule"><a href="#IO-schdule" class="headerlink" title="IO schdule"></a>IO schdule</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash">schdule = deadline<br>nr_request = 1024<br>max_sector_kb = 1024<br>read_ahead_kb = 8192<br>rq_affinity = 2<br><span class="hljs-comment"># For storage configurations that need to maximize distribution of completion processing setting this option to &#x27;2&#x27; forces the completion to run on the requesting cpu (bypassing the &quot;group&quot; aggregation logic).</span><br><br>vm.dirty_ratio = 40<br><span class="hljs-comment"># dirty_ratio     &quot;Dirty&quot; memory is that waiting to be written to disk. dirty_ratio is the number of memory pages at which a process will start writing out dirty data, expressed as a percentage out of the total free and reclaimable pages. A default of 20 is reasonable. Increase to 40 to improve throughput, decrease it to 5 to 10 to improve latency, even lower on systems with a lot of memory.</span><br><br>vm.dirty_background_ratio = 20<br><span class="hljs-comment"># dirty_background_ratio  Similar, but this is the number of memory pages at which the kernel background flusher thread will start writing out dirty data, expressed as a percentage out of the total free and reclaimable pages. Set this lower than dirty_ratio, dirty_ratio/2 makes sense and is what the kernel does by default. This page shows that dirty_ratio has the greater effect. Tune dirty_ratio for performance, then set dirty_background_ratio to half that value.</span><br><br>vm.vfs_cache_pressure = 50<br>This sets the <span class="hljs-string">&quot;pressure&quot;</span> or the importance the kernel places upon reclaiming memory used <span class="hljs-keyword">for</span> caching directory and inode objects. The default of 100 or relative <span class="hljs-string">&quot;fair&quot;</span> is appropriate <span class="hljs-keyword">for</span> compute servers. Set to lower than 100 <span class="hljs-keyword">for</span> file servers on <span class="hljs-built_in">which</span> the cache should be a priority. Set higher, maybe 500 to 1000, <span class="hljs-keyword">for</span> interactive systems.<br><br>overcommit_memory       Allows <span class="hljs-keyword">for</span> poorly designed programs <span class="hljs-built_in">which</span> malloc() huge amounts of memory <span class="hljs-string">&quot;just in case&quot;</span> but never really use it. Set this to 0 (disabled) unless you really need it.<br></code></pre></td></tr></table></figure>

<h4 id="deadline-parameters"><a href="#deadline-parameters" class="headerlink" title="deadline parameters"></a><a target="_blank" rel="noopener" href="https://cromwell-intl.com/open-source/performance-tuning/disks.html">deadline parameters</a></h4><p>fifo_batch      Number of read or write operations to issue in one batch.  Lower values may further reduce latency. Higher values can increase throughput on rotating mechanical disks, but at the cost of worse latency. You selected the deadline scheduler to limit latency, so you probably don’t want to increase this, at least not by very much.</p>
<p>read_expire     Number of milliseconds within which a read request should be served. Reduce this from the default of 500 to 100 on a system with interactive users.</p>
<p>rite_expire     Number of milliseconds within which a write request should be served.<br>Leave at default of 5000, let write operations be done asynchronously in the background unless your specialized application uses many synchronous writes.</p>
<p>writes_starved  Number read batches that can be processed before handling a write batch. Increase this from default of 2 to give higher priority to read operations.</p>
<p>nr_requests     Maximum number of read and write requests that can be queued at one time before the next process requesting a read or write is put to sleep. Default value of 128 means 128 read requests and 128 write requests can be queued at once. Larger values may increase throughput for workloads writing many small files, smaller values increase throughput with larger I/O operations. You could decrease this if you are using latency-sensitive applications, but then you shouldn’t be using NOOP if latency is sensitive!</p>
<p>read_ahead_kb   Number of kilobytes the kernel will read ahead during a sequential read operation. 128 kbytes by default, if the disk is used with LVM the device mapper may benefit from a higher value. If your workload does a lot of large streaming reads, larger values may improve performance.</p>
<p>max_sectors_kb  Maximum allowed size of an I/O request in kilobytes, which must be within these bounds:<br>Min value = max(1, logical_block_size/1024)<br>Max value = max_hw_sectors_kb</p>
<p>rotational      Should be 0 (no) for solid-state disks, but some do not correctly report their status to the kernel. If incorrectly set to 1 for an SSD, set it to 0 to disable unneeded scheduler logic meant to reduce number of seeks.</p>
<h3 id="scsi-driver-error-handaling-EH"><a href="#scsi-driver-error-handaling-EH" class="headerlink" title="scsi_driver error handaling (EH)"></a>scsi_driver error handaling (EH)</h3><p>scsi driver error Handaling (EH) timeout – eh_timeout (from default 10 second to 5 seconds)<br>HBA reset time - eh_deadline (from disable/0 to 5 seconds, default was off)</p>
<p>The SCSI error handling (EH) mechanism attempts to perform error recovery on failed SCSI devices. The SCSI host object eh_deadline parameter enables you to configure the maximum amount of time for the recovery. After the configured time expires, SCSI EH stops and resets the entire host bus adapter (HBA).</p>
<h3 id="iostat"><a href="#iostat" class="headerlink" title="iostat"></a><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/4458183/how-the-util-of-iostat-is-computed">iostat</a></h3><p>%util = blkio.ticks / deltams * 100%</p>
<p>deltams is the time elapsed since last snapshot in ms. It uses CPU stats from /proc/stat presumably because it gives better results than to rely on system time, but I don’t know for sure. (Side note: for some reason the times are divided by HZ, while the documentation states it’s in USER_HZ, I don’t understand that.)<br>blkio.ticks is “# of milliseconds spent doing I/Os”, from /proc/diskstats docs:</p>
<p>Field  9 – # of I/Os currently in progress<br>  The only field that should go to zero. Incremented as requests are<br>  given to appropriate struct request_queue and decremented as they finish.<br>Field 10 – # of milliseconds spent doing I/Os<br>  This field increases so long as field 9 is nonzero.</p>
<p>struct ext_disk_stats *xds<br>xds-&gt;util</p>
<p>Hypothesis:<br>Simple understand about util% =  IO time/the time(the clock) , if IO time &gt;= the time, the utils = 100%(in 1s), else, that means some times there is no IO ops in this second<br>Why no ops in this sec ? maybe something hang, maybe just no any IO loading</p>
<p>In SAS arch, there are multiple lane/phy in it, if single lane/phy has full IO loading, the util% will reach 100%<br>Yes the device could be parallel, there are a lot of lane/phy are free, but the single lane or phy has full. I think the util% was right</p>
<ol>
<li>The single path(resource) was full (nvme,sas ssd)</li>
<li>If the block device ‘s performance is infinity,  All of CPU/Mem/NIC speed behind it. the bottle-neck not in the block device. some of syscall cause the util%=100% too</li>
<li>Could the util% show the device are busy ? That right, util% is not enough, Could the iostat show the device are busy ? Yes, it can<br>You can watch the await and util%, you could know the device busy or free. it ‘s so easy</li>
</ol>
<p>“iostat was not correct, it just show the wrong value”. That ‘s alarmist for iostat ,and it ‘s so funny</p>
<p>This guy analyzed the code, but it ‘s not enough, he was not understand the iostat output.<br>The <a target="_blank" rel="noopener" href="https://bean-li.github.io/dive-into-iostat/">example</a> is good.</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">ID</span>      Time    Ops                                  in_flight  stamp   stamp_delta           io_ticks            time_in_queue<br><span class="hljs-attribute">0</span>       <span class="hljs-number">100</span>     new request in the queue                <span class="hljs-number">0</span>       <span class="hljs-number">0</span>       no need caculate          <span class="hljs-number">0</span>                 <span class="hljs-number">0</span><br><span class="hljs-attribute">1</span>       <span class="hljs-number">100</span>.<span class="hljs-number">10</span>  another request go to the queue         <span class="hljs-number">1</span>       <span class="hljs-number">100</span>     <span class="hljs-number">100</span>.<span class="hljs-number">10</span>-<span class="hljs-number">100</span> = <span class="hljs-number">0</span>.<span class="hljs-number">1</span>        <span class="hljs-number">0</span>.<span class="hljs-number">1</span>                 <span class="hljs-number">0</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">2</span>       <span class="hljs-number">101</span>.<span class="hljs-number">20</span>  finish the first request                <span class="hljs-number">2</span>       <span class="hljs-number">100</span>.<span class="hljs-number">10</span>  <span class="hljs-number">101</span>.<span class="hljs-number">20</span>-<span class="hljs-number">100</span>.<span class="hljs-number">10</span> = <span class="hljs-number">1</span>.<span class="hljs-number">1</span>     <span class="hljs-number">1</span>.<span class="hljs-number">2</span>(<span class="hljs-number">1</span>.<span class="hljs-number">1</span>+<span class="hljs-number">0</span>.<span class="hljs-number">1</span>)        <span class="hljs-number">0</span>.<span class="hljs-number">1</span>+<span class="hljs-number">1</span>.<span class="hljs-number">1</span>*<span class="hljs-number">2</span> (total <span class="hljs-number">2</span>x io requests) = <span class="hljs-number">2</span>.<span class="hljs-number">3</span><br><span class="hljs-attribute">3</span>       <span class="hljs-number">103</span>.<span class="hljs-number">60</span>  finish the second request               <span class="hljs-number">1</span>       <span class="hljs-number">101</span>.<span class="hljs-number">20</span>  <span class="hljs-number">103</span>.<span class="hljs-number">60</span>-<span class="hljs-number">101</span>.<span class="hljs-number">20</span> = <span class="hljs-number">2</span>.<span class="hljs-number">4</span>     <span class="hljs-number">3</span>.<span class="hljs-number">6</span>                 <span class="hljs-number">2</span>.<span class="hljs-number">3</span>+<span class="hljs-number">2</span>.<span class="hljs-number">4</span>*<span class="hljs-number">1</span>=<span class="hljs-number">4</span>.<span class="hljs-number">7</span><br><span class="hljs-attribute">4</span>       <span class="hljs-number">153</span>.<span class="hljs-number">60</span>  The third request go to the queue       <span class="hljs-number">0</span>       <span class="hljs-number">103</span>.<span class="hljs-number">60</span>  no need caculate        <span class="hljs-number">3</span>.<span class="hljs-number">6</span>                 <span class="hljs-number">4</span>.<span class="hljs-number">7</span><br><span class="hljs-attribute">5</span>       <span class="hljs-number">153</span>.<span class="hljs-number">90</span>  Finish the third request                <span class="hljs-number">1</span>       <span class="hljs-number">153</span>.<span class="hljs-number">60</span>  <span class="hljs-number">153</span>.<span class="hljs-number">90</span> - <span class="hljs-number">153</span>.<span class="hljs-number">60</span> = <span class="hljs-number">0</span>.<span class="hljs-number">3</span>   <span class="hljs-number">3</span>.<span class="hljs-number">9</span>                 <span class="hljs-number">4</span>.<span class="hljs-number">7</span>+<span class="hljs-number">0</span>.<span class="hljs-number">3</span> * <span class="hljs-number">1</span>= <span class="hljs-number">5</span><br></code></pre></td></tr></table></figure>
<p>In 53.9s, All io requests in 3.9s, the other times has no any IO in the queue.<br>io_ticks  –&gt; util %<br>time_in_queue –&gt; avgqu-sz</p>
<h3 id="Re-import-LVM"><a href="#Re-import-LVM" class="headerlink" title="Re-import LVM"></a>Re-import LVM</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ storcli64 /call/fall show<br>$ storcli64 /c0/fall import<br>$ vgscan<br> Reading all physical volumes.  This may take a <span class="hljs-keyword">while</span>...<br>  Found volume group <span class="hljs-string">&quot;xx&quot;</span> using metadata <span class="hljs-built_in">type</span> lvm2<br>$ vgchange -ay<br>  1 logical volume(s) <span class="hljs-keyword">in</span> volume group <span class="hljs-string">&quot;xx&quot;</span> now active<br>$ lvs<br>xx_lv  xx_vg -wi-a-----  1t<br>$ lvdisplay<br></code></pre></td></tr></table></figure>

<h3 id="Advance-reformat-4Kn-SCSI-devs-to-512-Bytes-sector"><a href="#Advance-reformat-4Kn-SCSI-devs-to-512-Bytes-sector" class="headerlink" title="Advance reformat 4Kn SCSI devs to 512 Bytes sector"></a>Advance reformat 4Kn SCSI devs to 512 Bytes sector</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ smartctl -a -d scsi /dev/sdd<br>smartctl 5.43 2012-06-30 r3573 [x86_64-linux-2.6.32-504.30.3.el6_lustre.x86_64] (<span class="hljs-built_in">local</span> build)<br>Copyright (C) 2002-12 by Bruce Allen, http://smartmontools.sourceforge.net<br><br>Vendor:               HGST<br>Product:              HUH728080AL4200<br>Revision:             A7J0<br>User Capacity:        8,001,563,222,016 bytes [8.00 TB]<br>Logical block size:   4096 bytes<br><br>$ smartctl -a -d scsi /dev/sdc<br>smartctl 5.43 2012-06-30 r3573 [x86_64-linux-2.6.32-504.30.3.el6_lustre.x86_64] (<span class="hljs-built_in">local</span> build)<br>Copyright (C) 2002-12 by Bruce Allen, http://smartmontools.sourceforge.net<br><br>Vendor:               HGST<br>Product:              HUH728080AL4200<br>Revision:             A7J0<br>User Capacity:        8,001,563,222,016 bytes [8.00 TB]<br>Logical block size:   512 bytes<br></code></pre></td></tr></table></figure>

<p>Update: If you have a lot of drives to format, it may take a long time with sg_format as it wait until it finished before doing the next one (with a while loop I mean). Useful tip is to use the “-e” flag with sg_format then monitor operations with sg_turs -p but it may hang you tty as well.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ cat /sys/class/block/sdc/queue/hw_sector_size<br>4096<br><br>$ sg_format --format --size=512 /dev/sdc<br>HGST      HUH728080AL4200   A7J0   peripheral_type: disk [0x0]<br>&lt;&lt; supports protection information&gt;&gt;<br>Mode Sense (block descriptor) data, prior to changes:<br>  Number of blocks=1953506646 [0x74702556]<br>  Block size=4096 [0x1000]<br><br>A FORMAT will commence <span class="hljs-keyword">in</span> 10 seconds<br>    ALL data on /dev/sdc will be DESTROYED<br>        Press control-C to abort<br>A FORMAT will commence <span class="hljs-keyword">in</span> 5 seconds<br>    ALL data on /dev/sdc will be DESTROYED<br>        Press control-C to abort<br><br>Format has started<br>FORMAT Complete<br><br>$ sg_turs -v /dev/sdb<br>   <span class="hljs-built_in">test</span> unit ready cdb: 00 00 00 00 00 00<br>   <span class="hljs-built_in">test</span> unit ready:  Descriptor format, current;  Sense key: Not Ready Additional sense: Logical unit not ready, format <span class="hljs-keyword">in</span> progress<br>   Descriptor <span class="hljs-built_in">type</span>: Information    00 00 00 00 00 00 00 00 00 00<br>   Descriptor <span class="hljs-built_in">type</span>: Command specific    0x0000000000000000<br>   device not ready<br><br>$ sg_inq -v /dev/sdb<br><span class="hljs-comment"># it will show the process of percent</span><br><br>$ cat /sys/class/block/sdc/queue/hw_sector_size<br>512<br></code></pre></td></tr></table></figure>

<h4 id="intel-nvme-switch-to-AF"><a href="#intel-nvme-switch-to-AF" class="headerlink" title="intel nvme switch to AF"></a>intel nvme switch to AF</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#change 512 Bytes to 4096 Bytes</span><br>$ nvme format -b 4096 /dev/nvme0n1<br><br>$ isdct start -intelssd 0 Function=NVMEFormat LBAForma=3 SecureEraseSetting=0 ProtectionInformation=0 MetaDataSetting=0<br></code></pre></td></tr></table></figure>

<h4 id="SATA-only-support-512-and-4096-Bytes"><a href="#SATA-only-support-512-and-4096-Bytes" class="headerlink" title="SATA only support 512 and 4096 Bytes"></a>SATA only support 512 and 4096 Bytes</h4><h4 id="long-data-sector"><a href="#long-data-sector" class="headerlink" title="long-data-sector"></a>long-data-sector</h4><p>From vendor spec, only SAS device support this feature<br>You could format the driver logic sector to 4096,4112,4116,4224KB or 512,520 Bytes<br>SATA device only support 4096 or 512</p>
<p>You could get the logic sector size(4096,4112,4224), the size could be read from driver</p>
<h5 id="T10-DIF-PI"><a href="#T10-DIF-PI" class="headerlink" title="T10 DIF/PI"></a><a target="_blank" rel="noopener" href="https://www.seagate.com/files/staticfiles/docs/pdf/whitepaper/safeguarding-data-from-corruption-technology-paper-tp621us.pdf">T10 DIF/PI</a></h5><p>T10 DIF/PI for avoid data corruption from some device no CRC/ECC product, so it check the data consistency from the source to the destination</p>
<p>The user data in ecc memory(512B) —–&gt; SAS HBA/iscsi driver(520B) -&gt; SAS IO expander/or some very complicated network contains HBA or the others -&gt; Storage medium(520)<br>Because the hardware just check from input or output port from itself</p>
<p>The risk of corrupted data falling through the inherent cracks in this protection methodology is significant, and the havoc such undetected, silent data corruption could wreak (lost or inaccurate data, significant downtime) is substantial</p>
<h5 id="About-DIF-520-Bytes"><a href="#About-DIF-520-Bytes" class="headerlink" title="About DIF 520 Bytes"></a><a target="_blank" rel="noopener" href="https://lwn.net/Articles/280023/">About DIF 520 Bytes</a></h5><p>SCSI drives can usually be reformatted to 520-byte sectors, yielding 8 extra bytes per sector.  These 8 bytes have traditionally been used by RAID controllers to store internal protection information.</p>
<p>DIF (Data Integrity Field) is an extension to the SCSI Block Commands that standardizes the format of the 8 extra bytes and defines ways to interact with the contents at the protocol level.  We refer to the extra information as “integrity metadata” or “IMD”.<br>Each 8-byte DIF tuple is split into three chunks:<br>        * a 16-bit guard tag containing a CRC of the 512-byte data portion of the sector.<br>        * a 16-bit application tag which is up for grabs.<br>        * a 32-bit reference tag which contains an incrementing counter for each sector.  For DIF Type 1 it also needs to match the physical LBA on the drive.<br>There are three types of DIF defined: Type 1, Type 2, and Type 3.  My patches are Type 1 only, although Type 3 devices should work.  Type 2 depends on 32-byte CDBs and is in progress.<br>Since the DIF tuple format is standardized, both initiators and targets (as well as potentially transport switches in-between) to verify the integrity of the data going over the bus.<br>When writing, the HBA will DMA 512-byte sectors from host memory, generate the matching integrity metadata and send out 520-byte sectors on the wire.  The disk will verify the integrity of the data before committing it to stable storage<br>When reading, the drive will send 520-byte sectors to the HBA.  The HBA will verify the data integrity and DMA 512-byte sectors to host memory.<br>IOW, DIF provides means for added integrity protection between HBA and disk</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.seagate.com/files/staticfiles/docs/pdf/whitepaper/safeguarding-data-from-corruption-technology-paper-tp621us.pdf">type 0</a><ul>
<li>Describes a drive that is not formatted with PI information bytes. This allows for legacy support in non-PI systems.</li>
</ul>
</li>
<li>type 1<ul>
<li>Provides support of PI protection using 10- and 16-byte commands. The RDPROTECT and WRTPROTECT bits allow for checking control through the CDB. Eight bytes of Protection Information are transmitted at sector boundaries across the interface if RDPROTECT and WRTPROTECT bits are non-zero values. Type I does not allow the use of 32-byte commands.</li>
</ul>
</li>
<li>type 2<ul>
<li>Provides checking control and additional expected fields within the 32-byte CDBs. Eight bytes of Protection Information are transmitted at sector boundaries across the interface if RDPROTECT and WRTPROTECT bits are non-zero values. Type II does allow the use of 10- and 16-byte commands with zero values in the RDPROTECT and WRTPROTECT fields. The drive will generate a dummy (for example, 0xFFFF) eight bytes of Protection Information in the media, but these eight bytes will not be transferred to the host during read.</li>
</ul>
</li>
<li>type 3<ul>
<li>seagate not support type 3</li>
</ul>
</li>
</ul>
<p>The T10-PI standard is an extension of the existing T10 SCSI Block Commands specification; covering communication between SCSI controllers and storage devices, Protection Information (PI) adds an extra eight bytes of information to the 512-byte sectors typical of enterprise hard drives. Increasing sector size to 520 bytes, these eight bytes of metadata consist of guard (GRD), application (APP) and reference (REF) tags that are used to verify the 512 bytes of data in the sector </p>
<p>512 bytes of data<br>GRD– 16-bit guard tag(crc of 512 byte data portion) 512-514 Bytes<br>APP– 16-bit application tag 514-516 Bytes (application custom)<br>REF– 32-bit reference tag 516-519 Bytes (For protect Misdirected Writes)<br>    Misdirected Write: the ture write LAB was not match with the write LBA target</p>
<p>DIF=512+8, DIF need apply a 520 bytes buffer<br>DIX=512B*8 (data buffer), 8x8Bytes(pi buffer), two parts</p>
<p>A device that supports protection information (i.e. supports one or more protection types of 1 or higher) sets the “PROTECT” bit in its standard INQUIRY response. It  also  sets the  SPT  field  in the EXTENDED INQUIRY VPD page response to indicate which protection types it supports. The current protection type of a disk can be found in the “P_TYPE” and “PROT_EN” fields in the response of a READ CAPACITY (16) command (e.g. with the ‘sg_readcap –long’ utility).</p>
<p>type 0: No checking but target device must generate on WRITE<br>type 1: GUARD + REF checking (LBA)<br>type 2: GUARD + REF checking (Extended Indirect LBA)<br>        READ(32)/WRITE(32) only<br>type 3: GUARD tag</p>
<p>Protection Information is intended as a standardized approach to system level LRC traditionally provided by systems using 520 byte formatted LBAs. Drives formatted with PI information provide the same, common LBA count (i.e. same capacity point) as non-PI formatted drives. Sequential performance of a PI drive will be reduced by approximately 1.56% due to the extra overhead of PI being transferred from the media that is not calculated as part of the data transferred to the host. To determine the full transfer rate of a PI drive, transfers should be calculated by adding the 8 extra bytes of PI to the transferred LBA length, i.e. 512 + 8 = 520. PI formatted drives are physically formatted to 520 byte sectors that store 512 bytes<br>of customer data with 8 bytes of Protection Information appended to it. The advantage of PI is that the Protection Information bits can be managed at the HBA and HBA driver level. Allowing a system that typically does not support 520 LBA formats to integrate this level of protection. Protection Information is valid with any supported LBA size. 512 LBA size is used here as common example.</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs angelscript"><span class="hljs-number">520</span>-Bytes Sector on Protection Information Disk Drive<br><span class="hljs-number">512</span>-Bytes  +  <span class="hljs-number">8</span> Bytes (metadata)<br><span class="hljs-number">512</span>-Bytes = User data<br><span class="hljs-number">8</span> Bytes = End-to-End Check = <span class="hljs-number">2</span> Bytes(logical block guard, CRC) + <span class="hljs-number">2</span> Bytes(logical block application guard) + <span class="hljs-number">4</span> Bytes(logical block <span class="hljs-built_in">ref</span>erence tag)<br><br>#smart  info <br>Vendor:               HGST<br>Product:              HUH721212AL5200<br>Revision:             NS05<br>Compliance:           SPC<span class="hljs-number">-4</span><br>User Capacity:        <span class="hljs-number">11</span>,<span class="hljs-number">756</span>,<span class="hljs-number">399</span>,<span class="hljs-number">230</span>,<span class="hljs-number">976</span> bytes [<span class="hljs-number">11.7</span> TB]<br>Logical block size:   <span class="hljs-number">512</span> bytes<br>Physical block size:  <span class="hljs-number">4096</span> bytes<br>Formatted with type <span class="hljs-number">2</span> protection<br><span class="hljs-number">8</span> bytes of protection information per logical block<br>LU <span class="hljs-keyword">is</span> fully provisioned<br>Rotation Rate:        <span class="hljs-number">7200</span> rpm<br><br>#type <span class="hljs-number">2</span> ,<span class="hljs-number">512</span> example<br>$ sg_readcap -l /dev/sdj<br>Read Capacity results:<br>   Protection: prot_en=<span class="hljs-number">1</span>, p_type=<span class="hljs-number">1</span>, p_i_exponent=<span class="hljs-number">0</span> [type <span class="hljs-number">2</span> protection]<br>   Logical block provisioning: lbpme=<span class="hljs-number">0</span>, lbprz=<span class="hljs-number">0</span><br>   Last logical block address=<span class="hljs-number">19134414847</span> (<span class="hljs-number">0x4747fffff</span>), Number of logical blocks=<span class="hljs-number">19134414848</span><br>   Logical block length=<span class="hljs-number">512</span> bytes<br>   Logical blocks per physical block exponent=<span class="hljs-number">3</span><br>   Lowest aligned logical block address=<span class="hljs-number">0</span><br>Hence:<br>   Device size: <span class="hljs-number">9796820402176</span> bytes, <span class="hljs-number">9342976.0</span> MiB, <span class="hljs-number">9796.82</span> GB<br><br><br>#type <span class="hljs-number">0</span>, <span class="hljs-number">4</span>K example<br>Read Capacity results:<br>   Protection: prot_en=<span class="hljs-number">0</span>, p_type=<span class="hljs-number">0</span>, p_i_exponent=<span class="hljs-number">0</span><br>   Logical block provisioning: lbpme=<span class="hljs-number">0</span>, lbprz=<span class="hljs-number">0</span><br>   Last LBA=<span class="hljs-number">2441609215</span> (<span class="hljs-number">0x9187ffff</span>), Number of logical blocks=<span class="hljs-number">2441609216</span><br>   Logical block length=<span class="hljs-number">4096</span> bytes<br>   Logical blocks per physical block exponent=<span class="hljs-number">0</span><br>   Lowest aligned LBA=<span class="hljs-number">0</span><br>Hence:<br>   Device size: <span class="hljs-number">10000831348736</span> bytes, <span class="hljs-number">9537536.0</span> MiB, <span class="hljs-number">10000.83</span> GB, <span class="hljs-number">10.00</span> TB<br><br>$ sg_readcap -<span class="hljs-number">-16</span> -b /dev/sdj<br><span class="hljs-number">0x474800000</span> <span class="hljs-number">0x200</span><br><br><span class="hljs-number">10</span>TB models with PI bytes<br>Sector   Dec           hex          <span class="hljs-number">10</span>TB models w/o PI bytes<br><span class="hljs-number">512</span> <span class="hljs-number">19</span>,<span class="hljs-number">134</span>,<span class="hljs-number">414</span>,<span class="hljs-number">848</span> <span class="hljs-number">474800000</span>        <span class="hljs-number">19</span>,<span class="hljs-number">532</span>,<span class="hljs-number">873</span>,<span class="hljs-number">728</span> <span class="hljs-number">48</span>C400000 (<span class="hljs-number">10000831348736</span> = <span class="hljs-number">19</span>,<span class="hljs-number">532</span>,<span class="hljs-number">873</span>,<span class="hljs-number">728</span> * <span class="hljs-number">512</span> = SATA <span class="hljs-number">10</span>TB raw size)<br><span class="hljs-number">520</span> <span class="hljs-number">18</span>,<span class="hljs-number">845</span>,<span class="hljs-number">007</span>,<span class="hljs-number">872</span> <span class="hljs-number">463400000</span>        <span class="hljs-number">19</span>,<span class="hljs-number">134</span>,<span class="hljs-number">414</span>,<span class="hljs-number">848</span> <span class="hljs-number">474800000</span><br><span class="hljs-number">524</span> <span class="hljs-number">18</span>,<span class="hljs-number">704</span>,<span class="hljs-number">498</span>,<span class="hljs-number">688</span> <span class="hljs-number">45</span>AE00000        <span class="hljs-number">18</span>,<span class="hljs-number">989</span>,<span class="hljs-number">711</span>,<span class="hljs-number">360</span> <span class="hljs-number">46</span>BE00000<br><span class="hljs-number">528</span> <span class="hljs-number">18</span>,<span class="hljs-number">563</span>,<span class="hljs-number">989</span>,<span class="hljs-number">504</span> <span class="hljs-number">452800000</span>        <span class="hljs-number">18</span>,<span class="hljs-number">845</span>,<span class="hljs-number">007</span>,<span class="hljs-number">872</span> <span class="hljs-number">463400000</span><br><span class="hljs-number">4096</span> <span class="hljs-number">2</span>,<span class="hljs-number">424</span>,<span class="hljs-number">569</span>,<span class="hljs-number">856</span> <span class="hljs-number">90840000</span>         <span class="hljs-number">2</span>,<span class="hljs-number">441</span>,<span class="hljs-number">609</span>,<span class="hljs-number">216</span> <span class="hljs-number">91880000</span> (<span class="hljs-number">10000831348736</span> = <span class="hljs-number">2</span>,<span class="hljs-number">441</span>,<span class="hljs-number">609</span>,<span class="hljs-number">216</span> * <span class="hljs-number">4096</span> = SATA <span class="hljs-number">10</span>TB raw size)<br><span class="hljs-number">4160</span> <span class="hljs-number">2</span>,<span class="hljs-number">387</span>,<span class="hljs-number">345</span>,<span class="hljs-number">408</span> <span class="hljs-number">8E4</span>C0000         <span class="hljs-number">2</span>,<span class="hljs-number">391</span>,<span class="hljs-number">801</span>,<span class="hljs-number">856</span> <span class="hljs-number">8E900000</span><br><span class="hljs-number">4192</span> <span class="hljs-number">2</span>,<span class="hljs-number">368</span>,<span class="hljs-number">995</span>,<span class="hljs-number">328</span> <span class="hljs-number">8</span>D340000         <span class="hljs-number">2</span>,<span class="hljs-number">373</span>,<span class="hljs-number">713</span>,<span class="hljs-number">920</span> <span class="hljs-number">8</span>D7C0000 <span class="hljs-number">1</span><br><span class="hljs-number">4224</span> <span class="hljs-number">2</span>,<span class="hljs-number">351</span>,<span class="hljs-number">169</span>,<span class="hljs-number">536</span> <span class="hljs-number">8</span>C240000         <span class="hljs-number">2</span>,<span class="hljs-number">355</span>,<span class="hljs-number">625</span>,<span class="hljs-number">984</span> <span class="hljs-number">8</span>C680000<br><br># models with PI bytes, the page only show <span class="hljs-number">512</span> <span class="hljs-keyword">or</span> <span class="hljs-number">4096</span> bytes, the extra space overhead hide <span class="hljs-keyword">in</span> HDD<br># modlls w/o PI bytes, smartctl could <span class="hljs-keyword">get</span> <span class="hljs-number">520</span>,<span class="hljs-number">524</span>,<span class="hljs-number">4160</span>,<span class="hljs-number">4192</span>,<span class="hljs-number">4224</span> <span class="hljs-keyword">from</span> logic sector bytes<br></code></pre></td></tr></table></figure>

<h5 id="T10-DIX-and-PI"><a href="#T10-DIX-and-PI" class="headerlink" title="T10 DIX and PI"></a><a target="_blank" rel="noopener" href="https://sp.ts.fujitsu.com/dmsp/Publications/public/dx_s3_Oracle_Linux_T10_PI_E16G_en.pdf">T10 DIX and PI</a></h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">|--------------Server----------------|                  |-------Storage system----|<br>Application -&gt; OS -&gt; IO controller/HBA -&gt; SAN/Switch -&gt; Controller/switch -&gt; Driver<br>|--------------DIX------------||----------------------PI--------------------------| <br></code></pre></td></tr></table></figure>
<p>DIX end at HBA input ( add extra 8 Bytes for protect in every level )<br>PI start at HBA output ( add extra 8 Bytes for protect in every level )<br>DIX + PI means end to end data protection<br>In some case, maybe you just support T10 PI and not DIX support</p>
<h3 id="Security-erasing"><a href="#Security-erasing" class="headerlink" title="Security erasing"></a>Security erasing</h3><p>SAS</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ sg_format --format /dev/sdx<br></code></pre></td></tr></table></figure>

<p>SATA</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hdparm --user-master u --security-set-pass password /dev/sdx<br></code></pre></td></tr></table></figure>

<p>NVME<br><a target="_blank" rel="noopener" href="https://nvmexpress.org/wp-content/uploads/NVM-Express-1_4-2019.06.10-Ratified.pdf">for more info</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ nvme list<br>Node             SN                   Model                                    Namespace Usage                      Format           FW Rev<br>---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------<br>/dev/nvme0n1     xxxxxxxxxxxxxxx      INTEL SSDPED1K375GA                      1         375.08  GB / 375.08  GB    512   B +  0 B   E2010324<br><br>$ nvme id-ctrl -H  /dev/nvme0n1 | grep format -i<br>  [1:1] : 0x1   Format NVM Supported<br>  [0:0] : 0     Admin Vendor Specific Commands uses Vendor Specific Format<br>  [0:0] : 0     Format Applies to Single Namespace(s)<br>  [0:0] : 0     NVM Vendor Specific Commands uses Vendor Specific Format<br><br>$ --lbaf<br>LBA Format: This field specifies the LBA format to apply to the NVM media. This corresponds to the LBA formats indicated <span class="hljs-keyword">in</span> the Identify Namespace <span class="hljs-built_in">command</span>. Defaults to 0.<br><br>$ nvme format /dev/nvme0n1 --ses=1 -pi 1<br><span class="hljs-comment"># ses</span><br><span class="hljs-comment">#Format namespace 1 with user data secure erase settings and protection information</span><br><br><span class="hljs-comment">#0 (default) means No secure erase operation requested</span><br><span class="hljs-comment">#1 User Data Erase: All user data shall be erased, contents of the user data after the erase is indeterminate (e.g., the user data may be zero filled, one filled, etc). The controller may perform a cryptographic erase when a User Data Erase is requested if all user data is encrypted. </span><br><span class="hljs-comment">#2 Cryptographic Erase: All user data shall be erased cryptographically. This is accomplished by deleting the encryption key.</span><br><span class="hljs-comment">#3-7 Reserved</span><br><br><span class="hljs-comment">#pi </span><br><span class="hljs-comment"># 0 default, not enabled</span><br><span class="hljs-comment"># 1,type 1</span><br><span class="hljs-comment"># 2,type 2 </span><br><span class="hljs-comment"># 3,type 3</span><br><span class="hljs-comment"># 4-7 Reserved</span><br><br>$ nvme id-ns /dev/nvme1n1 | grep LBA<br>LBA Format  0 : Metadata Size: 0   bytes - Data Size: 512 bytes - Relative Performance: 0x1 Better<br>LBA Format  1 : Metadata Size: 8   bytes - Data Size: 512 bytes - Relative Performance: 0x3 Degraded<br>LBA Format  2 : Metadata Size: 0   bytes - Data Size: 4096 bytes - Relative Performance: 0 Best  (<span class="hljs-keyword">in</span> use)<br>LBA Format  3 : Metadata Size: 8   bytes - Data Size: 4096 bytes - Relative Performance: 0x2 Good<br><br>$ nvme format /dev/nvme1n1 -l 2<br></code></pre></td></tr></table></figure>

<h3 id="Enable-the-blk-mq-and-scsi-mq"><a href="#Enable-the-blk-mq-and-scsi-mq" class="headerlink" title="Enable the blk_mq and scsi_mq"></a>Enable the blk_mq and scsi_mq</h3><p>scsi-mq:specify scsi_mod.use_blk_mq=y<br>blk-mq infrastructure if the dm_mod.use_blk_mq=y</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ grubby --update-kernel=ALL --args=<span class="hljs-string">&#x27;scsi_mod.use_blk_mq=y dm_mod.use_blk_mq=y&#x27;</span><br>$ reboot<br>$ cat  /sys/block/dm-X/dm/use_blk_mq<br>$ tree /sys/class/block/sdl/mq /sys/class/block/dm-0/mq<br></code></pre></td></tr></table></figure>

<h3 id="lsscsi"><a href="#lsscsi" class="headerlink" title="lsscsi"></a>lsscsi</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ lsscsi -t | grep sdio<br>[4:1:8:0]   disk    sas:0x5000c657afc33e1d          /dev/sdio<br><br>$ lsscsi -t -L 4:1:8:0<br>[4:1:8:0]   disk    sas:0x5000c657afc33e1d          /dev/sdio<br>  transport=sas<br>  vendor=HGST<br>  model=HUH721010AL5200<br>  bay_identifier=7<br>  enclosure_device:Slot07<br>  enclosure_identifier=0x50030103403449ca<br>  initiator_port_protocols=none<br>  initiator_response_timeout=1744<br>  I_T_nexus_loss_timeout=1744<br>  phy_identifier=33<br>  ready_led_meaning=0<br>  sas_address=0x5000c657afc33e1d<br>  target_port_protocols=ssp<br>  tlr_enabled=0<br>  tlr_supported=0<br><br>$ lsscsi -gis | grep sdio<br>[4:1:8:0]   disk    HGST     HUH721010AL5200  A21D  /dev/sdio  35000c657afc33e1c  /dev/sg180  10.0TB<br><br>$ lsblk -o NAME,MODEL,SERIAL,SIZE,STATE,TYPE,WWN --nodeps | grep sdio<br>sdio HUH721010AL5200  5000c657afc33e1c                   9.1T running disk 0x5000c657afc33e1c<br><br>$ $ lsscsi -t -H<br>[0]    megaraid_sas<br>[10]    ahci          sata:<br>[11]    ahci          sata:<br>[12]    mpt3sas       sas:0x500605b00bfa0982<br>[13]    mpt3sas       sas:0x500605b00acf0542<br>[14]    mpt3sas       sas:0x500605b00bfac033<br><br>$ lsscsi -tiv<br>[0:0:7:0]    disk    sas:0x500056b3787358c7          /dev/sdh   -<br>  dir: /sys/bus/scsi/devices/0:0:7:0  [/sys/devices/pci0000:ae/0000:ae:00.0/0000:af:00.0/host0/port-0:0/expander-0:0/port-0:0:7/end_device-0:0:7/target0:0:7/0:0:7:0]<br>[0:0:8:0]    disk    sas:0x500056b3787358c8          /dev/sdi   -<br>  dir: /sys/bus/scsi/devices/0:0:8:0  [/sys/devices/pci0000:ae/0000:ae:00.0/0000:af:00.0/host0/port-0:0/expander-0:0/port-0:0:8/end_device-0:0:8/target0:0:8/0:0:8:0]<br>[0:0:9:0]    disk    sas:0x5000cca26c1b8631          /dev/sdj   35000cca26c1b8630<br><br><span class="hljs-comment"># get single port</span><br>[1:0:0:0]    disk    sas:0x5000cca26fae54a6          /dev/sda   35000cca26fae54a4  11.7TB<br>  dir: /sys/bus/scsi/devices/1:0:0:0  [/sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0/host1/port-1:0/expander-1:0/port-1:0:0/expander-1:1/port-1:1:0/end_device-1:1:0/target1:0:0/1:0:0:0]<br>[1:0:1:0]    enclosu sas:0x500c0ff00b40aa3e          -          -       -<br>  dir: /sys/bus/scsi/devices/1:0:1:0  [/sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0/host1/port-1:0/expander-1:0/port-1:0:4/end_device-1:0:4/target1:0:1/1:0:1:0]<br>[1:0:2:0]    disk    sas:0x5000cca26fabd8c2          /dev/sdb   35000cca26fabd8c0  11.7TB<br>  dir: /sys/bus/scsi/devices/1:0:2:0  [/sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0/host1/port-1:0/expander-1:0/port-1:0:0/expander-1:1/port-1:1:1/end_device-1:1:1/target1:0:2/1:0:2:0]<br>.......<br><br>[1:0:87:0]   disk    sas:0x5000cca26fbac14e          /dev/sdcf  35000cca26fbac14c  11.7TB<br>  dir: /sys/bus/scsi/devices/1:0:87:0  [/sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0/host1/port-1:0/expander-1:0/port-1:0:3/expander-1:4/port-1:4:27/end_device-1:4:27/target1:0:87/1:0:87:0]<br>[1:0:88:0]   process sas:0x500c0ff5f12425fe          -          -       -<br>  dir: /sys/bus/scsi/devices/1:0:88:0  [/sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0/host1/port-1:0/expander-1:0/port-1:0:3/expander-1:4/port-1:4:28/end_device-1:4:28/target1:0:88/1:0:88:0]<br><br>$ grep 11.7 /tmp/tmp<br>[1:0:0:0]    disk    sas:0x5000cca26fae54a6          /dev/sda   35000cca26fae54a4  11.7TB<br>[1:0:2:0]    disk    sas:0x5000cca26fabd8c2          /dev/sdb   35000cca26fabd8c0  11.7TB<br>[1:0:3:0]    disk    sas:0x5000cca26fb405d2          /dev/sdc   35000cca26fb405d0  11.7TB<br>[1:0:4:0]    disk    sas:0x5000cca26fb99436          /dev/sdd   35000cca26fb99434  11.7TB<br>[1:0:5:0]    disk    sas:0x5000cca26fbac11a          /dev/sde   35000cca26fbac118  11.7TB<br>[1:0:6:0]    disk    sas:0x5000cca26fbac962          /dev/sdf   35000cca26fbac960  11.7TB<br>[1:0:7:0]    disk    sas:0x5000cca26fb5c9b2          /dev/sdg   35000cca26fb5c9b0  11.7TB<br>[1:0:8:0]    disk    sas:0x5000cca26fad67be          /dev/sdh   35000cca26fad67bc  11.7TB<br>[1:0:9:0]    disk    sas:0x5000cca26fb7d912          /dev/sdi   35000cca26fb7d910  11.7TB<br>[1:0:10:0]   disk    sas:0x5000cca26fbacac2          /dev/sdj   35000cca26fbacac0  11.7TB<br>[1:0:11:0]   disk    sas:0x5000cca26fbac81a          /dev/sdk   35000cca26fbac818  11.7TB<br>.......<br>[1:0:78:0]   disk    sas:0x5000cca26fa140aa          /dev/sdbw  35000cca26fa140a8  11.7TB<br>[1:0:79:0]   disk    sas:0x5000cca26fbac816          /dev/sdbx  35000cca26fbac814  11.7TB<br>[1:0:80:0]   disk    sas:0x5000cca26fb99fba          /dev/sdby  35000cca26fb99fb8  11.7TB<br>[1:0:81:0]   disk    sas:0x5000cca26faa42da          /dev/sdbz  35000cca26faa42d8  11.7TB<br>[1:0:82:0]   disk    sas:0x5000cca26fad6776          /dev/sdca  35000cca26fad6774  11.7TB<br>[1:0:83:0]   disk    sas:0x5000cca26fa46c8e          /dev/sdcb  35000cca26fa46c8c  11.7TB<br>[1:0:84:0]   disk    sas:0x5000cca26fbab7a6          /dev/sdcc  35000cca26fbab7a4  11.7TB<br>[1:0:85:0]   disk    sas:0x5000cca26fbab7e2          /dev/sdcd  35000cca26fbab7e0  11.7TB<br>[1:0:86:0]   disk    sas:0x5000cca26fb5ca42          /dev/sdce  35000cca26fb5ca40  11.7TB<br>[1:0:87:0]   disk    sas:0x5000cca26fbac14e          /dev/sdcf  35000cca26fbac14c  11.7TB<br><br></code></pre></td></tr></table></figure>

<h3 id="udevinfo"><a href="#udevinfo" class="headerlink" title="udevinfo"></a><a target="_blank" rel="noopener" href="https://sites.google.com/site/itmyshare/system-admin-tips-and-tools/udevadm---useage-examples">udevinfo</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ udevadm info --query=property --name /dev/sdd<br>$ udevadm info --query=path --name /dev/sdd<br>$ udevadm info --query=symlink --name /dev/sdd<br>$ udevadm info --a --name=/dev/sdd<br>$ udevadm info --attribute-walk --name /dev/sdd<br><br>$ udevadm info -a -p $(udevadm info -q path -n /dev/sda)<br>$ udevadm info -a -p /sys/class/net/eth0<br>$ udevadm info -q path -n /dev/sda1<br><br><span class="hljs-comment"># trigger, remember to have --dry-run option if you run it on production node.</span><br>$ udevadm trigger --verbose --dry-run --<span class="hljs-built_in">type</span>=devices --subsystem-match=scsi_host<br>$ udevadm trigger --verbose --dry-run --<span class="hljs-built_in">type</span>=devices --subsystem-match=scsi_disk<br>$ udevadm trigger --verbose --dry-run --<span class="hljs-built_in">type</span>=devices --subsystem-match=scsi_tape<br>$ udevadm trigger --verbose --dry-run --<span class="hljs-built_in">type</span>=devices --subsystem-match=fc_host <br><br><span class="hljs-comment"># This option only waits for events triggered by the same command to finish</span><br>$ udevadm settle<br><br><span class="hljs-comment"># control </span><br>$ udevadm control --reload-rules<br>$ --log-priority=&lt;level&gt;   <span class="hljs-built_in">set</span> the udev <span class="hljs-built_in">log</span> level <span class="hljs-keyword">for</span> the daemon<br><br><span class="hljs-comment"># monitor</span><br>$ udevadm monitor --<span class="hljs-built_in">help</span><br>Usage: udevadm monitor [--property] [--kernel] [--udev] [--<span class="hljs-built_in">help</span>]<br>  --property                    <span class="hljs-built_in">print</span> the event properties<br>  --kernel                      <span class="hljs-built_in">print</span> kernel uevents<br>  --udev                        <span class="hljs-built_in">print</span> udev events<br>  --subsystem-match=&lt;subsystem&gt; filter events<br>  --<span class="hljs-built_in">help</span><br><br><span class="hljs-comment"># test</span><br>$ udevadm <span class="hljs-built_in">test</span> /block/sdd<br><br><span class="hljs-comment">### show block info</span><br>$ hwinfo --block<br>$ udevadm info -a -n /dev/sda<br>$ lshw -class storage<br>$ lsblk -o NAME,MODEL,SERIAL,SIZE,STATE,TYPE,WWN --nodeps<br></code></pre></td></tr></table></figure>


<h3 id="Flush-cache-data"><a href="#Flush-cache-data" class="headerlink" title="Flush cache data"></a>Flush cache data</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">## flush fs buff</span><br>$ sync<br><br><span class="hljs-comment"># Flush fs buff</span><br>$ blockdev --flushbufs /dev/sdxx<br><br><span class="hljs-comment">#or for zfs</span><br>$ zpool <span class="hljs-built_in">export</span> your_pool<br><br><span class="hljs-comment"># Flush the SAS dev buff</span><br>$ sdparm --<span class="hljs-built_in">command</span>=sync /dev/sdxx<br><span class="hljs-comment">#spindown the device</span><br>$ sdparm --<span class="hljs-built_in">command</span>=stop /dev/sdxx<br><br><span class="hljs-comment"># Flush the SATA dev buff</span><br>$ hdparm -F /dev/sdxx<br><br>$ storcli64 /c0 <span class="hljs-built_in">set</span> flushwriteverify=on<br>$ storcli64 /c0 show flushwriteverify<br>$ storcli64 /c0 flushcache<br>Description = Adapter and/or disk caches flushed successfully.<br></code></pre></td></tr></table></figure>

<h3 id="dev-info"><a href="#dev-info" class="headerlink" title="dev info"></a><a target="_blank" rel="noopener" href="https://www.ibm.com/support/knowledgecenter/en/linuxonibm/com.ibm.linux.z.lgdd/lgdd_t_fcp_wrk_actinfo.html">dev info</a></h3><p>/sys/class/scsi_device/<device_name>/device/<attribute></p>
<p>ioerr_cnt       The number of SCSI commands that completed with an error.<br>scsi_level      The SCSI revision level, received from inquiry data.</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs routeros">access_denied   Flag that indicates whether access <span class="hljs-keyword">to</span> the device is restricted by the FCP channel.The value is 1 <span class="hljs-keyword">if</span> access is denied <span class="hljs-keyword">and</span> 0 <span class="hljs-keyword">if</span> access is permitted.<span class="hljs-keyword">If</span> access is denied <span class="hljs-keyword">to</span> your Linux instance, confirm that your SCSI devices are configured as intended. Also, be sure that you really want <span class="hljs-keyword">to</span> share a SCSI device. <span class="hljs-keyword">For</span> shared access <span class="hljs-keyword">to</span> a SCSI device, preferably use NPIV). You might also use different FCP channels <span class="hljs-keyword">or</span> target ports.<br>access_shared   This attribute is obsolete. The value is always 0.<br>access_readonly This attribute is obsolete. The value is always 0.<br>ked     Flag that indicates whether the device is <span class="hljs-keyword">in</span> blocked state (0 <span class="hljs-keyword">or</span> 1).<br>iocounterbits   The number of bits used <span class="hljs-keyword">for</span> I/O counters.<br>iodone_cnt      The number of completed <span class="hljs-keyword">or</span> rejected SCSI commands.<br>ioerr_cnt       The number of SCSI commands that completed with an error.<br>iorequest_cnt   The number of issued SCSI commands.<br>queue_type      The<span class="hljs-built_in"> type </span>of<span class="hljs-built_in"> queue </span><span class="hljs-keyword">for</span> the SCSI device. The value can be one of the following:<br>                none<br>                simple<br>                ordered<br>model   The model of the SCSI device, received <span class="hljs-keyword">from</span> inquiry data.<br>rev     The revision of the SCSI device, received <span class="hljs-keyword">from</span> inquiry data.<br>scsi_level      The SCSI revision level, received <span class="hljs-keyword">from</span> inquiry data.<br>type    The<span class="hljs-built_in"> type </span>of the SCSI device, received <span class="hljs-keyword">from</span> inquiry data.<br>vendor  The vendor of the SCSI device, received <span class="hljs-keyword">from</span> inquiry data.<br>fcp_lun The LUN of the SCSI device <span class="hljs-keyword">in</span> 64-bit format.<br>hba_id  The bus ID of the SCSI device.<br>wwpn    The WWPN of the remote port.<br>zfcp_access_denied      Flag that indicates whether access <span class="hljs-keyword">to</span> the device is restricted by the FCP channel.The value is 1 <span class="hljs-keyword">if</span> access is denied <span class="hljs-keyword">and</span> 0 <span class="hljs-keyword">if</span> access is permitted. <span class="hljs-keyword">If</span> access is denied <span class="hljs-keyword">to</span> your Linux instance, confirm that your SCSI devices are configured as intended. Also, be sure that you really want <span class="hljs-keyword">to</span> share a SCSI device. <span class="hljs-keyword">For</span> shared access <span class="hljs-keyword">to</span> a SCSI device, preferably use NPIV). You might also use different FCP channels <span class="hljs-keyword">or</span> target ports.<br>zfcp_in_recovery        Shows <span class="hljs-keyword">if</span> unit is <span class="hljs-keyword">in</span> recovery (0 <span class="hljs-keyword">or</span> 1).Shows <span class="hljs-keyword">if</span> unit is <span class="hljs-keyword">in</span> recovery (2 <span class="hljs-keyword">or</span> 1)<br><br><br><span class="hljs-comment">### Encrypt block device</span><br><span class="hljs-comment">#### *Input password*     </span><br><span class="hljs-keyword">or</span> you can <span class="hljs-keyword">not</span> use openssl          <br>``` bash<br>cryptsetup luksFormat --cipher aes-xts-plain64 --key-size 512 --hash sha256 /dev/vdb     <br>WARNING!     <br>========     <br>This will overwrite data on /dev/vdb irrevocably.     <br>     <br>Are you sure? (Type uppercase <span class="hljs-literal">yes</span>): <span class="hljs-literal">YES</span>      <br>Enter passphrase:      <br>Verify passphrase:      <br></code></pre></td></tr></table></figure>

<h4 id="mapper-test-device"><a href="#mapper-test-device" class="headerlink" title="mapper test device"></a><em>mapper test device</em></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs bash">cryptsetup luksOpen /dev/vdb <span class="hljs-built_in">test</span>     <br>Enter passphrase <span class="hljs-keyword">for</span> /dev/vdb:     <br>``` <br>or     <br>``` bash<br><span class="hljs-built_in">cd</span> /dev/shm     <br>openssl rand 128 -hex -out ./key     <br>openssl aes-256-cbc -<span class="hljs-keyword">in</span> ./key -out key.enc     <br><br>Verifying - enter aes-256-cbc encryption password:<br>openssl aes-256-cbc -d -<span class="hljs-keyword">in</span> ./key.enc | cryptsetup [--allow-discards] --key-file=- luksOpen /dev/vdb <span class="hljs-built_in">test</span>     <br>``` <br><br>If you have a SSD, you may want to use --allow-discards. It creates an information leak but it enhances your SSD’s lifetime.     <br><br>``` bash<br>mkfs.ext4 /dev/mapper/<span class="hljs-built_in">test</span>     <br>cryptsetup luksDump /dev/vdb     <br>     <br>``` bash     <br>LUKS header information <span class="hljs-keyword">for</span> /dev/vdb     <br>Key Slot 0: ENABLED     <br>	Iterations:         	80604     <br>	Salt:               	c4 c0 08 9c 77 ef 57 a5 d2 62 f4 94 03 56 24 7a      <br>	                      	86 0c f4 c6 06 6f 9e 01 80 <span class="hljs-built_in">fc</span> 0f 1d 3b a9 59 87      <br>	Key material offset:	8     <br>	AF stripes:            	4000     <br>Key Slot 1: DISABLED     <br></code></pre></td></tr></table></figure>

<h4 id="Add-remove-key"><a href="#Add-remove-key" class="headerlink" title="Add/remove key"></a><em>Add/remove key</em></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">cryptsetup luksAddKey --key-slot 1 /dev/vdb     <br>cryptsetup luksRemoveKey /dev/vdb     <br></code></pre></td></tr></table></figure>

<h4 id="Back-restore-header"><a href="#Back-restore-header" class="headerlink" title="Back/restore header"></a><em>Back/restore header</em></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">cryptsetup luksHeaderBackup /dev/vdb --header-backup-file /root/vdb-header-backup     <br>cryptsetup luksHeaderRestore /dev/vdb --header-backup-file /root/vdb-header-backup     <br></code></pre></td></tr></table></figure>

<h4 id="is-luks-partition"><a href="#is-luks-partition" class="headerlink" title="is luks partition"></a><em>is luks partition</em></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">cryptsetup -v isLuks /dev/vdb     <br></code></pre></td></tr></table></figure>

<h4 id="Clean-luks-partition"><a href="#Clean-luks-partition" class="headerlink" title="Clean luks partition"></a><em>Clean luks partition</em></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">head -c 3145728 /dev/zero &gt; /dev/vdb;sync     <br></code></pre></td></tr></table></figure>
<p>The default LUKS header (with only one key-slot enabled) takes 1052672 bytes, what is slightly more than 1 MiB. Having 2 key-slots enabled this would extend the header almost twice (key-slots * stripes * keysize + offset bytes). Therefore overwriting the first 3 MiB would do the job for us </p>
<h3 id="Why-Trim"><a href="#Why-Trim" class="headerlink" title="Why Trim"></a><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Trim_(computing)">Why Trim</a></h3><p>The Trim command is designed to enable the operating system to notify the SSD which pages no longer contain valid data due to erases either by the user or operating system itself. During a delete operation, the OS will mark the sectors as free for new data and send a Trim command to the SSD to mark them as not containing valid data. After that the SSD knows not to preserve the contents of the block when writing a page, resulting in less write amplification with fewer writes to the flash, higher write speed, and increased drive life.<br>Different SSDs implement the Trim command somewhat differently, so performance can vary.<br><img src="http://images.anandtech.com/reviews/storage/Intel/34nmSSD/Review/garbagecollection.png"></p>
<h4 id="Openzfs-on-linux"><a href="#Openzfs-on-linux" class="headerlink" title="Openzfs on linux"></a>Openzfs on linux</h4><p><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/pull/1016">not support trim funtion</a><br><a target="_blank" rel="noopener" href="http://list.zfsonlinux.org/pipermail/zfs-discuss/2016-January/024437.html">expect to get a upgrade this year that will add TRIM support</a></p>
<h4 id="Trim-patch-for-ZOL"><a href="#Trim-patch-for-ZOL" class="headerlink" title="Trim patch for ZOL"></a>Trim patch for ZOL</h4><p><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/pull/924">DO NOT APPLY THIS PATCH IF YOU CARE ABOUT YOUR DATA</a></p>
<h4 id="Mdadm"><a href="#Mdadm" class="headerlink" title="Mdadm"></a>Mdadm</h4><p><a target="_blank" rel="noopener" href="http://kernelnewbies.org/Linux_3.7#head-2fd9b183a4623d96e69ed24f88e0eb83217fa8df">Since version 3.7 of the Linux kernel mainline, md supports TRIM operations for the underlying solid-state drives (SSDs), for linear, RAID 0, RAID 1, RAID 5 and RAID 10 layouts</a><br><a target="_blank" rel="noopener" href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html-single/6.5_Release_Notes/">CentOS 6.5 The mdadm tool now supports the TRIM commands for RAID0, RAID1, and RAID10.</a></p>
<h4 id="LSI-Check-Interoperability-and-Compatibility"><a href="#LSI-Check-Interoperability-and-Compatibility" class="headerlink" title="LSI Check Interoperability and Compatibility"></a><a target="_blank" rel="noopener" href="http://www.avagotech.com/support/interop-compatibility">LSI Check Interoperability and Compatibility</a></h4><p><a target="_blank" rel="noopener" href="http://docs.avagotech.com/docs-and-downloads/raid-controllers/MegaRAID_SAS_Gen3CompatibilityList.pdf">MegaRAID SAS Gen3 Compatibility List</a><br><a target="_blank" rel="noopener" href="http://docs.avagotech.com/docs-and-downloads/raid-controllers/raid-controllers-common-files/iMR_SAS_Gen3CompatibilityList.pdf">iMR SAS Gen3 Compatibility List</a><br><a target="_blank" rel="noopener" href="http://www.media-clone.net/v/vspfiles/downloads/LSI_6Gb_SAS_SATA_HBA_Compatibility_List.pdf">LSI_6Gb_SAS_SATA_HBA Compatibility List</a><br>Just Compatibility, trim support not clean</p>
<h4 id="HP-Raid-support"><a href="#HP-Raid-support" class="headerlink" title="HP Raid support"></a>HP Raid support</h4><p><a target="_blank" rel="noopener" href="http://h20195.www2.hp.com/V2/GetPDF.aspx/4AA5-8637ENW">Is TRIM supported when using RAID with SSDs?</a><br><img src="/img/hp-trim-support.png"><br>Raid 1,10,0 could be support<br>Raid 1E,5,6 has not support</p>
<h3 id="Re-assign-bad-block"><a href="#Re-assign-bad-block" class="headerlink" title="Re-assign bad block"></a><a target="_blank" rel="noopener" href="https://www.smartmontools.org/wiki/BadBlockHowto">Re-assign bad block</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># dmesg error</span><br> sd 13:0:10:0: attempting task abort! scmd(ffff88bdf0eb8000)<br> sd 13:0:10:0: [sdff] tag<span class="hljs-comment">#22 CDB: Read(16) 88 00 00 00 00 04 16 2b fa 66 00 00 00 6c 00 00</span><br> scsi target13:0:10: _scsih_tm_display_info: handle(0x0015), sas_address(0x5000cca251b4462e), phy(37)<br> scsi target13:0:10: enclosurelogical id(0x500304800928dc3f), slot(10)<br> scsi target13:0:10: enclosure level(0x0000), connector name(     )<br> sd 13:0:10:0: task abort: SUCCESS scmd(ffff88bdf0eb8000)<br> sd 13:0:10:0: [sdff] tag<span class="hljs-comment">#22 FAILED Result: hostbyte=DID_TIME_OUT driverbyte=DRIVER_OK</span><br> sd 13:0:10:0: [sdff] tag<span class="hljs-comment">#22 CDB: Read(16) 88 00 00 00 00 04 16 2b fa 66 00 00 00 6c 00 00</span><br> blk_update_request: I/O error, dev sdff, sector 17551850086<br> sd 13:0:10:0: attempting task abort! scmd(ffff88b39ff33800)<br> sd 13:0:10:0: [sdff] tag<span class="hljs-comment">#21 CDB: Read(16) 88 00 00 00 00 04 16 2b f9 78 00 00 00 ee 00 00</span><br> scsi target13:0:10: _scsih_tm_display_info: handle(0x0015), sas_address(0x5000cca251b4462e), phy(37)<br> scsi target13:0:10: enclosurelogical id(0x500304800928dc3f), slot(10)<br> scsi target13:0:10: enclosure level(0x0000), connector name(     )<br> sd 13:0:10:0: task abort: SUCCESS scmd(ffff88b39ff33800)<br><br>$ smartctl -t long /dev/sdff<br><br><span class="hljs-comment">### after test</span><br>smartctl -l selftest /dev/sdb<br>smartctl version 5.37 [i686-pc-linux-gnu] Copyright (C) 2002-6 Bruce Allen<br>Home page is http://smartmontools.sourceforge.net/<br>SMART Self-test <span class="hljs-built_in">log</span><br>Num  Test              Status            segment  LifeTime  LBA_first_err [SK ASC ASQ]<br>     Description                         number   (hours)<br><span class="hljs-comment"># 1  Background long   Failed in segment      -     354           1193046 [0x3 0x11 0x0]</span><br><span class="hljs-comment"># 2  Background short  Completed              -     323                 - [-   -    -]</span><br><span class="hljs-comment"># 3  Background short  Completed              -     194                 - [-   -    -]</span><br><br><span class="hljs-comment">#That means sometimes the connection quality was bad</span><br>$ sg_verify --lba=1193046 /dev/sdff<br>$ <span class="hljs-built_in">echo</span> $?<br>0<br><br>0x123456=1193046<br><br><span class="hljs-comment">#The link case</span><br>$ sg_verify --lba=1193046 /dev/sdb<br>verify (10):  Fixed format, current;  Sense key: Medium Error<br> Additional sense: Unrecovered <span class="hljs-built_in">read</span> error<br>  Info fld=0x123456 [1193046]<br>  Field replaceable unit code: 228<br>  Actual retry count: 0x008b<br>medium or hardware error, reported lba=0x123456<br><br><span class="hljs-comment"># Now the GLIST length is checked before the block reassignment:</span><br><br>$ sg_reassign --grown /dev/sdb<br>&gt;&gt; Elements <span class="hljs-keyword">in</span> grown defect list: 0<br><span class="hljs-comment"># And now for the actual reassignment followed by another check of the GLIST length:</span><br><br>$ sg_reassign --address=1193046 /dev/sdb<br><br>$ sg_reassign --grown /dev/sdb<br>&gt;&gt; Elements <span class="hljs-keyword">in</span> grown defect list: 1<br><span class="hljs-comment"># The GLIST length has grown by one as expected. If the disk was unable to recover any data, then the &quot;new&quot; block at lba 0x123456 has vendor specific data in it. The sg_reassign utility can also do bulk reassigns, see man sg_reassign for more information.</span><br><br><span class="hljs-comment"># The dd command could be used to read the contents of the &quot;new&quot; block:</span><br><br>$ dd <span class="hljs-keyword">if</span>=/dev/sdb iflag=direct skip=1193046 of=blk.img bs=512 count=1<br><span class="hljs-comment"># and a hex editor [11] used to view and potentially change the blk.img file. An altered blk.img file (or /dev/zero) could be written back with:</span><br><br>$ dd <span class="hljs-keyword">if</span>=blk.img of=/dev/sdb seek=1193046 oflag=direct bs=512 count=1<br><span class="hljs-comment"># More work may be needed at the file system level, especially if the reassigned block held critical file system information such as a superblock or a directory.</span><br></code></pre></td></tr></table></figure>

<h3 id="linux-block-integrity"><a href="#linux-block-integrity" class="headerlink" title="linux block integrity"></a>linux block integrity</h3><p>config BLK_DEV_INTEGRITY</p>
<p><a target="_blank" rel="noopener" href="https://www.redhat.com/zh/blog/what-bit-rot-and-how-can-i-detect-it-rhel">Regarding support status: dm-integrity is not labeled as TechPreview, it is supported. Heavy stacks of RAID1 on top, with mdadm or LVM-raid, are not yet widely tested or recommended for production</a></p>
<p>Issues, </p>
<ul>
<li>reformat, it ‘s a long time for 10TB+ HDD</li>
<li>performance impact</li>
</ul>
<p>dm-integrity</p>
<ul>
<li>Emulates per-sector metadata</li>
<li>Optionally standalone mode (CRC32)</li>
</ul>
<p>Because the T13/ATA External Path Protection has <a target="_blank" rel="noopener" href="https://www.spinics.net/lists/raid/msg41308.html">dead</a><br>I can ‘t see any support from raid/HBA vendor<br>I found linux community support the block integrity, linux kernel need to &gt;= 4.12</p>
<h4 id="test"><a href="#test" class="headerlink" title="test"></a>test</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ dd <span class="hljs-keyword">if</span>=/dev/zero of=rawfile bs=1M count=128<br>$ MYLOOP=$(losetup -f --show rawfile)<br>$ integritysetup format <span class="hljs-variable">$MYLOOP</span> -I crc32<br>$ integritysetup open <span class="hljs-variable">$MYLOOP</span> mydata -I crc32<br>$ mkfs.xfs /dev/mapper/mydata<br>$ mount /dev/mapper/mydata /mnt<br>$ yes &gt;/mnt/infile<br>$ md5sum /mnt/infile <br>13e14c50aaf2054d987663ed31b5f786  /mnt/infile<br>$ umount /mnt/<br>$ losetup -d <span class="hljs-variable">$MYLOOP</span><br>$ integritysetup close mydata<br>$ dd <span class="hljs-keyword">if</span>=rawfile bs=1 count=10 skip=$((<span class="hljs-number">50</span>*<span class="hljs-number">1024</span>*<span class="hljs-number">1024</span>)) 2&gt;/dev/null|hexdump -vC<br>00000000  79 0a 79 0a 79 0a 79 0a  79 0a |y.y.y.y.y.|<br>0000000a<br>$ <span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;x&#x27;</span> | dd of=rawfile bs=1 count=1 seek=$((<span class="hljs-number">50</span>*<span class="hljs-number">1024</span>*<span class="hljs-number">1024</span>)) conv=notrunc<br>$ dd <span class="hljs-keyword">if</span>=rawfile bs=1 count=10 skip=$((<span class="hljs-number">50</span>*<span class="hljs-number">1024</span>*<span class="hljs-number">1024</span>)) 2&gt;/dev/null|hexdump -vC<br>00000000  78 0a 79 0a 79 0a 79 0a  79 0a |x.y.y.y.y.|<br>0000000a<br><br><span class="hljs-comment"># you can see the log in demsg</span><br>[Thu Jan 23 12:56:07 2020] device-mapper: integrity: Checksum failed at sector 0x18468<br><br>$ integritysetup status mydata<br>/dev/mapper/mydata is active and is <span class="hljs-keyword">in</span> use.<br>  <span class="hljs-built_in">type</span>:    INTEGRITY<br>  tag size: 4<br>  integrity: crc32c<br>  device:  /dev/loop0<br>  loop:    /root/rawfile<br>  sector size:  512 bytes<br>  interleave sectors: 32768<br>  size:    258152 sectors<br>  mode:    <span class="hljs-built_in">read</span>/write<br>  failures: 10<br>  journal size: 991232 bytes<br>  journal watermark: 50%<br>  journal commit time: 10000 ms<br><br>$  md5sum /mnt/*<br>md5sum: /mnt/infile: Input/output error <span class="hljs-comment">## error file could not be read</span><br>e7eee6d6c29d2f78bbd28e14df457dc7  /mnt/testfile<br></code></pre></td></tr></table></figure>

<p>Tech preview in CentOS 8<br>Regarding support status: dm-integrity is not labeled as TechPreview, it is supported. Heavy stacks of RAID1 on top, with mdadm or LVM-raid, are not yet widely tested or recommended for production.</p>
<p><a target="_blank" rel="noopener" href="https://gitlab.com/cryptsetup/cryptsetup/-/wikis/DMIntegrity">For more DMIntegrity</a></p>
<h3 id="Seagate-HDD-smart-issue"><a href="#Seagate-HDD-smart-issue" class="headerlink" title="Seagate HDD smart issue"></a>Seagate HDD smart issue</h3><h4 id="Seagate-SATA"><a href="#Seagate-SATA" class="headerlink" title="Seagate SATA"></a>Seagate SATA</h4><p>There are a lot of error in seagate sata smart, but the selftest process is OK, no return any error</p>
<p><a target="_blank" rel="noopener" href="//www.users.on.net/~fzabkar/HDD/Seagate_SER_RRER_HEC.html">The raw value of each SMART attribute occupies 48 bits. Seagate’s Seek Error Rate attribute consists of two parts – a 16-bit count of seek errors in the uppermost 4 nibbles, and a 32-bit count of seeks in the lowermost 8 nibbles. In order to see these data, we will need a SMART utility that reports all 48 bits, preferably in hexadecimal. Two such utilities are HD Sentinel and HDDScan.</a></p>
<p>normalised SER = -10 log (lifetime seek errors / lifetime seeks)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">7 Seek_Error_Rate         0x000f 072 060 030 Pre-fail Always      -       17262017054<br></code></pre></td></tr></table></figure>

<p>Error Recovery Usage Rate<br><a target="_blank" rel="noopener" href="http://www.users.on.net/~fzabkar/HDD/Seagate_SER_RRER_HEC.html">The raw values of the RRER and HER attributes represent a sector count, not an error count. This figure rolls over to 0 once the count reaches about 250 million. I suspect that the drive records the total number of errors in each block of 250 million sectors, and then recalculates the normalised values of each attribute accordingly. This means that RRER and HER would be updated according to a rolling average rather than on a lifetime basis. I’m almost certain that the normalised values are also logarithmic, but I’m not sure how they are calculated. The above figure of 250 million sectors applies to the 7200.11 and DiamondMax 22 models, but may not apply to all.</a></p>
<p>In fact the document mentions (but does not discuss) 5 different error recovery schemes:</p>
<p>HARD = multiple retries invoked and failed<br>FIRM = multiple retries invoked<br>SOFT = 5 retries invoked<br>OTF = 1 retry invoked (On The Fly)<br>RAW = OTF ECC invoked</p>
<p>“On The Fly” means that errored data is corrected using the ECC bytes, without an additional access of the platters.<br>Based on the abovementioned Error Recovery Usage Rate formula, I now postulate that the normalised value of the Raw Read Error Rate attribute could be calculated as follows:<br>normalised RRER = -10 log (number of errored sectors / total bits transferred)<br>The total number of bits is …</p>
<p>That why after smart test will not show any error.</p>
<h4 id="seagate-SAS-device"><a href="#seagate-SAS-device" class="headerlink" title="seagate SAS device"></a>seagate SAS device</h4><p>SAS device has the same issue.</p>
<p>In seagate document “Rapid Increase in ECC on-the-Fly”<br>Background<br>Per Seagate SCSI Commands Reference Manual, Rev D: Log page 03h, parameter code 0000: Errors corrected without substantial delay. Error correction was applied to get perfect data (a.k.a., ECC on-the-fly). “Without Substantial Delay” means the correction did not postpone reading of later sectors (e.g., a revolution was not lost). The counter is incremented once for each logical block that requires correction. Two different blocks corrected during the same command are counted as two events.</p>
<p>Root Cause<br>ECC on-the-fly is necessary for HDDs to function properly at current areal densities. Seagate HDDs will report an increasing value in this counter as part of normal operation. Seagate reports the counter value in accordance with the SCSI specification for informational purposes only; the SCSI specification does not require manufacturers to support the counter (T10/BSR INCITS 513 Revision 36h, pg564). Rate of ECC on the fly can vary based on drive model, drive capacity, areal density, disk speed, and environmental factors. Competitors may not support this parameter because the specification says that it is optional, or the arithmetic is different. It is common industry practice to use on-the-fly correction in order to ensure optimal drive operation. Disparity is caused because competitor does not define an accessible counter for this parameter.</p>
<p>Solution<br>Seagate recommends that customers do not compare values of the log attribute for Seagate brand HDDs to competitors’ drives. Increasing values are part of normal operation of high areal density HDDs. There are no performance concerns related to high values in this counter, and the counter does not indicate any concerns related to reliability or data retention.</p>
<h4 id="plugin-out-the-device"><a href="#plugin-out-the-device" class="headerlink" title="plugin out the device"></a>plugin out the device</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">[Thu Jul  9 11:05:08 2020] sd 12:0:13:0: [sdcz] killing request<br>[Thu Jul  9 11:05:08 2020] sd 12:0:13:0: [sdcz] FAILED Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>[Thu Jul  9 11:05:08 2020] sd 12:0:13:0: [sdcz] CDB: Write(16) 8a 00 00 00 00 04 8c 3f fd fe 00 00 00 02 00 00<br>[Thu Jul  9 11:05:08 2020] blk_update_request: I/O error, dev sdcz, sector 19532873214<br>[Thu Jul  9 11:05:08 2020] sd 12:0:13:0: [sdcz] Synchronizing SCSI cache<br>[Thu Jul  9 11:05:08 2020] sd 12:0:13:0: [sdcz] Synchronize Cache(10) failed: Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br></code></pre></td></tr></table></figure>

<h3 id="SMR-device"><a href="#SMR-device" class="headerlink" title="SMR device"></a>SMR device</h3><p>f2fs support zoned block device<br>for zoned block device</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ dmzadm –format /dev/sdb<br>$ dmsetup create dmz-sdb<br>$ ls -l /dev/mapper/dmz-sdb<br></code></pre></td></tr></table></figure>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Ginger</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2016/08/10/block_device/">http://yoursite.com/2016/08/10/block_device/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/scsi/">scsi</a><a class="post-meta__tags" href="/tags/block/">block</a></div><div class="post_share"><div class="social-share" data-image="/img/photo_by_spacex.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2016/09/04/kcptun/"><img class="prev-cover" src="/img/photo_by_spacex.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">proxychains and kcptun</div></div></a></div><div class="next-post pull-right"><a href="/2016/07/20/fault_ratio/"><img class="next-cover" src="/img/photo_by_spacex.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">calculate HDD SSD fault ratio</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2018/09/27/hba/" title="SAS HBA"><img class="cover" src="/img/photo_by_spacex.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-09-27</div><div class="title">SAS HBA</div></div></a></div><div><a href="/2019/11/26/smartctl/" title="smartctl"><img class="cover" src="/img/photo_by_spacex.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-11-26</div><div class="title">smartctl</div></div></a></div><div><a href="/2016/06/16/spec/" title="spec"><img class="cover" src="/img/photo_by_spacex.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2016-06-16</div><div class="title">spec</div></div></a></div><div><a href="/2016/12/04/openzfs-tips/" title="openzfs tips"><img class="cover" src="/img/photo_by_spacex.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2016-12-04</div><div class="title">openzfs tips</div></div></a></div><div><a href="/2018/07/08/scsi_dev/" title="scsi device maintain"><img class="cover" src="/img/photo_by_spacex.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-07-08</div><div class="title">scsi device maintain</div></div></a></div><div><a href="/2019/01/03/hdd-power-mode/" title="the HDD power mode"><img class="cover" src="/img/photo_by_spacex.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-01-03</div><div class="title">the HDD power mode</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></article></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2015 - 2020 By Ginger</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><div class="js-pjax"><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk({
      clientID: '0b9d5b98d0a972b33cf7',
      clientSecret: '769efc11b32f6c0d03bcbf3ee800dfc4e2690459',
      repo: 'homerl.github.io',
      owner: 'homerl',
      admin: ['homerl'],
      id: '32d7b9cfd8128305bcf1113729d90208',
      language: 'en',
      perPage: 10,
      distractionFreeMode: false,
      pagerDirection: 'last',
      createIssueManually: false,
      updateCountCallback: commentCount
    })
    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    $.getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js', initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>